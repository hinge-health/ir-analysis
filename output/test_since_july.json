{
  "export_metadata": {
    "timestamp": "2025-08-05T01:13:29.218787",
    "total_incidents": 10,
    "incidents_with_rca": 10,
    "rca_analyzed_count": 10,
    "date_range": "2025-07-01 to present",
    "export_type": "since_2025-07-01",
    "rca_match_rate": "100.0%",
    "analysis_success_rate": "100.0%",
    "schema_version": "1.0",
    "purpose": "LLM analysis and custom processing"
  },
  "incidents": [
    {
      "jira_data": {
        "ticket_key": "IR-375",
        "summary": "Unable to start playlist for FTU on alpha and prod app",
        "priority": "P3",
        "created_date": "2025-07-31T11:12:49.901-0700",
        "status": "Root Cause Analysis",
        "description": "IM on personal app on personal device noticed that she was unable to start her first playlist. @Kevin Huang and IM checked in her alpha app and saw the same issue. She also downloaded the app store prod app and its occurring there as well.   \n\nWe did not reach threshold so no incident was triggered. IM manually triggered event.   \n\n10:54 uuid 2690490",
        "custom_fields": {
          "incident_urgency": "P3",
          "pods_engaged": "Rewards and Insights (RAIN), Program Experience, ET Experience (ETE), Responsible: Activity Platform",
          "reporter": null,
          "assignee": null
        },
        "comments": [],
        "metadata": {
          "jira_url": "https://hingehealth.atlassian.net/browse/IR-375",
          "labels": [],
          "component": null,
          "resolution": null
        }
      },
      "confluence_data": {
        "rca_url": "https://hingehealth.atlassian.net/wiki/spaces/RND/pages/1634599285/RCA+IR-375+Unable+to+start+playlist+for+FTU+on+alpha+and+prod+app",
        "rca_available": true,
        "content_html": "<p><strong>Start date:</strong>&nbsp;<time datetime=\"2025-07-31\" /> at 10:54am PST</p><p><strong>Close date:</strong>&nbsp;11:30 am PST</p><p><strong>Severity</strong>:&nbsp;P3</p><p><strong>Adverse Event level (1-4)</strong>: &nbsp;<em>required for Enso, Global App, and Perifit</em></p><p /><p><strong><span style=\"color: rgb(0,102,68);\">NOTE: that this doc is written within 2 weeks of the incident happening. After this is written a review is setup with senior tech leaders in the org.</span></strong></p><p><strong>Author</strong>: <em><span style=\"color: rgb(151,160,175);\">The person writing the doc and going through the RCA in OE meetings. If there is more than one author, the first author is considered to be the primary point of contact.</span></em></p><p>&nbsp;</p><p><strong><span style=\"color: rgb(255,86,48);\">NOTE: Delete the instruction text in each section as you fill it out.</span></strong></p><h2><strong>Example of a well written RCA</strong> <a href=\"https://docs.google.com/document/d/1pP9aDigsCYf3WvPCSTEoGoipQbb-Ec1uDDkca1622MU/edit#heading=h.yih5usn5ed71\"><u>HingeHealth main_db Database Incident</u></a>&nbsp;</h2><h2>Client Facing Summary (if necessary):</h2><p>During the incident, FTU were unable to start their first playlist. The impact of this incident was 16 users. This is not an estimated number. Service has been restored and users are now able to engage with their first playlist.</p><p>&nbsp;</p><h2>Exec Summary</h2><p>A PR was deployed that uncovered a poorly documented check that presented a parameter in the <code>etSessionRecordCreate</code> as optional. This cause a session config to not be generated which fired a 404 resulting in FTU not being able to by pass or advance past the FTU intro video. This has been mitigated.</p><h2>Impact</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"acb6655f-880f-4237-8a86-b89fe25fe816\"><ac:rich-text-body><p>Fill out the details of the impact of the incident starting with the end user/customer and then working from there to the business&nbsp; impact. Be as specific as you can (X customers impacted, Y transactions failed, $Z lost&hellip;Don't put a lot of details about how or why the incident happened, but just focus on the impact. Keep the why for later. For example, &quot;users could not log in for 30 mins because of auth service unavailability&quot; instead of &quot;one aws zone (us-east-02) had a drop in availability and our load balancing setup had a bug so any requests for log in going to us-east-02 failed for 30 mins&quot;.)</p></ac:rich-text-body></ac:structured-macro><ul><li><p><strong>Customer Impact: </strong>Users were unable to start FTU, during the time the faulty deploy was live, approx. 10:30am PST to 11:30 am PST on July 31</p></li><li><p><strong>Security breach: N/A</strong><span style=\"color: rgb(151,160,175);\">/ PHI Data Leak if any (note- PHI Data cannot be added to this doc and RCA docs about security breaches should be access controlled - work with IT Sec on this</span></p></li><li><p><strong>Business Impact: </strong></p><ul><li><p>17 users were unable to start a new FTU playlist</p></li></ul></li><li><p><strong>Internal Impact: N/A</strong></p></li></ul><h2>Timeline (Pacific time, with full timestamps)</h2><p><em><strong>YYYY-MM-DD HH:MM</strong>: Timeline entries (everything from inception of the incident through incident closure. Err on the side providing more detail than less)</em></p><p><em>Specifically record, when <strong>Incident Detected/Reported </strong>and when <strong>Incident Mitigated</strong></em></p><p /><ul><li><p><em><strong>2025-07-31 10:54 PST </strong></em><ac:link><ri:user ri:account-id=\"712020:bf526aee-7514-45cf-a4d3-436c39bda770\" ri:local-id=\"fa639fb2-8576-4691-80a0-93f4d63c36e5\" /></ac:link> reported issues as a FTU being unable to access the first time user playlist to ETE. <ac:link><ri:user ri:account-id=\"6234e59d1dcf800070eaa177\" ri:local-id=\"0dc53eef-9603-4b11-b08f-7a5c15c21fed\" /></ac:link> and <ac:link><ri:user ri:account-id=\"712020:bf526aee-7514-45cf-a4d3-436c39bda770\" ri:local-id=\"2cc70ebd-3006-4bfb-a409-660759cdf3f8\" /></ac:link> investigated via sentry to see if this was a one off or multi user incident. </p></li><li><p><em><strong>2025-07-31 11:02 PST</strong></em> Reported by <ac:link><ri:user ri:account-id=\"712020:bf526aee-7514-45cf-a4d3-436c39bda770\" ri:local-id=\"ffb2c569-3e50-4c42-956d-6b25df1a3099\" /></ac:link> via #incidents <a href=\"https://hingehealth.slack.com/archives/C03QM11HXPE/p1753984940616319\">slack channel </a></p></li><li><p><em><strong>2025-07-31 11:06 PST</strong></em> <a href=\"https://hingehealth.enterprise.slack.com/admin/user_groups\">@ops-ir-comms</a> is alerted </p></li><li><p><em><strong>2025-07-31 11:08 PST</strong></em> <a href=\"https://hingehealth.sentry.io/issues/6732753068/events/4ac3a8c80bb34ba18d3ce4a75064a62c/events/?project=5173911&amp;statsPeriod=24h\" data-card-appearance=\"inline\">https://hingehealth.sentry.io/issues/6732753068/events/4ac3a8c80bb34ba18d3ce4a75064a62c/events/?project=5173911&amp;statsPeriod=24h</a></p></li><li><p><em><strong>2025-07-31 11:16 PST</strong></em> Revert was created <a href=\"https://github.com/hinge-health/phoenix-bff/pull/4206\" data-card-appearance=\"inline\">https://github.com/hinge-health/phoenix-bff/pull/4206</a> </p></li><li><p><em><strong>2025-07-31 11:16 PST</strong></em> PHX BFF frozen</p></li><li><p><em><strong>2025-07-31 11:18 PST</strong></em> Mitigated with prod deploy to last-known-working-version around</p><ul><li><p>CI run <a href=\"https://github.com/hinge-health/phoenix-bff/actions/runs/16656705682\">https://github.com/hinge-health/phoenix-bff/actions/runs/16656705682</a> </p></li></ul></li><li><p><em><strong>2025-07-31 11:45 PST</strong></em> <a href=\"https://github.com/hinge-health/phoenix-bff/pull/4212\" data-card-appearance=\"inline\">https://github.com/hinge-health/phoenix-bff/pull/4212</a> merged</p></li><li><p><em><strong>2025-07-31 12:37 PST</strong></em> PHX BFF unfrozen</p></li></ul><h2>5 WHYs - Why did this incident happen?</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"9ab52040-ad82-4125-b979-c921e975849e\"><ac:rich-text-body><p>Start from how the incident was discovered and then work down from there. Note that there may be a few branches of 5 Whys for issues that have multiple root causes. Make sure you don&rsquo;t answer the question in the first why. Let the trail continue till you get to the root issue and then the root issue has to mandatorily have an action item/lesson learned. This should also result in answers to: What was the technical issue that caused the incident? What was the reason the incident wasn't mitigated sooner?</p></ac:rich-text-body></ac:structured-macro><ol start=\"1\"><li><p>Why:&nbsp; Was there an incident? yes</p></li><li><p>Why:&nbsp;<a href=\"https://github.com/hinge-health/phoenix-bff/pull/4206\" data-card-appearance=\"inline\">https://github.com/hinge-health/phoenix-bff/pull/4206</a> Adds pain trends to the HlnVariant enum and bumps the acitivty service package verstion to get the same update to the shared type PR was merged</p></li><li><p>Why:&nbsp;We tracked it back to this PR <a href=\"https://github.com/hinge-health/phoenix-bff/pull/4182\" data-card-appearance=\"inline\">https://github.com/hinge-health/phoenix-bff/pull/4182</a> </p></li><li><p>Why:&nbsp;We were not passing the session config which was in fact not optional resulting in the config source not being passed to the front end</p></li></ol><h2>Architectural Context</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"53c73de9-b6cf-4dbe-aeef-9c8950a41635\"><ac:rich-text-body><p>Give background and architectural context of the service/data flow in question, this helps reader understand the whole picture.</p></ac:rich-text-body></ac:structured-macro><p>&nbsp;</p><h2>Root Cause Analysis</h2><h3>Major Problems Identified</h3><p>List the major issues that the analysis identified.</p><ul><li><p><strong>Immediate Cause</strong>: <strong>IM used the production app</strong></p></li><li><p><strong>Contributing Factors</strong>: <strong>Human error, could have tested more, need better documentation in line in the app to alert ICs to the two paths of data flow (session config is not actually optional)</strong></p></li><li><p><strong>Underlying Causes:</strong></p><ul><li><p><strong>Technical Infrastructure:</strong> <em><span style=\"color: rgb(151,160,175);\">Assess the state of the company's technical infrastructure, including network stability, server performance, and software dependencies.</span></em></p></li><li><p><strong>Processes and Procedures</strong>: <em><span style=\"color: rgb(151,160,175);\">Evaluate the effectiveness of existing processes and procedures related to deployment, maintenance, and troubleshooting. Identify any gaps or weaknesses.</span></em></p></li><li><p><strong>Human Factors</strong>: see above</p></li></ul></li></ul><h2>Lessons Learned</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"e4850bd1-cc69-453c-b2d3-a58c3630d745\"><ac:rich-text-body><p>The lessons learned from the 5 Whys and potentially more get added as a list. Add a mandatory section on how Time to Detection and Time to Resolution can be cut in half.</p><p>Focus on how lessons learnt become a part of our dev/deploy/monitoring flow, what steps we have taken to integrate those lessons.</p></ac:rich-text-body></ac:structured-macro><h4>Actionable Insights:</h4><p>Summarize the key insights and lessons learned from the 5 Whys analysis. Focus on actionable recommendations and corrective actions that can be implemented to prevent similar incidents in the future. Include recommendations for process improvements, system enhancements, training needs, and any other measures deemed necessary based on the analysis.</p><ul><li><p>Better inline commenting to alert ICs to the split nature of our users during unification</p></li><li><p>Better end to end testing before merge on app workflows that might potentially be impacted</p></li></ul><h4>Time to Detection</h4><p>10:20 PST&nbsp;</p><h4>Time to Resolution</h4><p>&nbsp;11:30 PST until full resolution</p><h2>Action Items</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"f5fe8302-e70c-4592-93a6-5afa64b732ff\"><ac:rich-text-body><p>Action items from the 5 Whys section get added here. Every action item needs to have an owner, a JIRA ticket and an ETA.</p></ac:rich-text-body></ac:structured-macro><p>Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:</p><ul><li><p><strong>Technical Solutions:</strong> <em><span style=\"color: rgb(151,160,175);\">Implementing upgrades, patches, or enhancements to improve system reliability, scalability, and performance.</span></em></p></li><li><p><strong>Process Improvements: Better monitoring of FTU workflows </strong></p></li><li><p><strong>Training and Education: </strong><span style=\"color: rgb(151,160,175);\">Providing training programs or workshops to enhance technical skills, promote best practices, and improve communication within teams.</span></p></li><li><p><strong>Monitoring and Alerting: Better monitoring of FTU workflows, consider lowering threshold for earlier alerting </strong> <em><span style=\"color: rgb(151,160,175);\">Enhancing monitoring tools and alerting mechanisms to detect issues proactively and minimize downtime.</span></em></p></li><li><p><strong>Testing Improvements:</strong>&nbsp; <em><span style=\"color: rgb(151,160,175);\">Address gaps in testing that allowed this incident to occur and will prevent this from happening again.</span></em></p></li></ul><p>&nbsp;</p><hr /><h3>Post Mortem Notes:&nbsp;</h3><p><a href=\"https://github.com/hinge-health/phoenix-bff/pull/4214\" data-card-appearance=\"inline\">https://github.com/hinge-health/phoenix-bff/pull/4214</a> </p>",
        "content_text": "Start date:  at 10:54am PSTClose date: 11:30 am PSTSeverity: P3Adverse Event level (1-4):  required for Enso, Global App, and PerifitNOTE: that this doc is written within 2 weeks of the incident happening. After this is written a review is setup with senior tech leaders in the org.Author: The person writing the doc and going through the RCA in OE meetings. If there is more than one author, the first author is considered to be the primary point of contact. NOTE: Delete the instruction text in each section as you fill it out.Example of a well written RCA HingeHealth main_db Database Incident Client Facing Summary (if necessary):During the incident, FTU were unable to start their first playlist. The impact of this incident was 16 users. This is not an estimated number. Service has been restored and users are now able to engage with their first playlist. Exec SummaryA PR was deployed that uncovered a poorly documented check that presented a parameter in the etSessionRecordCreate as optional. This cause a session config to not be generated which fired a 404 resulting in FTU not being able to by pass or advance past the FTU intro video. This has been mitigated.ImpactFill out the details of the impact of the incident starting with the end user/customer and then working from there to the business  impact. Be as specific as you can (X customers impacted, Y transactions failed, $Z lost…Don't put a lot of details about how or why the incident happened, but just focus on the impact. Keep the why for later. For example, \"users could not log in for 30 mins because of auth service unavailability\" instead of \"one aws zone (us-east-02) had a drop in availability and our load balancing setup had a bug so any requests for log in going to us-east-02 failed for 30 mins\".)Customer Impact: Users were unable to start FTU, during the time the faulty deploy was live, approx. 10:30am PST to 11:30 am PST on July 31Security breach: N/A/ PHI Data Leak if any (note- PHI Data cannot be added to this doc and RCA docs about security breaches should be access controlled - work with IT Sec on thisBusiness Impact: 17 users were unable to start a new FTU playlistInternal Impact: N/ATimeline (Pacific time, with full timestamps)YYYY-MM-DD HH:MM: Timeline entries (everything from inception of the incident through incident closure. Err on the side providing more detail than less)Specifically record, when Incident Detected/Reported and when Incident Mitigated2025-07-31 10:54 PST reported issues as a FTU being unable to access the first time user playlist to ETE. and investigated via sentry to see if this was a one off or multi user incident. 2025-07-31 11:02 PST Reported by via #incidents slack channel 2025-07-31 11:06 PST @ops-ir-comms is alerted 2025-07-31 11:08 PST https://hingehealth.sentry.io/issues/6732753068/events/4ac3a8c80bb34ba18d3ce4a75064a62c/events/?project=5173911&statsPeriod=24h2025-07-31 11:16 PST Revert was created https://github.com/hinge-health/phoenix-bff/pull/4206 2025-07-31 11:16 PST PHX BFF frozen2025-07-31 11:18 PST Mitigated with prod deploy to last-known-working-version aroundCI run https://github.com/hinge-health/phoenix-bff/actions/runs/16656705682 2025-07-31 11:45 PST https://github.com/hinge-health/phoenix-bff/pull/4212 merged2025-07-31 12:37 PST PHX BFF unfrozen5 WHYs - Why did this incident happen?Start from how the incident was discovered and then work down from there. Note that there may be a few branches of 5 Whys for issues that have multiple root causes. Make sure you don’t answer the question in the first why. Let the trail continue till you get to the root issue and then the root issue has to mandatorily have an action item/lesson learned. This should also result in answers to: What was the technical issue that caused the incident? What was the reason the incident wasn't mitigated sooner?Why:  Was there an incident? yesWhy: https://github.com/hinge-health/phoenix-bff/pull/4206 Adds pain trends to the HlnVariant enum and bumps the acitivty service package verstion to get the same update to the shared type PR was mergedWhy: We tracked it back to this PR https://github.com/hinge-health/phoenix-bff/pull/4182 Why: We were not passing the session config which was in fact not optional resulting in the config source not being passed to the front endArchitectural ContextGive background and architectural context of the service/data flow in question, this helps reader understand the whole picture. Root Cause AnalysisMajor Problems IdentifiedList the major issues that the analysis identified.Immediate Cause: IM used the production appContributing Factors: Human error, could have tested more, need better documentation in line in the app to alert ICs to the two paths of data flow (session config is not actually optional)Underlying Causes:Technical Infrastructure: Assess the state of the company's technical infrastructure, including network stability, server performance, and software dependencies.Processes and Procedures: Evaluate the effectiveness of existing processes and procedures related to deployment, maintenance, and troubleshooting. Identify any gaps or weaknesses.Human Factors: see aboveLessons LearnedThe lessons learned from the 5 Whys and potentially more get added as a list. Add a mandatory section on how Time to Detection and Time to Resolution can be cut in half.Focus on how lessons learnt become a part of our dev/deploy/monitoring flow, what steps we have taken to integrate those lessons.Actionable Insights:Summarize the key insights and lessons learned from the 5 Whys analysis. Focus on actionable recommendations and corrective actions that can be implemented to prevent similar incidents in the future. Include recommendations for process improvements, system enhancements, training needs, and any other measures deemed necessary based on the analysis.Better inline commenting to alert ICs to the split nature of our users during unificationBetter end to end testing before merge on app workflows that might potentially be impactedTime to Detection10:20 PST Time to Resolution 11:30 PST until full resolutionAction ItemsAction items from the 5 Whys section get added here. Every action item needs to have an owner, a JIRA ticket and an ETA.Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:Technical Solutions: Implementing upgrades, patches, or enhancements to improve system reliability, scalability, and performance.Process Improvements: Better monitoring of FTU workflows Training and Education: Providing training programs or workshops to enhance technical skills, promote best practices, and improve communication within teams.Monitoring and Alerting: Better monitoring of FTU workflows, consider lowering threshold for earlier alerting Enhancing monitoring tools and alerting mechanisms to detect issues proactively and minimize downtime.Testing Improvements:  Address gaps in testing that allowed this incident to occur and will prevent this from happening again. Post Mortem Notes: https://github.com/hinge-health/phoenix-bff/pull/4214",
        "title": "RCA: IR-375 Unable to start playlist for FTU on alpha and prod app",
        "page_metadata": {
          "space": "RND",
          "page_id": "1634599285",
          "estimated_word_count": 1038
        }
      },
      "analysis_results": {
        "content_analysis": {
          "incident_summary": "A PR was deployed that uncovered a poorly documented check that presented a parameter in the etSessionRecordCreate as optional. This cause a session config to not be generated which fired a 404 resulting in FTU not being able to by pass or advance past the FTU intro video.",
          "users_impacted": "Duration: From 2025-07-31 10:54 PST to incident resolution",
          "root_causes": [
            "IdentifiedList the major issues that the analysis identified",
            "Processes and Procedures: Evaluate the effectiveness of existing processes and procedures related to deployment, maintenance, and troubleshooting",
            "Identify any gaps or weaknesses",
            "IM used the production app",
            "Assess the state of the company's"
          ],
          "analysis_quality": "low"
        },
        "quality_assessment": {
          "score": 95,
          "grade": "A",
          "feedback": "Exceptional RCA (95/100). Strongest area: Root Cause Analysis. Consider sharing as template. Minor enhancement opportunity: Action Items Prevention.",
          "strengths": [
            "Timeline Detection: Detailed timestamps provided",
            "Timeline Detection: Dedicated timeline section",
            "Timeline Detection: Detection methods described"
          ],
          "critical_gaps": [
            "None identified"
          ]
        },
        "business_impact": {
          "impact_score": 4,
          "customer_count_affected": "16 users",
          "revenue_impact_est": "Low: $1K-5K potential impact",
          "service_downtime_minutes": 30,
          "severity_justification": "Moderate business impact; Service interruption (30 minutes)"
        },
        "technical_analysis": {
          "root_cause_category": "Process/Human Error",
          "detection_time_minutes": 0,
          "resolution_time_minutes": 0,
          "technical_debt_level": "Medium - Some improvements required",
          "automation_score": 3
        }
      }
    },
    {
      "jira_data": {
        "ticket_key": "IR-374",
        "summary": "First Week Goal workflow is not transitioning to the correct state.",
        "priority": "P3",
        "created_date": "2025-07-24T09:36:57.310-0700",
        "status": "Follow Up Work",
        "description": "First Week Goal workflow is not transitioning to the correct state.",
        "custom_fields": {
          "incident_urgency": "P3",
          "pods_engaged": "Workflows, Responsible: Workflows (Care Hub)",
          "reporter": null,
          "assignee": null
        },
        "comments": [],
        "metadata": {
          "jira_url": "https://hingehealth.atlassian.net/browse/IR-374",
          "labels": [],
          "component": null,
          "resolution": null
        }
      },
      "confluence_data": {
        "rca_url": "https://hingehealth.atlassian.net/wiki/spaces/RND/pages/1621688511/RCA+IR-374+First+Week+Goal+workflow+is+not+transitioning+to+the+correct+state.",
        "rca_available": true,
        "content_html": "<p><strong>Start date:</strong>&nbsp;July 7 10:50 AM PST</p><p><strong>Close date:</strong>&nbsp;July 25 4:30 AM PST</p><p><strong>Severity</strong>:&nbsp;P3</p><p /><p><strong><span style=\"color: rgb(0,102,68);\">NOTE: that this doc is written within 2 weeks of the incident happening. After this is written a review is setup with senior tech leaders in the org.</span></strong></p><p><strong>Author</strong>: <ac:link><ri:user ri:account-id=\"712020:70a30034-8ebe-41b2-8439-fb874d2f12a7\" ri:local-id=\"2c5dceae-6eec-4baf-9a60-4db026599427\" /></ac:link> </p><p>&nbsp;</p><p><strong><span style=\"color: rgb(255,86,48);\">NOTE: Delete the instruction text in each section as you fill it out.</span></strong></p><h2>Exec Summary</h2><p>On July 24th, QA identified FWG workflows not appearing timely in Carehub UI during regression testing, initially blocking the weekly release. Investigation showed workflows were generated but not transitioning to visible states due to a faulty timer mechanism in the User Workflow Service. After the release proceeded and the issue reproduced in production, triage team determined that timer-dependent workflows weren't transitioning correctly post-expiry. The problem was resolved on July 25th through an emergency patch deployment and data migration to emit missed events and correct affected workflow due dates.</p><h2>Impact</h2><ul><li><p><strong>Customer Impact: </strong>CTMS were not able to see the FWG and pelvic trainer outreach workflows on Carehub UI</p></li><li><p><strong>Security breach: </strong>NA</p></li><li><p><strong>Internal Impact: </strong>17k FWG workflows and 1.2k pelvic trainer outreach workflows were delayed in Carehub UI</p></li></ul><h2>Timeline (Pacific time, with full timestamps)</h2><p><em><strong>2025-07-07 03:39 PM IST</strong>: </em><strong>Incident began</strong>, <a href=\"https://github.com/hinge-health/user-workflow-service/pull/2109\" data-card-appearance=\"inline\">https://github.com/hinge-health/user-workflow-service/pull/2109</a>  PR<ac:inline-comment-marker ac:ref=\"fe174e71-0751-47d4-bfe4-67b70a20440a\"> which transitioned from eslint to oxlint as part of performance improvement initiative was merged</ac:inline-comment-marker></p><p><em><strong>2025-07-09 10:12 PM IST: </strong></em>QA team reported that pelvic trainer outreach workflow wasn&rsquo;t triggering on stage env, even reported failures in automation test suite and after sometime confirmed the same issue in prod env, cut a ticket <ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"17846c1a-43fd-47ef-b2d3-f42277761b5f\"><ac:parameter ac:name=\"key\">MERU-4976</ac:parameter><ac:parameter ac:name=\"serverId\">b75f938a-9135-3e8a-b70b-9bd825e38cc0</ac:parameter><ac:parameter ac:name=\"server\">System Jira</ac:parameter></ac:structured-macro>  and called it out as a blocker for carehub weekly release.</p><p><em><strong>2025-07-09 11:21 PM IST: </strong></em>Developers confirmed that the workflow was being generated in both stage and prod environments. The workflow is designed to transition after 7 days, and the remaining workflows were scheduled to transition at their expected times. Based on this, the issue was considered to be behaving as expected at that point in time, and the QA team deprioritized it.</p><p><em><strong>2025-07-24 11:57 AM IST: </strong></em>QA team reported that FWG(first week goal) workflow was showing up in Carehub UI with a delay intermittently, cut a ticket <ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"50c5dcd1-863e-4174-bfa9-3c2ad7b34489\"><ac:parameter ac:name=\"key\">MERU-5177</ac:parameter><ac:parameter ac:name=\"serverId\">b75f938a-9135-3e8a-b70b-9bd825e38cc0</ac:parameter><ac:parameter ac:name=\"server\">System Jira</ac:parameter></ac:structured-macro>  and called it out as a blocker for the weekly release.</p><p><em><strong>2025-07-24 04:21 PM IST:  </strong></em>On initial investigation, it looked like a redis issue in stage environment given that the workflows were showing up on the UI after some delay intermittently. Since UWS is on CD and workflows were created after a delay, The issue was deprioritized and the team decided to proceed with the weekly release of other carehub services and validate the issue on Prod. </p><p><em><strong>2025-07-24 07:48 PM IST</strong>:  </em><strong>Incident detected</strong>, QE confirmed that the issue is reproducible on prod after the deployment. On checking the prod database, it was found that there are ~17k workflows which did not transition to correct state after the scheduled time. The prod logs were also missing for the workflow transition process which validated the bug. The triage team was assembled to look further into it from different angles.</p><p><em><strong>2025-07-24 10:07 PM IST</strong>:  </em><strong>Incident reported</strong>, and the triage team continued looking into possible causes and understand the actual impact.</p><p><em><strong>2025-07-25 12:30 PM IST</strong>:  </em>Triage team found the root cause and raised a patch <a href=\"https://github.com/hinge-health/user-workflow-service/pull/2146\">PR</a> to fix the delayed timer issue.</p><p><em><strong>2025-07-25 12:57 PM IST</strong>:  </em>QA team confirmed that the fix is working and the issue is mitigated on stage environment.</p><p><em><strong>2025-07-25 01:19 PM IST</strong>:  </em><strong>Incident mitigated,</strong> <ac:inline-comment-marker ac:ref=\"8dee81f1-13b9-4411-a727-ebc645b6d481\">The </ac:inline-comment-marker><ac:inline-comment-marker ac:ref=\"8dee81f1-13b9-4411-a727-ebc645b6d481\"><a href=\"https://github.com/hinge-health/user-workflow-service/pull/2146\" data-card-appearance=\"inline\">https://github.com/hinge-health/user-workflow-service/pull/2146</a></ac:inline-comment-marker><ac:inline-comment-marker ac:ref=\"8dee81f1-13b9-4411-a727-ebc645b6d481\">  PR to patch</ac:inline-comment-marker><strong><ac:inline-comment-marker ac:ref=\"8dee81f1-13b9-4411-a727-ebc645b6d481\"> </ac:inline-comment-marker></strong><ac:inline-comment-marker ac:ref=\"8dee81f1-13b9-4411-a727-ebc645b6d481\">the issue was merged and was deployed to prod env and QA team validated the fix to be working in prod env.</ac:inline-comment-marker></p><p><em><strong>2025-07-25 03:29 PM IST</strong></em>: <a href=\"https://github.com/hinge-health/user-workflow-service/pull/2145\" data-card-appearance=\"inline\">https://github.com/hinge-health/user-workflow-service/pull/2145</a>  PR to emit the respective events to transition the workflows to correct state was merged.</p><p><em><strong>2025-07-25 04:53 PM IST</strong></em>: <a href=\"https://github.com/hinge-health/user-workflow-service/pull/2148\" data-card-appearance=\"inline\">https://github.com/hinge-health/user-workflow-service/pull/2148</a>  data migration PR to update the due date of the transitioned workflows was merged.</p><p><em><strong>2025-07-25 05:03 PM IST</strong>:  </em><strong>Incident closed</strong></p><p /><h2>5 WHYs - Why did this incident happen?</h2><ol start=\"1\"><li><p><strong>Why did workflows fail to transition from one state to another?</strong></p></li></ol><ul><li><p>Because the timer mechanism in UWS failed to function properly, preventing workflows from transitioning to the appropriate state post expiry.</p></li></ul><ol start=\"2\"><li><p><strong>Why did the timer mechanism fail?</strong>&nbsp;</p></li></ol><ul><li><p>Because automatic linting during the eslint to oxlint transition removed a critical variable from destructuring that was essential for the timer functionality.</p></li></ul><ol start=\"3\"><li><p><strong>Why wasn't this breaking change detected earlier before an incident was created?</strong></p></li></ol><ul><li><p>Because the initial bug report on July 9th for pelvic trainer outreach was incorrectly assessed as being isolated to the staging environment and was deprioritized, preventing proper investigation of what was actually a systemic issue affecting the timer mechanism.</p></li></ul><ol start=\"4\"><li><p><strong>Why didn't the code review process catch this critical issue?</strong></p></li></ol><ul><li><p>Because the large scope of the PR made it difficult for reviewers to thoroughly assess all automated changes, and there was likely insufficient focus on verifying that automatic linting changes didn't break runtime dependencies, especially for complex mechanisms like timers.</p></li></ul><ol start=\"5\"><li><p><strong>Why didn't automated testing or monitoring catch this runtime dependency break?</strong></p></li></ol><ul><li><p>Because the existing test suite lacked comprehensive integration tests that validated timer-dependent workflow state transitions, and there were no monitoring setup to catch the absence of workflow transitions.</p></li></ul><h2>Architectural Context</h2><p>&nbsp;<ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"User Workflow Service - Quick Guide\" ri:version-at-save=\"15\" /><ac:link-body>User Workflow Service - Quick Guide</ac:link-body></ac:link> </p><h2>Root Cause Analysis</h2><h3>Major Problems Identified</h3><p>List the major issues that the analysis identified.</p><ul><li><p><strong>Immediate Cause</strong>: PR to migrate eslint to oxlint as part of performance improvement initiative.</p></li><li><p><strong>Contributing Factors</strong>: Human error and inadequate integration tests to validate timer-dependent workflow state transitions</p></li><li><p><strong>Underlying Causes:</strong></p><ul><li><p><strong>Technical Infrastructure:</strong> NA</p></li><li><p><strong>Processes and Procedures</strong>: NA</p></li><li><p><strong>Human Factors</strong>: <em><span style=\"color: rgb(151,160,175);\">Consider the role of human factors, such as training deficiencies, communication issues, or decision-making processes, in the incident.</span></em></p></li></ul></li></ul><h2>Lessons Learned</h2><h4>Actionable Insights:</h4><p>Summarize the key insights and lessons learned from the 5 Whys analysis. Focus on actionable recommendations and corrective actions that can be implemented to prevent similar incidents in the future. Include recommendations for process improvements, system enhancements, training needs, and any other measures deemed necessary based on the analysis.</p><ul><li><p>Setup monitoring to detect any anomalies in workflow transition from passive states</p></li><li><p>&nbsp;Develop integration tests specifically validating timer-dependent workflow state transitions and other critical runtime dependencies</p></li><li><p>Establish guidelines limiting PR scope for automated changes to enable thorough manual review</p></li></ul><h4>Time to Detection</h4><p>&nbsp;~2h 20m</p><h4>Time to Resolution</h4><p>&nbsp;~15h</p><h2>Action Items</h2><p>Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:</p><ul><li><p><strong><ac:inline-comment-marker ac:ref=\"384a2407-1574-432e-9e99-eef0c3e36841\">Technical Solutions:</ac:inline-comment-marker></strong><ac:inline-comment-marker ac:ref=\"384a2407-1574-432e-9e99-eef0c3e36841\"> </ac:inline-comment-marker></p></li></ul><ac:task-list>\n<ac:task>\n<ac:task-id>1</ac:task-id>\n<ac:task-uuid>34430da3-2ed0-46fe-97b1-ccfa13eb9d77</ac:task-uuid>\n<ac:task-status>complete</ac:task-status>\n<ac:task-body><span class=\"placeholder-inline-tasks\">Patch fix: <a href=\"https://github.com/hinge-health/user-workflow-service/pull/2146\" data-card-appearance=\"inline\">https://github.com/hinge-health/user-workflow-service/pull/2146</a> </span></ac:task-body>\n</ac:task>\n<ac:task>\n<ac:task-id>2</ac:task-id>\n<ac:task-uuid>3cb42ce2-0242-4313-842c-a64b8693a526</ac:task-uuid>\n<ac:task-status>complete</ac:task-status>\n<ac:task-body><span class=\"placeholder-inline-tasks\">Script to emit transition events: <a href=\"https://github.com/hinge-health/user-workflow-service/pull/2145\" data-card-appearance=\"inline\">https://github.com/hinge-health/user-workflow-service/pull/2145</a> </span></ac:task-body>\n</ac:task>\n<ac:task>\n<ac:task-id>3</ac:task-id>\n<ac:task-uuid>9d6bbee1-4e32-48a4-bb25-b2be257ad2f3</ac:task-uuid>\n<ac:task-status>complete</ac:task-status>\n<ac:task-body><span class=\"placeholder-inline-tasks\">Data migration to update workflow due dates: <a href=\"https://github.com/hinge-health/user-workflow-service/pull/2148\" data-card-appearance=\"inline\">https://github.com/hinge-health/user-workflow-service/pull/2148</a> </span></ac:task-body>\n</ac:task>\n</ac:task-list><ul><li><p><strong>Process Improvements:</strong></p><ul><li><p>Break large PRs into smaller ones for ease of review</p></li></ul></li><li><p><strong>Training and Education: </strong>NA</p></li><li><p><strong>Monitoring and Alerting:</strong></p><ul><li><p>Setup Mode report to identify workflows in bad states</p><ul><li><p>Owner: <ac:link><ri:user ri:account-id=\"712020:e1dbe56c-0d43-4c28-bd7d-dd50759cc1a8\" ri:local-id=\"82a8b7e2-2339-4048-b0cb-b837704346fb\" /></ac:link> </p></li><li><p><ac:inline-comment-marker ac:ref=\"86404d4a-cbeb-45a3-8006-4ee53f2dc80d\">JIRA</ac:inline-comment-marker>: <ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"2b9aed7a-69cd-4a12-9b69-648372cc76c1\"><ac:parameter ac:name=\"key\">OPSDATA-910</ac:parameter><ac:parameter ac:name=\"serverId\">b75f938a-9135-3e8a-b70b-9bd825e38cc0</ac:parameter><ac:parameter ac:name=\"server\">System Jira</ac:parameter></ac:structured-macro> </p><ac:task-list>\n<ac:task>\n<ac:task-id>4</ac:task-id>\n<ac:task-uuid>8e895381-7bdd-4031-bde0-eba4edd5d696</ac:task-uuid>\n<ac:task-status>complete</ac:task-status>\n<ac:task-body><span class=\"placeholder-inline-tasks\">Completed</span></ac:task-body>\n</ac:task>\n</ac:task-list></li><li><p><a href=\"https://app.mode.com/hinge_health/reports/7822aa515578\">https://app.mode.com/hinge_health/reports/7822aa515578</a></p></li></ul></li><li><p>Implement anomaly based monitors on passive state transitions</p><ul><li><p>Owner: <ac:link><ri:user ri:account-id=\"641dd3d89d2bc6c90a8bb27d\" ri:local-id=\"16b0135d-4722-42c3-8d00-6e302de0edb8\" /></ac:link> </p></li><li><p>JIRA: <ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"94f3fe22-d7fd-43d0-949e-644282e5fae6\"><ac:parameter ac:name=\"key\">NAN-5061</ac:parameter><ac:parameter ac:name=\"serverId\">b75f938a-9135-3e8a-b70b-9bd825e38cc0</ac:parameter><ac:parameter ac:name=\"server\">System Jira</ac:parameter></ac:structured-macro> </p><ac:task-list>\n<ac:task>\n<ac:task-id>5</ac:task-id>\n<ac:task-uuid>fe192056-7bff-492f-a7c6-2b22c14df7ed</ac:task-uuid>\n<ac:task-status>complete</ac:task-status>\n<ac:task-body><span class=\"placeholder-inline-tasks\">Completed</span></ac:task-body>\n</ac:task>\n</ac:task-list></li></ul></li></ul></li><li><p><strong>Testing Improvements:</strong>&nbsp;</p><ul><li><p>Improve integration tests to validate timer-dependent workflow state transitions</p><ul><li><p>Owner: <ac:link><ri:user ri:account-id=\"712020:72cada1b-1910-4e7a-9ff8-75484d2c00e3\" ri:local-id=\"4868af82-b1d0-4866-8329-1ab80b01df00\" /></ac:link> </p></li><li><p>JIRA: <ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"19a6c9f2-cbe7-44c2-b52c-6eb668a1b961\"><ac:parameter ac:name=\"key\">MERU-5243</ac:parameter><ac:parameter ac:name=\"serverId\">b75f938a-9135-3e8a-b70b-9bd825e38cc0</ac:parameter><ac:parameter ac:name=\"server\">System Jira</ac:parameter></ac:structured-macro> </p></li><li><p>ETA: August 29th, 2025</p><ac:task-list>\n<ac:task>\n<ac:task-id>6</ac:task-id>\n<ac:task-uuid>05dbdcf8-cb13-4b1e-adec-bf0fcf06db88</ac:task-uuid>\n<ac:task-status>incomplete</ac:task-status>\n<ac:task-body><span class=\"placeholder-inline-tasks\">Pending</span></ac:task-body>\n</ac:task>\n</ac:task-list></li></ul></li></ul></li></ul><p>&nbsp;</p><hr /><h3>Post Mortem Notes:&nbsp;</h3><p>NA</p>",
        "content_text": "Start date: July 7 10:50 AM PSTClose date: July 25 4:30 AM PSTSeverity: P3NOTE: that this doc is written within 2 weeks of the incident happening. After this is written a review is setup with senior tech leaders in the org.Author: NOTE: Delete the instruction text in each section as you fill it out.Exec SummaryOn July 24th, QA identified FWG workflows not appearing timely in Carehub UI during regression testing, initially blocking the weekly release. Investigation showed workflows were generated but not transitioning to visible states due to a faulty timer mechanism in the User Workflow Service. After the release proceeded and the issue reproduced in production, triage team determined that timer-dependent workflows weren't transitioning correctly post-expiry. The problem was resolved on July 25th through an emergency patch deployment and data migration to emit missed events and correct affected workflow due dates.ImpactCustomer Impact: CTMS were not able to see the FWG and pelvic trainer outreach workflows on Carehub UISecurity breach: NAInternal Impact: 17k FWG workflows and 1.2k pelvic trainer outreach workflows were delayed in Carehub UITimeline (Pacific time, with full timestamps)2025-07-07 03:39 PM IST: Incident began, https://github.com/hinge-health/user-workflow-service/pull/2109 PR which transitioned from eslint to oxlint as part of performance improvement initiative was merged2025-07-09 10:12 PM IST: QA team reported that pelvic trainer outreach workflow wasn’t triggering on stage env, even reported failures in automation test suite and after sometime confirmed the same issue in prod env, cut a ticket MERU-4976b75f938a-9135-3e8a-b70b-9bd825e38cc0System Jira and called it out as a blocker for carehub weekly release.2025-07-09 11:21 PM IST: Developers confirmed that the workflow was being generated in both stage and prod environments. The workflow is designed to transition after 7 days, and the remaining workflows were scheduled to transition at their expected times. Based on this, the issue was considered to be behaving as expected at that point in time, and the QA team deprioritized it.2025-07-24 11:57 AM IST: QA team reported that FWG(first week goal) workflow was showing up in Carehub UI with a delay intermittently, cut a ticket MERU-5177b75f938a-9135-3e8a-b70b-9bd825e38cc0System Jira and called it out as a blocker for the weekly release.2025-07-24 04:21 PM IST: On initial investigation, it looked like a redis issue in stage environment given that the workflows were showing up on the UI after some delay intermittently. Since UWS is on CD and workflows were created after a delay, The issue was deprioritized and the team decided to proceed with the weekly release of other carehub services and validate the issue on Prod. 2025-07-24 07:48 PM IST: Incident detected, QE confirmed that the issue is reproducible on prod after the deployment. On checking the prod database, it was found that there are ~17k workflows which did not transition to correct state after the scheduled time. The prod logs were also missing for the workflow transition process which validated the bug. The triage team was assembled to look further into it from different angles.2025-07-24 10:07 PM IST: Incident reported, and the triage team continued looking into possible causes and understand the actual impact.2025-07-25 12:30 PM IST: Triage team found the root cause and raised a patch PR to fix the delayed timer issue.2025-07-25 12:57 PM IST: QA team confirmed that the fix is working and the issue is mitigated on stage environment.2025-07-25 01:19 PM IST: Incident mitigated, The https://github.com/hinge-health/user-workflow-service/pull/2146 PR to patch the issue was merged and was deployed to prod env and QA team validated the fix to be working in prod env.2025-07-25 03:29 PM IST: https://github.com/hinge-health/user-workflow-service/pull/2145 PR to emit the respective events to transition the workflows to correct state was merged.2025-07-25 04:53 PM IST: https://github.com/hinge-health/user-workflow-service/pull/2148 data migration PR to update the due date of the transitioned workflows was merged.2025-07-25 05:03 PM IST: Incident closed5 WHYs - Why did this incident happen?Why did workflows fail to transition from one state to another?Because the timer mechanism in UWS failed to function properly, preventing workflows from transitioning to the appropriate state post expiry.Why did the timer mechanism fail? Because automatic linting during the eslint to oxlint transition removed a critical variable from destructuring that was essential for the timer functionality.Why wasn't this breaking change detected earlier before an incident was created?Because the initial bug report on July 9th for pelvic trainer outreach was incorrectly assessed as being isolated to the staging environment and was deprioritized, preventing proper investigation of what was actually a systemic issue affecting the timer mechanism.Why didn't the code review process catch this critical issue?Because the large scope of the PR made it difficult for reviewers to thoroughly assess all automated changes, and there was likely insufficient focus on verifying that automatic linting changes didn't break runtime dependencies, especially for complex mechanisms like timers.Why didn't automated testing or monitoring catch this runtime dependency break?Because the existing test suite lacked comprehensive integration tests that validated timer-dependent workflow state transitions, and there were no monitoring setup to catch the absence of workflow transitions.Architectural Context User Workflow Service - Quick Guide Root Cause AnalysisMajor Problems IdentifiedList the major issues that the analysis identified.Immediate Cause: PR to migrate eslint to oxlint as part of performance improvement initiative.Contributing Factors: Human error and inadequate integration tests to validate timer-dependent workflow state transitionsUnderlying Causes:Technical Infrastructure: NAProcesses and Procedures: NAHuman Factors: Consider the role of human factors, such as training deficiencies, communication issues, or decision-making processes, in the incident.Lessons LearnedActionable Insights:Summarize the key insights and lessons learned from the 5 Whys analysis. Focus on actionable recommendations and corrective actions that can be implemented to prevent similar incidents in the future. Include recommendations for process improvements, system enhancements, training needs, and any other measures deemed necessary based on the analysis.Setup monitoring to detect any anomalies in workflow transition from passive states Develop integration tests specifically validating timer-dependent workflow state transitions and other critical runtime dependenciesEstablish guidelines limiting PR scope for automated changes to enable thorough manual reviewTime to Detection ~2h 20mTime to Resolution ~15hAction ItemsBased on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:Technical Solutions: 1 34430da3-2ed0-46fe-97b1-ccfa13eb9d77 complete Patch fix: https://github.com/hinge-health/user-workflow-service/pull/2146 2 3cb42ce2-0242-4313-842c-a64b8693a526 complete Script to emit transition events: https://github.com/hinge-health/user-workflow-service/pull/2145 3 9d6bbee1-4e32-48a4-bb25-b2be257ad2f3 complete Data migration to update workflow due dates: https://github.com/hinge-health/user-workflow-service/pull/2148 Process Improvements:Break large PRs into smaller ones for ease of reviewTraining and Education: NAMonitoring and Alerting:Setup Mode report to identify workflows in bad statesOwner: JIRA: OPSDATA-910b75f938a-9135-3e8a-b70b-9bd825e38cc0System Jira 4 8e895381-7bdd-4031-bde0-eba4edd5d696 complete Completed https://app.mode.com/hinge_health/reports/7822aa515578Implement anomaly based monitors on passive state transitionsOwner: JIRA: NAN-5061b75f938a-9135-3e8a-b70b-9bd825e38cc0System Jira 5 fe192056-7bff-492f-a7c6-2b22c14df7ed complete Completed Testing Improvements: Improve integration tests to validate timer-dependent workflow state transitionsOwner: JIRA: MERU-5243b75f938a-9135-3e8a-b70b-9bd825e38cc0System Jira ETA: August 29th, 2025 6 05dbdcf8-cb13-4b1e-adec-bf0fcf06db88 incomplete Pending Post Mortem Notes: NA",
        "title": "RCA: IR-374 First Week Goal workflow is not transitioning to the correct state.",
        "page_metadata": {
          "space": "RND",
          "page_id": "1621688511",
          "estimated_word_count": 1095
        }
      },
      "analysis_results": {
        "content_analysis": {
          "incident_summary": "On July 24th, QA identified FWG workflows not appearing timely in Carehub UI during regression testing, initially blocking the weekly release. Investigation showed workflows were generated but not transitioning to visible states due to a faulty timer mechanism in the User Workflow Service.",
          "users_impacted": "User impact details not clearly specified in RCA document",
          "root_causes": [
            "IdentifiedList the major issues that the analysis identified",
            "PR to migrate eslint to oxlint as part of performance improvement initiative",
            "Database connectivity issues",
            "Deployment failure"
          ],
          "analysis_quality": "low"
        },
        "quality_assessment": {
          "score": 81,
          "grade": "B",
          "feedback": "Solid RCA (81/100) with good foundation. Strong: Root Cause Analysis. Focus improvement on: Impact Assessment. Address: Business impact not addressed.",
          "strengths": [
            "Timeline Detection: Detailed timestamps provided",
            "Timeline Detection: Dedicated timeline section",
            "Timeline Detection: Detection methods described"
          ],
          "critical_gaps": [
            "Impact Assessment: Business impact not addressed"
          ]
        },
        "business_impact": {
          "impact_score": 3,
          "customer_count_affected": "User impact not specified",
          "revenue_impact_est": "Minimal: <$1K potential impact",
          "service_downtime_minutes": 0,
          "severity_justification": "Limited business impact"
        },
        "technical_analysis": {
          "root_cause_category": "Deployment Issue",
          "detection_time_minutes": 0,
          "resolution_time_minutes": 0,
          "technical_debt_level": "Low - Minor cleanup needed",
          "automation_score": 5
        }
      }
    },
    {
      "jira_data": {
        "ticket_key": "IR-373",
        "summary": "AQ/mobileapp/carehub Was inaccessible to members and coaches.",
        "priority": "P2",
        "created_date": "2025-07-22T11:18:58.718-0700",
        "status": "Root Cause Analysis",
        "description": null,
        "custom_fields": {
          "incident_urgency": "P2",
          "pods_engaged": "Access (EBB), Program Experience, Onboarding, Security",
          "reporter": null,
          "assignee": null
        },
        "comments": [],
        "metadata": {
          "jira_url": "https://hingehealth.atlassian.net/browse/IR-373",
          "labels": [],
          "component": null,
          "resolution": null
        }
      },
      "confluence_data": {
        "rca_url": "https://hingehealth.atlassian.net/wiki/spaces/RND/pages/1616052558/RCA+IR-373+Carehub+and+admin+panel+not+loading",
        "rca_available": true,
        "content_html": "<p><strong>Start date:</strong>&nbsp;July 22, 2025</p><p><strong>Close date:</strong>&nbsp;July 22, 2025</p><p><strong>Severity</strong>:&nbsp;P2</p><p /><p><strong>NOTE: that this doc is written within 2 weeks of the incident happening. After this is written a review is setup with senior tech leaders in the org.</strong></p><p><strong>Author</strong>: Yufei Fu</p><p>&nbsp;<strong>NOTE: Delete the instruction text in each section as you fill it out.</strong></p><h2>Client Facing Summary (if necessary):</h2><p>During the incident, members were unable to load the application. Per the authentication logs in Okta , as traffic was redirected there, the impact of this incident was <ac:inline-comment-marker ac:ref=\"44ecb016-410e-4f1b-b88c-d8cd5d4a8161\">173 users</ac:inline-comment-marker>. <ac:inline-comment-marker ac:ref=\"1e909c1c-d5d8-48d2-ada9-7326ef489987\">This is not an estimated number,  instead these are recorded as attempted authentications in Okta. </ac:inline-comment-marker></p><p>Pending : We are getting other numbers from people that might&rsquo;ve been affected from losing connectivity to backend services waiting on Data Science team. </p><h2>Exec Summary</h2><p>A change to Cloudflare origin rules briefly redirected all traffic to Retool instead of the intended target. Phoenix-bff, operations-hub-bff, airflow, were excluded. </p><h2>Impact</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"7aa30a3e-a672-4e33-b499-70af7919c220\"><ac:rich-text-body><p>Fill out the details of the impact of the incident starting with the end user/customer and then working from there to the business&nbsp; impact. Be as specific as you can (X customers impacted, Y transactions failed, $Z lost&hellip;Don't put a lot of details about how or why the incident happened, but just focus on the impact. Keep the why for later. For example, &quot;users could not log in for 30 mins because of auth service unavailability&quot; instead of &quot;one aws zone (us-east-02) had a drop in availability and our load balancing setup had a bug so any requests for log in going to us-east-02 failed for 30 mins&quot;.)</p></ac:rich-text-body></ac:structured-macro><ul><li><p><strong>Customer Impact: </strong>Services were not reachable for 10 minutes between 11:09AM to 11:19AM PST. </p></li><li><p><strong>Business Impact: </strong>Backend services were unreachable, which may have caused unexpected behavior on requests.</p></li><li><p><strong>Internal Impact: </strong>Internal apps behind the WAF were redirected to retool.</p></li></ul><h2>Timeline (Pacific time, with full timestamps)</h2><p><strong>2025-07-26 11:08AM  PST</strong>: Erroneous Cloudflare rule was deployed to onboard an application from the migration effort from F5. </p><p><strong>2025-07-26 11:09AM</strong>: Traffic started redirecting to the incorrect destination due to the deployed rule. </p><p><strong>2025-07-26 11:13AM</strong>: Reports from #it_support channel that Admin panel was not working. Followed by other reports of different apps being down. </p><p><strong>2025-07-26 11:18AM: </strong>Cloudflare rule reverted, and traffic was resuming correctly. </p><p><strong>2025-07-26 11:19AM: </strong>Monitor services for expected behavior and flow of network requests. Incident Mitigated</p><p><strong>2025-07-26 11:23: </strong>Confirmed and closed incident. </p><p>Specifically record, when <strong>Incident Detected/Reported </strong>and when <strong>Incident Mitigated</strong></p><h2>5 WHYs - Why did this incident happen?</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"12955489-8019-47d9-8c97-7af016b2c6c4\"><ac:rich-text-body><p>Start from how the incident was discovered and then work down from there. Note that there may be a few branches of 5 Whys for issues that have multiple root causes. Make sure you don&rsquo;t answer the question in the first why. Let the trail continue till you get to the root issue and then the root issue has to mandatorily have an action item/lesson learned. This should also result in answers to: What was the technical issue that caused the incident? What was the reason the incident wasn't mitigated sooner?</p></ac:rich-text-body></ac:structured-macro><ol start=\"1\"><li><p>Why:&nbsp; Internal tool traffic was redirected to Retool. External traffic (except the the services noted above) were also affected, Including the Hinge Health app.</p></li><li><p>Why:&nbsp;An incorrect Cloudflare rule was directly pushed to prod that redirected all traffic.</p></li><li><p>Why:&nbsp;In an attempt to rapidly onboard an application to Cloudflare, mistake was made by selecting &ldquo;Apply to all traffic&rdquo; instead of &ldquo;Apply to specific traffic&rdquo;.</p></li><li><p>Why:&nbsp;The usual Terraform approach had issues with the types and did not want to build, and would result in a longer delay to onboard the applications. The application needed to onboard quickly per business needs. </p></li><li><p>Why:&nbsp;We had a hybrid approach to management of Cloudflare due to the migration, and emergency change that took place for that migration. </p></li></ol><h2>Architectural Context</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"34f5b646-366a-495f-adc5-fe26ded13b55\"><ac:rich-text-body><p>Give background and architectural context of the service/data flow in question, this helps reader understand the whole picture.</p></ac:rich-text-body></ac:structured-macro><p>&nbsp;</p><h2>Root Cause Analysis</h2><h3>Major Problems Identified</h3><p>List the major issues that the analysis identified.</p><ul><li><p><strong>Immediate Cause</strong>: Manual changes. </p></li><li><p><strong>Contributing Factors</strong>: Unexpected expedited migration to Cloudflare from F5 that resulted in a hybrid of IaC and manual changes. </p></li><li><p><strong>Underlying Causes:</strong></p><ul><li><p><strong>Technical Infrastructure:</strong> NA</p></li><li><p><strong>Processes and Procedures</strong>: Completely move management of Cloudflare resources to Terraform, and terraform only. Create templates in IaC that disallow unwanted configurations, so this is not accidentally configured incorrectly. </p></li><li><p><strong>Human Factors</strong>: Poor decisioning as a result of attempting to move quickly. </p></li></ul></li></ul><h2>Lessons Learned</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"e4b3e239-f638-463b-8200-1d2aaffbb944\"><ac:rich-text-body><p>The lessons learned from the 5 Whys and potentially more get added as a list. Add a mandatory section on how Time to Detection and Time to Resolution can be cut in half.</p><p>Focus on how lessons learnt become a part of our dev/deploy/monitoring flow, what steps we have taken to integrate those lessons.</p></ac:rich-text-body></ac:structured-macro><h4>Actionable Insights:</h4><p>Summarize the key insights and lessons learned from the 5 Whys analysis. Focus on actionable recommendations and corrective actions that can be implemented to prevent similar incidents in the future. Include recommendations for process improvements, system enhancements, training needs, and any other measures deemed necessary based on the analysis.</p><ul><li><p>Align dev, stage, prod Terraform for Cloudflare to be the exact same.</p></li><li><p>Remove all permissions that can allow for manual changes, admin access only by request and approval.</p></li><li><p>Deploy of WAF configurations will be done via SRE team. </p></li></ul><h4>Time to Detection</h4><p>4 minutes. &nbsp;</p><h4>Time to Resolution</h4><p>&nbsp;10 minutes. </p><h2>Action Items</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"9576062f-8009-45fa-aab0-eb7253d62281\"><ac:rich-text-body><p>Action items from the 5 Whys section get added here. Every action item needs to have an owner, a JIRA ticket and an ETA.</p></ac:rich-text-body></ac:structured-macro><p>Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:</p><ul><li><p><strong>Technical Solutions:</strong> <ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"876b6418-ae5e-4efd-9432-2d8810233a41\"><ac:parameter ac:name=\"key\">SECIAM-3359</ac:parameter><ac:parameter ac:name=\"serverId\">b75f938a-9135-3e8a-b70b-9bd825e38cc0</ac:parameter><ac:parameter ac:name=\"server\">System Jira</ac:parameter></ac:structured-macro> - ETA <time datetime=\"2025-07-31\" /></p></li><li><p><strong>Process Improvements: </strong>Set specific maintenance windows for production level changes for Cloudflare. Per the following document <ac:link ac:card-appearance=\"inline\"><ri:page ri:space-key=\"SRECPUS\" ri:content-title=\"Scheduled Maintenance Procedure\" ri:version-at-save=\"9\" /><ac:link-body>Scheduled Maintenance Procedure</ac:link-body></ac:link> - Immediate</p></li></ul><p><ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"15f909a9-41d9-494d-9db0-37c89b4ac6a1\"><ac:parameter ac:name=\"key\">SECIAM-3598</ac:parameter><ac:parameter ac:name=\"serverId\">b75f938a-9135-3e8a-b70b-9bd825e38cc0</ac:parameter><ac:parameter ac:name=\"server\">System Jira</ac:parameter></ac:structured-macro> - <time datetime=\"2025-08-15\" /></p><ul><li><p><strong>Training and Education: </strong>NA</p></li><li><p><strong>Monitoring and Alerting:</strong>  <ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"1e689b7f-3e22-4760-9ba7-d1594716da0c\"><ac:parameter ac:name=\"key\">SECIAM-3542</ac:parameter><ac:parameter ac:name=\"serverId\">b75f938a-9135-3e8a-b70b-9bd825e38cc0</ac:parameter><ac:parameter ac:name=\"server\">System Jira</ac:parameter></ac:structured-macro> - <time datetime=\"2025-07-31\" /></p></li><li><p><strong>Testing Improvements:</strong>&nbsp; </p></li></ul><p>&nbsp;</p><hr /><h3>Post Mortem Notes:&nbsp;</h3>",
        "content_text": "Start date: July 22, 2025Close date: July 22, 2025Severity: P2NOTE: that this doc is written within 2 weeks of the incident happening. After this is written a review is setup with senior tech leaders in the org.Author: Yufei Fu NOTE: Delete the instruction text in each section as you fill it out.Client Facing Summary (if necessary):During the incident, members were unable to load the application. Per the authentication logs in Okta , as traffic was redirected there, the impact of this incident was 173 users. This is not an estimated number, instead these are recorded as attempted authentications in Okta. Pending : We are getting other numbers from people that might’ve been affected from losing connectivity to backend services waiting on Data Science team. Exec SummaryA change to Cloudflare origin rules briefly redirected all traffic to Retool instead of the intended target. Phoenix-bff, operations-hub-bff, airflow, were excluded. ImpactFill out the details of the impact of the incident starting with the end user/customer and then working from there to the business  impact. Be as specific as you can (X customers impacted, Y transactions failed, $Z lost…Don't put a lot of details about how or why the incident happened, but just focus on the impact. Keep the why for later. For example, \"users could not log in for 30 mins because of auth service unavailability\" instead of \"one aws zone (us-east-02) had a drop in availability and our load balancing setup had a bug so any requests for log in going to us-east-02 failed for 30 mins\".)Customer Impact: Services were not reachable for 10 minutes between 11:09AM to 11:19AM PST. Business Impact: Backend services were unreachable, which may have caused unexpected behavior on requests.Internal Impact: Internal apps behind the WAF were redirected to retool.Timeline (Pacific time, with full timestamps)2025-07-26 11:08AM PST: Erroneous Cloudflare rule was deployed to onboard an application from the migration effort from F5. 2025-07-26 11:09AM: Traffic started redirecting to the incorrect destination due to the deployed rule. 2025-07-26 11:13AM: Reports from #it_support channel that Admin panel was not working. Followed by other reports of different apps being down. 2025-07-26 11:18AM: Cloudflare rule reverted, and traffic was resuming correctly. 2025-07-26 11:19AM: Monitor services for expected behavior and flow of network requests. Incident Mitigated2025-07-26 11:23: Confirmed and closed incident. Specifically record, when Incident Detected/Reported and when Incident Mitigated5 WHYs - Why did this incident happen?Start from how the incident was discovered and then work down from there. Note that there may be a few branches of 5 Whys for issues that have multiple root causes. Make sure you don’t answer the question in the first why. Let the trail continue till you get to the root issue and then the root issue has to mandatorily have an action item/lesson learned. This should also result in answers to: What was the technical issue that caused the incident? What was the reason the incident wasn't mitigated sooner?Why:  Internal tool traffic was redirected to Retool. External traffic (except the the services noted above) were also affected, Including the Hinge Health app.Why: An incorrect Cloudflare rule was directly pushed to prod that redirected all traffic.Why: In an attempt to rapidly onboard an application to Cloudflare, mistake was made by selecting “Apply to all traffic” instead of “Apply to specific traffic”.Why: The usual Terraform approach had issues with the types and did not want to build, and would result in a longer delay to onboard the applications. The application needed to onboard quickly per business needs. Why: We had a hybrid approach to management of Cloudflare due to the migration, and emergency change that took place for that migration. Architectural ContextGive background and architectural context of the service/data flow in question, this helps reader understand the whole picture. Root Cause AnalysisMajor Problems IdentifiedList the major issues that the analysis identified.Immediate Cause: Manual changes. Contributing Factors: Unexpected expedited migration to Cloudflare from F5 that resulted in a hybrid of IaC and manual changes. Underlying Causes:Technical Infrastructure: NAProcesses and Procedures: Completely move management of Cloudflare resources to Terraform, and terraform only. Create templates in IaC that disallow unwanted configurations, so this is not accidentally configured incorrectly. Human Factors: Poor decisioning as a result of attempting to move quickly. Lessons LearnedThe lessons learned from the 5 Whys and potentially more get added as a list. Add a mandatory section on how Time to Detection and Time to Resolution can be cut in half.Focus on how lessons learnt become a part of our dev/deploy/monitoring flow, what steps we have taken to integrate those lessons.Actionable Insights:Summarize the key insights and lessons learned from the 5 Whys analysis. Focus on actionable recommendations and corrective actions that can be implemented to prevent similar incidents in the future. Include recommendations for process improvements, system enhancements, training needs, and any other measures deemed necessary based on the analysis.Align dev, stage, prod Terraform for Cloudflare to be the exact same.Remove all permissions that can allow for manual changes, admin access only by request and approval.Deploy of WAF configurations will be done via SRE team. Time to Detection4 minutes.  Time to Resolution 10 minutes. Action ItemsAction items from the 5 Whys section get added here. Every action item needs to have an owner, a JIRA ticket and an ETA.Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:Technical Solutions: SECIAM-3359b75f938a-9135-3e8a-b70b-9bd825e38cc0System Jira - ETA Process Improvements: Set specific maintenance windows for production level changes for Cloudflare. Per the following document Scheduled Maintenance Procedure - ImmediateSECIAM-3598b75f938a-9135-3e8a-b70b-9bd825e38cc0System Jira - Training and Education: NAMonitoring and Alerting: SECIAM-3542b75f938a-9135-3e8a-b70b-9bd825e38cc0System Jira - Testing Improvements:   Post Mortem Notes:",
        "title": "RCA: IR-373 Carehub and admin panel not loading",
        "page_metadata": {
          "space": "RND",
          "page_id": "1616052558",
          "estimated_word_count": 932
        }
      },
      "analysis_results": {
        "content_analysis": {
          "incident_summary": "A change to Cloudflare origin rules briefly redirected all traffic to Retool instead of the intended target. Phoenix-bff, operations-hub-bff, airflow, were excluded.",
          "users_impacted": "User impact details not clearly specified in RCA document",
          "root_causes": [
            "IdentifiedList the major issues that the analysis identified",
            "Manual changes",
            "Unexpected expedited migration to Cloudflare from F5 that resulted in a hybrid of IaC and manual changes",
            "NAProcesses and Procedures: Completely move management of Cloudflare resources to Terraform, and terraform only",
            "Create templates in IaC that disallow unwanted configurations, so this is not accidentally configured incorrectly"
          ],
          "analysis_quality": "low"
        },
        "quality_assessment": {
          "score": 92,
          "grade": "A",
          "feedback": "Exceptional RCA (92/100). Strongest area: Root Cause Analysis. Consider sharing as template. Minor enhancement opportunity: Timeline Detection.",
          "strengths": [
            "Timeline Detection: Detailed timestamps provided",
            "Timeline Detection: Dedicated timeline section",
            "Timeline Detection: Detection methods described"
          ],
          "critical_gaps": [
            "None identified"
          ]
        },
        "business_impact": {
          "impact_score": 5,
          "customer_count_affected": "173 users",
          "revenue_impact_est": "Medium: $5K-25K potential impact",
          "service_downtime_minutes": 30,
          "severity_justification": "Moderate business impact; P2 incident with high urgency; Service interruption (30 minutes)"
        },
        "technical_analysis": {
          "root_cause_category": "Process/Human Error",
          "detection_time_minutes": 0,
          "resolution_time_minutes": 0,
          "technical_debt_level": "Medium - Some improvements required",
          "automation_score": 3
        }
      }
    },
    {
      "jira_data": {
        "ticket_key": "IR-372",
        "summary": "Configurator 2.0 is not loading for few PTs",
        "priority": "P4",
        "created_date": "2025-07-21T09:41:54.504-0700",
        "status": "Root Cause Analysis",
        "description": "PTs were unable to load member ET Plans in the Configurator 2.0\n\n!55fba9aa-89fb-42cb-915a-160123722cd2 (1).mov|width=1534,height=702,alt=\"55fba9aa-89fb-42cb-915a-160123722cd2 (1).mov\"!",
        "custom_fields": {
          "incident_urgency": "P4",
          "pods_engaged": "ET Experience (ETE), Member Record",
          "reporter": null,
          "assignee": null
        },
        "comments": [],
        "metadata": {
          "jira_url": "https://hingehealth.atlassian.net/browse/IR-372",
          "labels": [],
          "component": null,
          "resolution": null
        }
      },
      "confluence_data": {
        "rca_url": "https://hingehealth.atlassian.net/wiki/spaces/RND/pages/1612907732/RCA+IR-372+Configurator+2.0+is+not+loading+for+few+PTs",
        "rca_available": true,
        "content_html": "<p><strong>Start date:</strong>&nbsp;July 17th 12:06 PM</p><p><strong>Close date:</strong>&nbsp;July 17th 1:38 PM</p><p><strong>Severity</strong>:&nbsp;P4</p><p><strong>Adverse Event level (1-4)</strong>: </p><p><strong>Author: </strong>Mike Wroblewski&nbsp;</p><h2>Exec Summary</h2><p><em><span style=\"color: rgb(151,160,175);\">[A paragraph about the incident, how it was detected, how it was resolved along with a sentence on the impact]</span></em></p><p>On July 17th Physical Therapists began reporting that they were unable to load Exercise Therapy plans in CareHub and an incident was created.  It was quickly determined that a recently deployed PR in the Exercise Service was the cause of the incident and the commit before the offending PR was deployed to production to resolve the issue.  An official revert of the offending PR was later deployed to all environments.  There was no direct member impact, but during the incident PTs were unable to view or customize Exercise Therapy plans for members in CareHub.</p><h2>Impact</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"636290d3-d91d-4075-bdff-c5ee7316ea06\"><ac:rich-text-body><p>Fill out the details of the impact of the incident starting with the end user/customer and then working from there to the business&nbsp; impact. Be as specific as you can (X customers impacted, Y transactions failed, $Z lost&hellip;Don't put a lot of details about how or why the incident happened, but just focus on the impact. Keep the why for later. For example, &quot;users could not log in for 30 mins because of auth service unavailability&quot; instead of &quot;one aws zone (us-east-02) had a drop in availability and our load balancing setup had a bug so any requests for log in going to us-east-02 failed for 30 mins&quot;.)</p></ac:rich-text-body></ac:structured-macro><ul><li><p><strong>Customer Impact: </strong>Physical Therapists</p></li><li><p><strong>Security breach: </strong>No</p></li><li><p><strong>Business Impact: </strong>PTs were unable to view or customize Exercise Therapy plans for members in CareHub</p></li><li><p><strong>Internal Impact: </strong>Same as above</p></li></ul><h2>Timeline (Pacific time, with full timestamps)</h2><p><em><strong>YYYY-MM-DD HH:MM</strong>: Timeline entries (everything from inception of the incident through incident closure. Err on the side providing more detail than less)</em></p><p><em>Specifically record, when <strong>Incident Detected/Reported </strong>and when <strong>Incident Mitigated</strong></em></p><p>2025-07-17 12:06 PM PT - PTs begin reporting issues with Exercise Therapy plans in CareHub<br />2025-07-17 1:13 PM PT - Incident Reported<br />2025-07-17 1:20 PM PT - Offending PR in Exercise Service identified and rollback begins<br />2025-07-17 1:38 PM PT - Rollback complete and incident is resolved</p><h2>5 WHYs - Why did this incident happen?</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"598fc86c-b2d9-4ddb-a559-a9181f09193c\"><ac:rich-text-body><p>Start from how the incident was discovered and then work down from there. Note that there may be a few branches of 5 Whys for issues that have multiple root causes. Make sure you don&rsquo;t answer the question in the first why. Let the trail continue till you get to the root issue and then the root issue has to mandatorily have an action item/lesson learned. This should also result in answers to: What was the technical issue that caused the incident? What was the reason the incident wasn't mitigated sooner?</p></ac:rich-text-body></ac:structured-macro><ol start=\"1\"><li><p><strong>Why:</strong>&nbsp; Was there an incident?</p><ol start=\"1\"><li><p><strong>Answer:</strong>  Because a PR was deployed that began sending <code>exercise.dosage: null;</code> values from the <code>GET/POST v1/exercises</code> endpoint.</p></li><li><p><strong>Why:</strong>  Did we make this change if it could brake CareHub?</p><ol start=\"1\"><li><p><strong>Answer: </strong> This change did not actually break our API contract for those endpoints.  A <code>null</code> value for <code>dosage</code> has been an acceptable value for quite some time but the UI in CareHub does not appear to be able to handle that value.</p></li><li><p><strong>Why:</strong>  Didn&rsquo;t Exercise Service know that the UI in CareHub could not handle a <code>null</code> value</p><ol start=\"1\"><li><p><strong>Answer:</strong>  This is an area of improvement that we should address - when making changes to existing APIs, we need to verify with our consumer(s) that those changes can be handled by them.  In this case, we relied upon the API contract, but that clearly wasn&rsquo;t enough.</p></li></ol></li><li><p><strong>Why:</strong>  Doesn&rsquo;t CareHub handle a <code>null</code> value for <code>dosage</code> if it&rsquo;s an acceptable response value from our API?</p><ol start=\"1\"><li><p><strong>Answer:</strong>  We will need to ask them - there is probably a good reason why, but it&rsquo;s unknown at this time.</p></li></ol></li></ol></li><li><p><strong>Why:</strong>  Wasn&rsquo;t this tested in DEV before being deployed to PROD?</p><ol start=\"1\"><li><p><strong>Answer:</strong>  The <code>GET/POST v1/exercises</code> endpoints were tested in DEV, but only by directly hitting them via Postman.  This gave a false sense of security because the API contract held up, as expected.  However, we should have gone one step further by logging into CareHub and navigating to the view that actually calls the endpoints - this would have exposed the issue before we deployed to PROD.</p></li></ol></li></ol></li></ol><h2>Architectural Context</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"cbf25f8e-99c3-48a0-b493-0a5a69c90bff\"><ac:rich-text-body><p>Give background and architectural context of the service/data flow in question, this helps reader understand the whole picture.</p></ac:rich-text-body></ac:structured-macro><p>&nbsp;</p><h2>Root Cause Analysis</h2><h3>Major Problems Identified</h3><p>List the major issues that the analysis identified.</p><ul><li><p><strong>Immediate Cause</strong>: <em><span style=\"color: rgb(151,160,175);\">Describe the specific event or trigger that initiated the incident. This could include software updates, hardware failures, configuration changes, etc.</span></em><br />A PR in the Exercise Service was deployed to PROD that began sending <code>exercise.dosage: null</code> values for any exercise that didn&rsquo;t have v2 Dosage data for the <code>GET/POST v1/exercises</code> endpoints.  CareHub&rsquo;s UI for Exercise Therapy plans failed to load due to the <code>null</code> values, causing this incident.</p></li><li><p><strong>Contributing Factors</strong>: <em><span style=\"color: rgb(151,160,175);\">Identify any additional factors that contributed to the incident, such as human error, lack of redundancy, insufficient monitoring, or inadequate testing procedures.</span></em></p></li><li><p><strong>Underlying Causes:</strong></p><ul><li><p><strong>Technical Infrastructure:</strong> <em><span style=\"color: rgb(151,160,175);\">Assess the state of the company's technical infrastructure, including network stability, server performance, and software dependencies.</span></em></p></li><li><p><strong>Processes and Procedures</strong>: <em><span style=\"color: rgb(151,160,175);\">Evaluate the effectiveness of existing processes and procedures related to deployment, maintenance, and troubleshooting. Identify any gaps or weaknesses.</span></em></p></li><li><p><strong>Human Factors</strong>: <em><span style=\"color: rgb(151,160,175);\">Consider the role of human factors, such as training deficiencies, communication issues, or decision-making processes, in the incident.</span></em><br />The changes to the <code>GET/POST v1/exercises</code> endpoints were tested in DEV - via Postman - before deploying to PROD, but a full e2e test would have caught this issue.  If we would have logged into CareHub and navigated to the view that hit those endpoints, this incident could have been avoided.</p></li></ul></li></ul><h2>Lessons Learned</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"020aa167-7222-41f7-b43c-a887f349b6eb\"><ac:rich-text-body><p>The lessons learned from the 5 Whys and potentially more get added as a list. Add a mandatory section on how Time to Detection and Time to Resolution can be cut in half.</p><p>Focus on how lessons learnt become a part of our dev/deploy/monitoring flow, what steps we have taken to integrate those lessons.</p></ac:rich-text-body></ac:structured-macro><h4>Actionable Insights:</h4><p>Summarize the key insights and lessons learned from the 5 Whys analysis. Focus on actionable recommendations and corrective actions that can be implemented to prevent similar incidents in the future. Include recommendations for process improvements, system enhancements, training needs, and any other measures deemed necessary based on the analysis.</p><ul><li><p /></li><li><p>&nbsp;</p></li></ul><h4>Time to Detection</h4><p>&nbsp;</p><h4>Time to Resolution</h4><p>&nbsp;</p><h2>Action Items</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"027c3bec-6fd3-43db-b617-5c7884d347aa\"><ac:rich-text-body><p>Action items from the 5 Whys section get added here. Every action item needs to have an owner, a JIRA ticket and an ETA.</p></ac:rich-text-body></ac:structured-macro><p>Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:</p><ul><li><p><strong>Technical Solutions:</strong> <em><span style=\"color: rgb(151,160,175);\">Implementing upgrades, patches, or enhancements to improve system reliability, scalability, and performance.</span></em></p></li><li><p><strong>Process Improvements: </strong><em><span style=\"color: rgb(151,160,175);\">Reviewing and updating processes for change management, incident response, and disaster recovery.</span></em></p></li><li><p><strong>Training and Education: </strong><span style=\"color: rgb(151,160,175);\">Providing training programs or workshops to enhance technical skills, promote best practices, and improve communication within teams.</span></p></li><li><p><strong>Monitoring and Alerting:</strong> <em><span style=\"color: rgb(151,160,175);\">Enhancing monitoring tools and alerting mechanisms to detect issues proactively and minimize downtime.</span></em></p></li><li><p><strong>Testing Improvements:</strong>&nbsp; When making changes to existing APIs, we should require a full e2e test in DEV to ensure our changes don&rsquo;t impact our consumer apps/services - even if these changes don&rsquo;t break the API contract.</p></li></ul><p>&nbsp;</p><hr /><h3>Post Mortem Notes:&nbsp;</h3>",
        "content_text": "Start date: July 17th 12:06 PMClose date: July 17th 1:38 PMSeverity: P4Adverse Event level (1-4): Author: Mike Wroblewski Exec Summary[A paragraph about the incident, how it was detected, how it was resolved along with a sentence on the impact]On July 17th Physical Therapists began reporting that they were unable to load Exercise Therapy plans in CareHub and an incident was created. It was quickly determined that a recently deployed PR in the Exercise Service was the cause of the incident and the commit before the offending PR was deployed to production to resolve the issue. An official revert of the offending PR was later deployed to all environments. There was no direct member impact, but during the incident PTs were unable to view or customize Exercise Therapy plans for members in CareHub.ImpactFill out the details of the impact of the incident starting with the end user/customer and then working from there to the business  impact. Be as specific as you can (X customers impacted, Y transactions failed, $Z lost…Don't put a lot of details about how or why the incident happened, but just focus on the impact. Keep the why for later. For example, \"users could not log in for 30 mins because of auth service unavailability\" instead of \"one aws zone (us-east-02) had a drop in availability and our load balancing setup had a bug so any requests for log in going to us-east-02 failed for 30 mins\".)Customer Impact: Physical TherapistsSecurity breach: NoBusiness Impact: PTs were unable to view or customize Exercise Therapy plans for members in CareHubInternal Impact: Same as aboveTimeline (Pacific time, with full timestamps)YYYY-MM-DD HH:MM: Timeline entries (everything from inception of the incident through incident closure. Err on the side providing more detail than less)Specifically record, when Incident Detected/Reported and when Incident Mitigated2025-07-17 12:06 PM PT - PTs begin reporting issues with Exercise Therapy plans in CareHub2025-07-17 1:13 PM PT - Incident Reported2025-07-17 1:20 PM PT - Offending PR in Exercise Service identified and rollback begins2025-07-17 1:38 PM PT - Rollback complete and incident is resolved5 WHYs - Why did this incident happen?Start from how the incident was discovered and then work down from there. Note that there may be a few branches of 5 Whys for issues that have multiple root causes. Make sure you don’t answer the question in the first why. Let the trail continue till you get to the root issue and then the root issue has to mandatorily have an action item/lesson learned. This should also result in answers to: What was the technical issue that caused the incident? What was the reason the incident wasn't mitigated sooner?Why:  Was there an incident?Answer: Because a PR was deployed that began sending exercise.dosage: null; values from the GET/POST v1/exercises endpoint.Why: Did we make this change if it could brake CareHub?Answer: This change did not actually break our API contract for those endpoints. A null value for dosage has been an acceptable value for quite some time but the UI in CareHub does not appear to be able to handle that value.Why: Didn’t Exercise Service know that the UI in CareHub could not handle a null valueAnswer: This is an area of improvement that we should address - when making changes to existing APIs, we need to verify with our consumer(s) that those changes can be handled by them. In this case, we relied upon the API contract, but that clearly wasn’t enough.Why: Doesn’t CareHub handle a null value for dosage if it’s an acceptable response value from our API?Answer: We will need to ask them - there is probably a good reason why, but it’s unknown at this time.Why: Wasn’t this tested in DEV before being deployed to PROD?Answer: The GET/POST v1/exercises endpoints were tested in DEV, but only by directly hitting them via Postman. This gave a false sense of security because the API contract held up, as expected. However, we should have gone one step further by logging into CareHub and navigating to the view that actually calls the endpoints - this would have exposed the issue before we deployed to PROD.Architectural ContextGive background and architectural context of the service/data flow in question, this helps reader understand the whole picture. Root Cause AnalysisMajor Problems IdentifiedList the major issues that the analysis identified.Immediate Cause: Describe the specific event or trigger that initiated the incident. This could include software updates, hardware failures, configuration changes, etc.A PR in the Exercise Service was deployed to PROD that began sending exercise.dosage: null values for any exercise that didn’t have v2 Dosage data for the GET/POST v1/exercises endpoints. CareHub’s UI for Exercise Therapy plans failed to load due to the null values, causing this incident.Contributing Factors: Identify any additional factors that contributed to the incident, such as human error, lack of redundancy, insufficient monitoring, or inadequate testing procedures.Underlying Causes:Technical Infrastructure: Assess the state of the company's technical infrastructure, including network stability, server performance, and software dependencies.Processes and Procedures: Evaluate the effectiveness of existing processes and procedures related to deployment, maintenance, and troubleshooting. Identify any gaps or weaknesses.Human Factors: Consider the role of human factors, such as training deficiencies, communication issues, or decision-making processes, in the incident.The changes to the GET/POST v1/exercises endpoints were tested in DEV - via Postman - before deploying to PROD, but a full e2e test would have caught this issue. If we would have logged into CareHub and navigated to the view that hit those endpoints, this incident could have been avoided.Lessons LearnedThe lessons learned from the 5 Whys and potentially more get added as a list. Add a mandatory section on how Time to Detection and Time to Resolution can be cut in half.Focus on how lessons learnt become a part of our dev/deploy/monitoring flow, what steps we have taken to integrate those lessons.Actionable Insights:Summarize the key insights and lessons learned from the 5 Whys analysis. Focus on actionable recommendations and corrective actions that can be implemented to prevent similar incidents in the future. Include recommendations for process improvements, system enhancements, training needs, and any other measures deemed necessary based on the analysis. Time to Detection Time to Resolution Action ItemsAction items from the 5 Whys section get added here. Every action item needs to have an owner, a JIRA ticket and an ETA.Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:Technical Solutions: Implementing upgrades, patches, or enhancements to improve system reliability, scalability, and performance.Process Improvements: Reviewing and updating processes for change management, incident response, and disaster recovery.Training and Education: Providing training programs or workshops to enhance technical skills, promote best practices, and improve communication within teams.Monitoring and Alerting: Enhancing monitoring tools and alerting mechanisms to detect issues proactively and minimize downtime.Testing Improvements:  When making changes to existing APIs, we should require a full e2e test in DEV to ensure our changes don’t impact our consumer apps/services - even if these changes don’t break the API contract. Post Mortem Notes:",
        "title": "RCA: IR-372 Configurator 2.0 is not loading for few PTs",
        "page_metadata": {
          "space": "RND",
          "page_id": "1612907732",
          "estimated_word_count": 1158
        }
      },
      "analysis_results": {
        "content_analysis": {
          "incident_summary": "[A paragraph about the incident, how it was detected, how it was resolved along with a sentence on the.",
          "users_impacted": "Services: Production endpoints affected",
          "root_causes": [
            "IdentifiedList the major issues that the analysis identified",
            "Describe the specific event or trigger that initiated the incident",
            "This could include software updates, hardware failures, configuration changes, etc",
            "A PR in the Exercise Service was deployed to PROD that began sending exercise",
            "Assess the state of the company's"
          ],
          "analysis_quality": "low"
        },
        "quality_assessment": {
          "score": 92,
          "grade": "A",
          "feedback": "Exceptional RCA (92/100). Strongest area: Root Cause Analysis. Consider sharing as template. Minor enhancement opportunity: Impact Assessment.",
          "strengths": [
            "Timeline Detection: Detailed timestamps provided",
            "Timeline Detection: Dedicated timeline section",
            "Timeline Detection: Detection methods described"
          ],
          "critical_gaps": [
            "None identified"
          ]
        },
        "business_impact": {
          "impact_score": 3,
          "customer_count_affected": "User impact not specified",
          "revenue_impact_est": "Minimal: <$500 potential impact",
          "service_downtime_minutes": 30,
          "severity_justification": "Limited business impact; Service interruption (30 minutes)"
        },
        "technical_analysis": {
          "root_cause_category": "Process/Human Error",
          "detection_time_minutes": 0,
          "resolution_time_minutes": 0,
          "technical_debt_level": "Medium - Some improvements required",
          "automation_score": 3
        }
      }
    },
    {
      "jira_data": {
        "ticket_key": "IR-370",
        "summary": "PHX Playlist Generation failing with \"Socket Hang Up\"",
        "priority": "P4",
        "created_date": "2025-07-17T11:58:06.750-0700",
        "status": "Root Cause Analysis",
        "description": "Latest release to asset-service caused high latency for one of the end point leading to failure to generate playlist between ~11:42 to 12:10 impacting 875 members.\n\nAt 12:10 – SRE(@aaron) bumpoed up the memory from 1 GB to 2GB, that mitigated the issue. \n\nAt 12:49 – [~accountid:712020:f149fcfe-1d0b-4073-bb5f-846ea50d55b8]  reverted last deployment to alleviate any risks. ",
        "custom_fields": {
          "incident_urgency": "P4",
          "pods_engaged": "Content Platform, SRE & Cloud Platform",
          "reporter": null,
          "assignee": null
        },
        "comments": [],
        "metadata": {
          "jira_url": "https://hingehealth.atlassian.net/browse/IR-370",
          "labels": [],
          "component": null,
          "resolution": null
        }
      },
      "confluence_data": {
        "rca_url": "https://hingehealth.atlassian.net/wiki/spaces/RND/pages/1608417691/RCA+IR-370+PHX+Playlist+Generation+failing+with+Socket+Hang+Up",
        "rca_available": true,
        "content_html": "<p><strong>Start date:</strong>&nbsp;2025-07-17  11:42 AM PT</p><p><strong>Close date:</strong>&nbsp;2025-07-17  12:49 PM PT</p><p><strong>Severity</strong>:&nbsp;P4</p><p><strong>Adverse Event level (1-4)</strong>: &nbsp;</p><p><strong>Author</strong>: <ac:link><ri:user ri:account-id=\"712020:db4ac70f-fa01-49f8-ad90-d522cbb23ed4\" ri:local-id=\"90c36fd4-2754-4874-9ee0-e94e7c50a737\" /></ac:link> <ac:link><ri:user ri:account-id=\"712020:a1188b75-2d2b-42d6-94c9-b7d4fe8d12d8\" ri:local-id=\"52004c4e-2fab-4650-b48e-df79182f3c1e\" /></ac:link> </p><h2>Exec Summary</h2><p>On July 17,  asset-service encountered high latency for one of the end point /assets/search leading to failure to generate playlist between ~11:42 to 12:10 impacting 875 members.</p><p>At 12:10 &ndash; SRE(@aaron) bumped up the memory from 1 GB to 2GB, that mitigated the issue.</p><p>At 12:49 &ndash; <ac:link><ri:user ri:account-id=\"712020:f149fcfe-1d0b-4073-bb5f-846ea50d55b8\" ri:local-id=\"5c9e7753-254a-446d-8a19-5d35d068d835\" /></ac:link>  reverted last deployment to alleviate any risks.</p><h2>Impact</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"ce6a189c-d6a3-42b4-b144-0f97ef186674\"><ac:rich-text-body><p>Fill out the details of the impact of the incident starting with the end user/customer and then working from there to the business&nbsp; impact. Be as specific as you can (X customers impacted, Y transactions failed, $Z lost&hellip;Don't put a lot of details about how or why the incident happened, but just focus on the impact. Keep the why for later. For example, &quot;users could not log in for 30 mins because of auth service unavailability&quot; instead of &quot;one aws zone (us-east-02) had a drop in availability and our load balancing setup had a bug so any requests for log in going to us-east-02 failed for 30 mins&quot;.)</p></ac:rich-text-body></ac:structured-macro><ul><li><p><strong>Customer Impact: </strong>875 members<strong> </strong></p></li><li><p><strong>Security breach: </strong>No</p></li><li><p><strong>Business Impact: </strong>Playlist was not generated for impacted members</p></li><li><p><strong>Internal Impact: </strong>Playlist was not generated for impacted members</p></li></ul><h2>Timeline (Pacific time, with full timestamps)</h2><p>2025-07-17  11:42 AM PT - asset-service encountered high latency for one of the end point /assets/search</p><p>2025-07-17  12:10 PM PT &ndash; SRE( <ac:link><ri:user ri:account-id=\"6140cfb6481377006bb0c883\" ri:local-id=\"7dcab585-06b7-43c0-8bc5-61086bab2c84\" /></ac:link>  ) bumped up the memory from 1 GB to 2GB, that mitigated the issue.</p><p>2025-07-17  12:49 PM PT &ndash;  Rollback complete and incident is resolved.</p><h2>5 WHYs - Why did this incident happen?</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"a82cacea-5dfa-4961-b470-7201c450ae1d\"><ac:rich-text-body><p>Start from how the incident was discovered and then work down from there. Note that there may be a few branches of 5 Whys for issues that have multiple root causes. Make sure you don&rsquo;t answer the question in the first why. Let the trail continue till you get to the root issue and then the root issue has to mandatorily have an action item/lesson learned. This should also result in answers to: What was the technical issue that caused the incident? What was the reason the incident wasn't mitigated sooner?</p></ac:rich-text-body></ac:structured-macro><ol start=\"1\"><li><p>Why:&nbsp; Was there an incident?</p><ol start=\"1\"><li><p>Answer: There was a sudden latency surge to the v1/assets/search endpoint. </p></li></ol></li><li><p>Why:&nbsp;Was there a code change to the endpoint ?</p><ol start=\"1\"><li><p>No code change was made. But v1/assets/search was consumed by phx, onboarding and configurator app/bff. There is a new consumer Carehub (care-team-tools-bff) in the list</p></li></ol></li><li><p>Why:&nbsp;Is new consumer a problem ?</p><ol start=\"1\"><li><p>Though there were no huge volume of requests from care-team-tools-bff, the latencies started after care-team-tools-bff started consuming asset-service endpoints and a pattern of 401 requests started coming up to the v1/assets/search endpoint.</p></li></ol></li><li><p>Why:&nbsp;were there Unauthorized requests ?</p><ol start=\"1\"><li><p>care-team-tools-bff uses admin token to hit the endpoint. They are valid tokens and hitting the asset-service in local/dev works fine</p></li><li><p>Seems for some reason, there is a lag in authenticating the admin tokens</p></li></ol></li><li><p>Why:&nbsp;was there a lag in auth</p><ol start=\"1\"><li><p>search endpoint was using auth guard that use strategies [<code>&quot;hh-jwt&quot; , &quot;auth-jwt&quot;, &quot;hinge-health-jwt&quot;</code>] in the mentioned order. Hence for a token to be validated for admin token it has to be checked against strategies before reaching hinge-health-jwt which can verify it. This adds a subtle delay to each requests. Now this delay in order of million requests slowly becomes observable latency. This keeps on increasing to the point where it throws 401 errors. </p></li></ol></li></ol><h2>Architectural Context</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"dabd2424-f7ef-4643-9d8d-082b56302eb5\"><ac:rich-text-body><p>Give background and architectural context of the service/data flow in question, this helps reader understand the whole picture.</p></ac:rich-text-body></ac:structured-macro><p>&nbsp;</p><h2>Root Cause Analysis</h2><p>Details in <a href=\"https://hingehealth.atlassian.net/wiki/spaces/RND/pages/1608417691/RCA+IR-370+PHX+Playlist+Generation+failing+with+Socket+Hang+Up#Root-Cause-Analysis\" data-card-appearance=\"inline\">https://hingehealth.atlassian.net/wiki/spaces/RND/pages/1608417691/RCA+IR-370+PHX+Playlist+Generation+failing+with+Socket+Hang+Up#Root-Cause-Analysis</a> </p><h3>Major Problems Identified</h3><p>List the major issues that the analysis identified.</p><ul><li><p><strong>Immediate Cause</strong>: Subtle latency building up due to new calls from the care-team-tools-bff until a breaking point</p></li><li><p><strong>Contributing Factors</strong>:  care-team-tools-bff making more calls to the assets/search and the guided-breathing apis with the admin jwt token. The token validation adds a certain degree of latency to the request as asset-service first tries to validate token with Member specific auth strategies like Okta and LegacyAuth, before trying the admin auth strategy (hinge-health-jwt)</p></li><li><p><strong>Underlying Causes:</strong></p><ul><li><p><strong>Technical Infrastructure: </strong>This was a hard issue to figure out, as there were no visible errors on care-team-tools-bff or asset-service. search endpoint being one of the high availability endpoints could handle more load. Problem here was the auth being a choking point. </p></li><li><p><strong>Processes and Procedures</strong>: Load testing with emphasis on auth should be considered.</p></li><li><p><strong>Human Factors</strong>: <em><span style=\"color: rgb(151,160,175);\">Consider the role of human factors, such as training deficiencies, communication issues, or decision-making processes, in the incident.</span></em></p></li></ul></li></ul><h2>Lessons Learned</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"593c0553-4bc0-418e-b9dd-6289b0d96b35\"><ac:rich-text-body><p>The lessons learned from the 5 Whys and potentially more get added as a list. Add a mandatory section on how Time to Detection and Time to Resolution can be cut in half.</p><p>Focus on how lessons learnt become a part of our dev/deploy/monitoring flow, what steps we have taken to integrate those lessons.</p></ac:rich-text-body></ac:structured-macro><h4>Actionable Insights:</h4><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"2f99ab11-035b-4e43-b4c4-6190acce48ef\"><ac:rich-text-body><p>Summarize the key insights and lessons learned from the 5 Whys analysis. Focus on actionable recommendations and corrective actions that can be implemented to prevent similar incidents in the future. Include recommendations for process improvements, system enhancements, training needs, and any other measures deemed necessary based on the analysis.</p></ac:rich-text-body></ac:structured-macro><p /><p>We have exposed new endpoints in asset-service specific for the care-team-tools-bff that validates token against admin strategy. These are clones of existing member facing endpoints. For the last 2 weeks of deploying the new endpoints, the issue has not resurfaced or no significant latency is noticed in datadog</p><h4>Time to Detection</h4><p>&nbsp;</p><h4>Time to Resolution</h4><p>&nbsp;</p><h2>Action Items</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"38979d96-11f4-43ec-8116-c681467265f4\"><ac:rich-text-body><p>Action items from the 5 Whys section get added here. Every action item needs to have an owner, a JIRA ticket and an ETA.</p></ac:rich-text-body></ac:structured-macro><p>Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:</p><ul><li><p><strong>Technical Solutions:</strong> <em><span style=\"color: rgb(151,160,175);\">Implementing upgrades, patches, or enhancements to improve system reliability, scalability, and performance.</span></em></p></li><li><p><strong>Process Improvements:</strong> On using existing APIs for a new feature, if there is an expectation on the scale, then the API owner should be consulted (may be tagged in the TDD)<strong>.</strong></p></li><li><p><strong>Training and Education: </strong><span style=\"color: rgb(151,160,175);\">Providing training programs or workshops to enhance technical skills, promote best practices, and improve communication within teams.</span></p></li><li><p><strong>Monitoring and Alerting:</strong> <em><span style=\"color: rgb(151,160,175);\">Enhancing monitoring tools and alerting mechanisms to detect issues proactively and minimize downtime.</span></em></p></li><li><p><strong>Testing Improvements: </strong>Newer features added should be stress tested.</p></li></ul><p>&nbsp;</p><hr /><h3>Post Mortem Notes:&nbsp;</h3>",
        "content_text": "Start date: 2025-07-17 11:42 AM PTClose date: 2025-07-17 12:49 PM PTSeverity: P4Adverse Event level (1-4):  Author: Exec SummaryOn July 17, asset-service encountered high latency for one of the end point /assets/search leading to failure to generate playlist between ~11:42 to 12:10 impacting 875 members.At 12:10 – SRE(@aaron) bumped up the memory from 1 GB to 2GB, that mitigated the issue.At 12:49 – reverted last deployment to alleviate any risks.ImpactFill out the details of the impact of the incident starting with the end user/customer and then working from there to the business  impact. Be as specific as you can (X customers impacted, Y transactions failed, $Z lost…Don't put a lot of details about how or why the incident happened, but just focus on the impact. Keep the why for later. For example, \"users could not log in for 30 mins because of auth service unavailability\" instead of \"one aws zone (us-east-02) had a drop in availability and our load balancing setup had a bug so any requests for log in going to us-east-02 failed for 30 mins\".)Customer Impact: 875 members Security breach: NoBusiness Impact: Playlist was not generated for impacted membersInternal Impact: Playlist was not generated for impacted membersTimeline (Pacific time, with full timestamps)2025-07-17 11:42 AM PT - asset-service encountered high latency for one of the end point /assets/search2025-07-17 12:10 PM PT – SRE( ) bumped up the memory from 1 GB to 2GB, that mitigated the issue.2025-07-17 12:49 PM PT – Rollback complete and incident is resolved.5 WHYs - Why did this incident happen?Start from how the incident was discovered and then work down from there. Note that there may be a few branches of 5 Whys for issues that have multiple root causes. Make sure you don’t answer the question in the first why. Let the trail continue till you get to the root issue and then the root issue has to mandatorily have an action item/lesson learned. This should also result in answers to: What was the technical issue that caused the incident? What was the reason the incident wasn't mitigated sooner?Why:  Was there an incident?Answer: There was a sudden latency surge to the v1/assets/search endpoint. Why: Was there a code change to the endpoint ?No code change was made. But v1/assets/search was consumed by phx, onboarding and configurator app/bff. There is a new consumer Carehub (care-team-tools-bff) in the listWhy: Is new consumer a problem ?Though there were no huge volume of requests from care-team-tools-bff, the latencies started after care-team-tools-bff started consuming asset-service endpoints and a pattern of 401 requests started coming up to the v1/assets/search endpoint.Why: were there Unauthorized requests ?care-team-tools-bff uses admin token to hit the endpoint. They are valid tokens and hitting the asset-service in local/dev works fineSeems for some reason, there is a lag in authenticating the admin tokensWhy: was there a lag in authsearch endpoint was using auth guard that use strategies [\"hh-jwt\" , \"auth-jwt\", \"hinge-health-jwt\"] in the mentioned order. Hence for a token to be validated for admin token it has to be checked against strategies before reaching hinge-health-jwt which can verify it. This adds a subtle delay to each requests. Now this delay in order of million requests slowly becomes observable latency. This keeps on increasing to the point where it throws 401 errors. Architectural ContextGive background and architectural context of the service/data flow in question, this helps reader understand the whole picture. Root Cause AnalysisDetails in https://hingehealth.atlassian.net/wiki/spaces/RND/pages/1608417691/RCA+IR-370+PHX+Playlist+Generation+failing+with+Socket+Hang+Up#Root-Cause-Analysis Major Problems IdentifiedList the major issues that the analysis identified.Immediate Cause: Subtle latency building up due to new calls from the care-team-tools-bff until a breaking pointContributing Factors: care-team-tools-bff making more calls to the assets/search and the guided-breathing apis with the admin jwt token. The token validation adds a certain degree of latency to the request as asset-service first tries to validate token with Member specific auth strategies like Okta and LegacyAuth, before trying the admin auth strategy (hinge-health-jwt)Underlying Causes:Technical Infrastructure: This was a hard issue to figure out, as there were no visible errors on care-team-tools-bff or asset-service. search endpoint being one of the high availability endpoints could handle more load. Problem here was the auth being a choking point. Processes and Procedures: Load testing with emphasis on auth should be considered.Human Factors: Consider the role of human factors, such as training deficiencies, communication issues, or decision-making processes, in the incident.Lessons LearnedThe lessons learned from the 5 Whys and potentially more get added as a list. Add a mandatory section on how Time to Detection and Time to Resolution can be cut in half.Focus on how lessons learnt become a part of our dev/deploy/monitoring flow, what steps we have taken to integrate those lessons.Actionable Insights:Summarize the key insights and lessons learned from the 5 Whys analysis. Focus on actionable recommendations and corrective actions that can be implemented to prevent similar incidents in the future. Include recommendations for process improvements, system enhancements, training needs, and any other measures deemed necessary based on the analysis.We have exposed new endpoints in asset-service specific for the care-team-tools-bff that validates token against admin strategy. These are clones of existing member facing endpoints. For the last 2 weeks of deploying the new endpoints, the issue has not resurfaced or no significant latency is noticed in datadogTime to Detection Time to Resolution Action ItemsAction items from the 5 Whys section get added here. Every action item needs to have an owner, a JIRA ticket and an ETA.Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:Technical Solutions: Implementing upgrades, patches, or enhancements to improve system reliability, scalability, and performance.Process Improvements: On using existing APIs for a new feature, if there is an expectation on the scale, then the API owner should be consulted (may be tagged in the TDD).Training and Education: Providing training programs or workshops to enhance technical skills, promote best practices, and improve communication within teams.Monitoring and Alerting: Enhancing monitoring tools and alerting mechanisms to detect issues proactively and minimize downtime.Testing Improvements: Newer features added should be stress tested. Post Mortem Notes:",
        "title": "RCA: IR-370 PHX Playlist Generation failing with \"Socket Hang Up\"",
        "page_metadata": {
          "space": "RND",
          "page_id": "1608417691",
          "estimated_word_count": 1003
        }
      },
      "analysis_results": {
        "content_analysis": {
          "incident_summary": "On July 17, asset-service encountered high latency for one of the end point /assets/search leading to failure to generate playlist between ~11:42 to 12:10.",
          "users_impacted": "875 members; Services: Production endpoints affected; Services: Onboarding services impacted",
          "root_causes": [
            "//hingehealth",
            "search endpoint being one of the high availability endpoints could handle more load",
            "Problem here was the auth being a choking point",
            "Subtle latency building up due to new calls from the care-team-tools-bff until a breaking point",
            "This was a hard issue to figure out, as there were no visible errors on care-team-tools-bff or asset-service"
          ],
          "analysis_quality": "low"
        },
        "quality_assessment": {
          "score": 95,
          "grade": "A",
          "feedback": "Exceptional RCA (95/100). Strongest area: Root Cause Analysis. Consider sharing as template. Minor enhancement opportunity: Action Items Prevention.",
          "strengths": [
            "Timeline Detection: Detailed timestamps provided",
            "Timeline Detection: Dedicated timeline section",
            "Timeline Detection: Detection methods described"
          ],
          "critical_gaps": [
            "None identified"
          ]
        },
        "business_impact": {
          "impact_score": 3,
          "customer_count_affected": "875 users",
          "revenue_impact_est": "Minimal: <$500 potential impact",
          "service_downtime_minutes": 30,
          "severity_justification": "Limited business impact; Service interruption (30 minutes)"
        },
        "technical_analysis": {
          "root_cause_category": "Process/Human Error",
          "detection_time_minutes": 0,
          "resolution_time_minutes": 0,
          "technical_debt_level": "High - Major refactoring needed",
          "automation_score": 4
        }
      }
    },
    {
      "jira_data": {
        "ticket_key": "IR-369",
        "summary": "Members are not able to login post registration ",
        "priority": "P2",
        "created_date": "2025-07-15T14:34:13.679-0700",
        "status": "In Progress",
        "description": "During registration in Auth0, the member's metadata was being replaced with an empty object. The user metadata should have included an \"HHUUID,\" but because it was missing, members' couldn't log in. This only impacted new registration",
        "custom_fields": {
          "incident_urgency": "P2",
          "pods_engaged": "Eligibility/Partner Interactions (ELIG), Enrollment, Onboarding, Responsible: Eligibility/Partner Integrations",
          "reporter": null,
          "assignee": null
        },
        "comments": [],
        "metadata": {
          "jira_url": "https://hingehealth.atlassian.net/browse/IR-369",
          "labels": [],
          "component": null,
          "resolution": null
        }
      },
      "confluence_data": {
        "rca_url": "https://hingehealth.atlassian.net/wiki/spaces/RND/pages/1603732409/RCA+IR-369+Members+are+not+able+to+login+post+registration",
        "rca_available": true,
        "content_html": "<p><strong>Start date:</strong>&nbsp;July 15th, 2:30 PM</p><p><strong>Close date:</strong> July 15th, 6:00 PM</p><p><strong>Severity</strong>:&nbsp;P2</p><p><strong>Adverse Event level (1-4)</strong>: &nbsp;<em>required for Enso, Global App, and Perifit</em></p><hr /><h2>Exec Summary</h2><p>An issue effecting only newly created accounts caused members inability to login to Phoenix app. The issue was quickly identified and resolved. This appears to have affected around 800 users&mdash; login functionality has been restored for those users.</p><h2>Impact</h2><ul><li><p><strong>Customer Impact: </strong>About <ac:inline-comment-marker ac:ref=\"9e7a37b6-8a96-497b-96b3-0526b56e57a8\">800 users</ac:inline-comment-marker> were unable to logon with prevented them from proceeding to movement assement in <ac:inline-comment-marker ac:ref=\"8d2b4338-31af-4d64-be65-c3f7154fa1e1\">Phoenix app</ac:inline-comment-marker>. This initial poor experience in onboarding could decrease members overall impression of our service and lead to reduced engagement</p></li><li><p><strong>Security breach: </strong>N/A</p></li><li><p><strong>Business Impact: </strong>Initial friction and difficulties in onboarding could have ramifications in members not completing initial exercise therapies reducing number of billable events.</p></li><li><p><strong>Internal Impact: </strong>N/A</p></li></ul><h2>Timeline (Pacific time, with full timestamps)</h2><ul><li><p><strong>2025-07-15 12:26 Pacific</strong> (<em>Incident Detected/Reported</em>) Fist slack message on <a href=\"https://hingehealth.slack.com/archives/C06VDAHF1AM/p1752607602991329\">#ping-engineering-public</a> informing us of issue where users were unable to login. Initial investigation by Robert Feist is correct:</p></li></ul><blockquote><p>Their Auth0 profiles are created but there is no Metadata for the members</p></blockquote><ul><li><p><strong>2025-07-15  14:27 Pacific</strong> After additional probing questions and troubleshooting, incident is declared</p></li><li><p><strong>2025-07-15 18:21 Pacific </strong><em>(Incident Mitigated</em>) Issue identified and resolve <a href=\"https://github.com/hinge-health/auth-service/pull/626\" data-card-appearance=\"inline\">https://github.com/hinge-health/auth-service/pull/626</a>  </p></li></ul><h2>5 WHYs - Why did this incident happen?</h2><ol start=\"1\"><li><p>Why:&nbsp;Users were unable to logon</p></li><li><p>Why: After account creating, backend was unable to retrieve <code>hh_uuid</code> associated with account (this is necessary for successful login)</p></li><li><p>Why:&nbsp;The <code>app_metadata.hh_uuid</code> values on auth0 was being overwritten during an<code>hh_user_updated</code> event</p></li><li><p>Why:&nbsp;In <a href=\"https://github.com/hinge-health/auth-service/pull/621\">solving for an &ldquo;eventual consistency&rdquo;</a> issue in auth0, we exposed a subsequent update that was using an undefined field to overwrite the contents of <code>app_metadata</code> </p></li><li><p>Why:&nbsp;<em>exactly.</em></p></li></ol><h2>Architectural Context</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"f1083b7b-f9ba-4d55-82c3-e5f1393c1c09\"><ac:rich-text-body><p>Give background and architectural context of the service/data flow in question, this helps reader understand the whole picture.</p></ac:rich-text-body></ac:structured-macro><p>&nbsp;Compare the old vs new workflow: <a href=\"https://github.com/hinge-health/auth-service/blob/main/src/modules/identity-management/identity-management.service.ts\">/modules/identity-management/identity-management.service.ts</a></p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"847\" ac:original-width=\"875\" ac:custom-width=\"true\" ac:alt=\"image-20250720-004157.png\" ac:width=\"760\"><ri:attachment ri:filename=\"image-20250720-004157.png\" ri:version-at-save=\"1\" /></ac:image><p>On left, the record wouldn&rsquo;t be found based on search query used. The whole process would end early. Subsequent requests would be fulfilled because we found the user based on the query.</p><p>The new implementation requests the record by ID directly. Before the update occurring in the illustration above, a stale current_user object appears to be being passed down. This is inconsequential considering <code>app_metadata</code> should not be being updated.</p><h2>Root Cause Analysis</h2><h3>Major Problems Identified</h3><p>List the major issues that the analysis identified.</p><ul><li><p><strong>Immediate Cause</strong>: <code>app_metadata</code> object was empty prevent members from logging in</p></li><li><p><strong>Contributing Factors</strong>: Not realizing that <code>app_metadata</code> was being updated in a spot that really didn&rsquo;t need to be updated.</p></li><li><p><strong>Underlying Causes:</strong></p><ul><li><p><strong>Technical Infrastructure:</strong> Solving for said eventual consistency problem exposed an issue where we were updating an object by spreading in an empty object effectively nullifying the critical <code>hh_uuid</code> and <code>hh_id</code> values (required for successful login). </p></li><li><p><strong>Processes and Procedures</strong>: We&rsquo;ve effectivel determined that under no circumstances should these very critical values in auth0&rsquo;s app_metadata ever be updated. This is exclusive domain of registration process. Any subsequent changes that should be required should be very specific.</p></li></ul></li></ul><h4>Time to Detection</h4><ul><li><p><ac:link><ri:user ri:account-id=\"61c24b9168926d00689e1694\" ri:local-id=\"e72040f8-24b6-4c76-be00-ad65cbe2ef12\" /></ac:link> first reported issue in <a href=\"https://hingehealth.slack.com/archives/C06VDAHF1AM/p1752607602991329\">#ping-engineering-public</a> at 2025-07-15T12:26:00</p><ul><li><p>support first reported issue from user:</p></li><li><p><a href=\"https://docs.google.com/spreadsheets/d/1yEPFkdzG54u-WcQf3RilCibDA2uQBH25KBzH1ltVWDQ/edit?gid=0#gid=0\">user&rsquo;s with reported issue</a></p><ul><li><p>these have since been confirmed resolved either <a href=\"https://hingehealth.atlassian.net/browse/PING-2061\">in backfill/sync operation</a> or by support itself</p></li></ul></li></ul></li></ul><h4>Time to Resolution</h4><ul><li><p>~4 hours from first reported in slack message to when first PR was merged in.</p></li><li><p>Final resolution &mdash; restoring effected user accounts (~800) took around 48 hours from first report) </p></li></ul><h2>Action Items</h2><p>Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:</p><ul><li><p><strong>Technical Solutions:</strong> </p><ul><li><p>Immediate fix:</p><ul><li><p><ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"5615109d-392a-4d29-934c-cfdf9fea2f0e\"><ac:parameter ac:name=\"key\">PING-2054</ac:parameter><ac:parameter ac:name=\"serverId\">b75f938a-9135-3e8a-b70b-9bd825e38cc0</ac:parameter><ac:parameter ac:name=\"server\">System Jira</ac:parameter></ac:structured-macro> </p></li></ul></li><li><p>(follow up) Remove updates to <code>app_metadata</code> outside registration:</p><ul><li><p><ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"05909cb2-4879-4f31-86f1-751b9b203b17\"><ac:parameter ac:name=\"key\">PING-2059</ac:parameter><ac:parameter ac:name=\"serverId\">b75f938a-9135-3e8a-b70b-9bd825e38cc0</ac:parameter><ac:parameter ac:name=\"server\">System Jira</ac:parameter></ac:structured-macro> </p></li></ul></li><li><p>Scripts to identify and update effected users: </p><ul><li><p><ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"e0791afa-a33d-46b1-a19d-73766efcbb98\"><ac:parameter ac:name=\"key\">PING-2061</ac:parameter><ac:parameter ac:name=\"serverId\">b75f938a-9135-3e8a-b70b-9bd825e38cc0</ac:parameter><ac:parameter ac:name=\"server\">System Jira</ac:parameter></ac:structured-macro> </p></li><li><p><ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"0d33909c-cee0-4a89-a992-4a0a690933c5\"><ac:parameter ac:name=\"key\">PING-2063</ac:parameter><ac:parameter ac:name=\"serverId\">b75f938a-9135-3e8a-b70b-9bd825e38cc0</ac:parameter><ac:parameter ac:name=\"server\">System Jira</ac:parameter></ac:structured-macro> </p></li></ul></li></ul></li><li><p><strong>Process Improvements: </strong></p><p>There are a few issues identified for improvement during this IR for considered. Expanding on that is out of scope of this IR but is actively being discussed within the PING team:<br />1. Duplicate account detection<br />2. Account linking process</p></li></ul><p>&nbsp;</p><hr /><h3>Post Mortem Notes:&nbsp;</h3>",
        "content_text": "Start date: July 15th, 2:30 PMClose date: July 15th, 6:00 PMSeverity: P2Adverse Event level (1-4):  required for Enso, Global App, and PerifitExec SummaryAn issue effecting only newly created accounts caused members inability to login to Phoenix app. The issue was quickly identified and resolved. This appears to have affected around 800 users— login functionality has been restored for those users.ImpactCustomer Impact: About 800 users were unable to logon with prevented them from proceeding to movement assement in Phoenix app. This initial poor experience in onboarding could decrease members overall impression of our service and lead to reduced engagementSecurity breach: N/ABusiness Impact: Initial friction and difficulties in onboarding could have ramifications in members not completing initial exercise therapies reducing number of billable events.Internal Impact: N/ATimeline (Pacific time, with full timestamps)2025-07-15 12:26 Pacific (Incident Detected/Reported) Fist slack message on #ping-engineering-public informing us of issue where users were unable to login. Initial investigation by Robert Feist is correct:Their Auth0 profiles are created but there is no Metadata for the members2025-07-15 14:27 Pacific After additional probing questions and troubleshooting, incident is declared2025-07-15 18:21 Pacific (Incident Mitigated) Issue identified and resolve https://github.com/hinge-health/auth-service/pull/626 5 WHYs - Why did this incident happen?Why: Users were unable to logonWhy: After account creating, backend was unable to retrieve hh_uuid associated with account (this is necessary for successful login)Why: The app_metadata.hh_uuid values on auth0 was being overwritten during anhh_user_updated eventWhy: In solving for an “eventual consistency” issue in auth0, we exposed a subsequent update that was using an undefined field to overwrite the contents of app_metadata Why: exactly.Architectural ContextGive background and architectural context of the service/data flow in question, this helps reader understand the whole picture. Compare the old vs new workflow: /modules/identity-management/identity-management.service.tsOn left, the record wouldn’t be found based on search query used. The whole process would end early. Subsequent requests would be fulfilled because we found the user based on the query.The new implementation requests the record by ID directly. Before the update occurring in the illustration above, a stale current_user object appears to be being passed down. This is inconsequential considering app_metadata should not be being updated.Root Cause AnalysisMajor Problems IdentifiedList the major issues that the analysis identified.Immediate Cause: app_metadata object was empty prevent members from logging inContributing Factors: Not realizing that app_metadata was being updated in a spot that really didn’t need to be updated.Underlying Causes:Technical Infrastructure: Solving for said eventual consistency problem exposed an issue where we were updating an object by spreading in an empty object effectively nullifying the critical hh_uuid and hh_id values (required for successful login). Processes and Procedures: We’ve effectivel determined that under no circumstances should these very critical values in auth0’s app_metadata ever be updated. This is exclusive domain of registration process. Any subsequent changes that should be required should be very specific.Time to Detection first reported issue in #ping-engineering-public at 2025-07-15T12:26:00support first reported issue from user:user’s with reported issuethese have since been confirmed resolved either in backfill/sync operation or by support itselfTime to Resolution~4 hours from first reported in slack message to when first PR was merged in.Final resolution — restoring effected user accounts (~800) took around 48 hours from first report) Action ItemsBased on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:Technical Solutions: Immediate fix:PING-2054b75f938a-9135-3e8a-b70b-9bd825e38cc0System Jira (follow up) Remove updates to app_metadata outside registration:PING-2059b75f938a-9135-3e8a-b70b-9bd825e38cc0System Jira Scripts to identify and update effected users: PING-2061b75f938a-9135-3e8a-b70b-9bd825e38cc0System Jira PING-2063b75f938a-9135-3e8a-b70b-9bd825e38cc0System Jira Process Improvements: There are a few issues identified for improvement during this IR for considered. Expanding on that is out of scope of this IR but is actively being discussed within the PING team:1. Duplicate account detection2. Account linking process Post Mortem Notes:",
        "title": "RCA: IR-369 Members are not able to login post registration",
        "page_metadata": {
          "space": "RND",
          "page_id": "1603732409",
          "estimated_word_count": 609
        }
      },
      "analysis_results": {
        "content_analysis": {
          "incident_summary": "An issue effecting only newly created accounts caused members inability to login to Phoenix app. The issue was quickly identified and resolved.",
          "users_impacted": "800 users; Services: Onboarding services impacted",
          "root_causes": [
            "IdentifiedList the major issues that the analysis identified",
            "This is exclusive domain of registration process",
            "Any subsequent changes that should be required should be very specific",
            "app_metadata object was empty prevent members from logging in",
            "Final resolution — restoring effected user accounts (~800) took around 48 hours from first report)"
          ],
          "analysis_quality": "low"
        },
        "quality_assessment": {
          "score": 84,
          "grade": "B",
          "feedback": "Solid RCA (84/100) with good foundation. Strong: Impact Assessment. Focus improvement on: Learning Knowledge Sharing. Address: Missing lessons learned section.",
          "strengths": [
            "Timeline Detection: Detailed timestamps provided",
            "Timeline Detection: Dedicated timeline section",
            "Timeline Detection: Detection methods described"
          ],
          "critical_gaps": [
            "Learning Knowledge Sharing: Missing lessons learned section"
          ]
        },
        "business_impact": {
          "impact_score": 6,
          "customer_count_affected": "800 users",
          "revenue_impact_est": "Low: $1K-10K potential impact",
          "service_downtime_minutes": 240,
          "severity_justification": "Significant business impact; P2 incident with high urgency; Extended downtime (240 minutes)"
        },
        "technical_analysis": {
          "root_cause_category": "External Dependency",
          "detection_time_minutes": 0,
          "resolution_time_minutes": 0,
          "technical_debt_level": "Medium - Some improvements required",
          "automation_score": 2
        }
      }
    },
    {
      "jira_data": {
        "ticket_key": "IR-368",
        "summary": "Members are impacted by app crash ",
        "priority": "P3",
        "created_date": "2025-07-14T16:31:55.754-0700",
        "status": "New Incident",
        "description": "\nWe have sentry report indicating 700+ members were impacted by app crash. \n\n[https://hingehealth.sentry.io/explore/discover/homepage/?dataset=errors&end=2025-07-09T06%3A53%3A45.000&field=title&field=project&field=user&field=level&field=boundaryError&name=All Errors&project=5173911&query=\"SPACE-1027 There are no exercise records in exercise list state to update\" title%3A\"Error%3A SPACE-1027 There are no exercise records in exercise list state to update\" project%3Aphoenix level%3Afatal boundaryError%3ATrue&queryDataset=error-events&sort=-title&start=2025-07-07T19%3A25%3A44.000&yAxis=count()|https://hingehealth.sentry.io/explore/discover/homepage/?dataset=errors&end=2025-07-09T06%3A53%3A45.000&field=title&field=project&field=user&field=level&field=boundaryError&name=All%20Errors&project=5173911&query=%22SPACE-1027%20There%20are%20no%20exercise%20records%20in%20exercise%20list%20state%20to%20update%22%20title%3A%22Error%3A%20SPACE-1027%20There%20are%20no%20exercise%20records%20in%20exercise%20list%20state%20to%20update%22%20project%3Aphoenix%20level%3Afatal%20boundaryError%3ATrue&queryDataset=error-events&sort=-title&start=2025-07-07T19%3A25%3A44.000&yAxis=count%28%29]\n",
        "custom_fields": {
          "incident_urgency": "P3",
          "pods_engaged": "Not Set",
          "reporter": null,
          "assignee": null
        },
        "comments": [],
        "metadata": {
          "jira_url": "https://hingehealth.atlassian.net/browse/IR-368",
          "labels": [],
          "component": null,
          "resolution": null
        }
      },
      "confluence_data": {
        "rca_url": "https://hingehealth.atlassian.net/wiki/spaces/RND/pages/1601405375/RCA+IR-368+Members+are+impacted+by+app+crash",
        "rca_available": true,
        "content_html": "<p><strong>Start date:</strong>&nbsp;7/7/2025</p><p><strong>Close date:</strong>&nbsp;7/9/2025</p><p><strong>Severity</strong>:&nbsp;</p><p><strong>Adverse Event level (1-4)</strong>: &nbsp;<em>required for Enso, Global App, and Perifit</em></p><p>Initial RCA Draft: <a href=\"https://docs.google.com/document/d/1fz7tC2yZ-HpbLtogNt0E_REQhF6fzMLpwxfsYV05Gho/edit?tab=t.0#heading=h.l6qqzv3ruynr\" data-card-appearance=\"inline\">https://docs.google.com/document/d/1fz7tC2yZ-HpbLtogNt0E_REQhF6fzMLpwxfsYV05Gho/edit?tab=t.0#heading=h.l6qqzv3ruynr</a> </p><p><strong><span style=\"color: rgb(0,102,68);\">NOTE: that this doc is written within 2 weeks of the incident happening. After this is written a review is setup with senior tech leaders in the org.</span></strong></p><p><strong>Author</strong>: <em><span style=\"color: rgb(151,160,175);\">The person writing the doc and going through the RCA in OE meetings. If there is more than one author, the first author is considered to be the primary point of contact.</span></em></p><p>&nbsp;</p><p><strong><span style=\"color: rgb(255,86,48);\">NOTE: Delete the instruction text in each section as you fill it out.</span></strong></p><h2><strong>Example of a well written RCA</strong> <a href=\"https://docs.google.com/document/d/1pP9aDigsCYf3WvPCSTEoGoipQbb-Ec1uDDkca1622MU/edit#heading=h.yih5usn5ed71\"><u>HingeHealth main_db Database Incident</u></a>&nbsp;</h2><h2>Client Facing Summary (if necessary):</h2><p>&nbsp;</p><h2>Exec Summary</h2><p>~700 members were impacted by deep link iterable experiment. These members were facing app crash issue. This was found by old alert (sentry?). Because of volume was low, team paid attention on the second day. </p><h2>Impact</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"d0c89c4f-df96-4b95-b521-1c7dd529cc89\"><ac:rich-text-body><p>Fill out the details of the impact of the incident starting with the end user/customer and then working from there to the business&nbsp; impact. Be as specific as you can (X customers impacted, Y transactions failed, $Z lost&hellip;Don't put a lot of details about how or why the incident happened, but just focus on the impact. Keep the why for later. For example, &quot;users could not log in for 30 mins because of auth service unavailability&quot; instead of &quot;one aws zone (us-east-02) had a drop in availability and our load balancing setup had a bug so any requests for log in going to us-east-02 failed for 30 mins&quot;.)</p></ac:rich-text-body></ac:structured-macro><ul><li><p><strong>Customer Impact: </strong><em><span style=\"color: rgb(151,160,175);\">(note who we consider the customer here; eg participant, client)</span></em></p></li><li><p><strong>Security breach: </strong><span style=\"color: rgb(151,160,175);\">/ PHI Data Leak if any (note- PHI Data cannot be added to this doc and RCA docs about security breaches should be access controlled - work with IT Sec on this</span></p></li><li><p><strong>Business Impact:</strong></p></li><li><p><strong>Internal Impact:</strong></p></li></ul><h2>Timeline (Pacific time, with full timestamps)</h2><p><em><strong>YYYY-MM-DD HH:MM</strong>: Timeline entries (everything from inception of the incident through incident closure. Err on the side providing more detail than less)</em></p><p><em>Specifically record, when <strong>Incident Detected/Reported </strong>and when <strong>Incident Mitigated</strong></em></p><h2>5 WHYs - Why did this incident happen?</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"aa77eb93-5d5a-4219-a7ab-9da980a08a5a\"><ac:rich-text-body><p>Start from how the incident was discovered and then work down from there. Note that there may be a few branches of 5 Whys for issues that have multiple root causes. Make sure you don&rsquo;t answer the question in the first why. Let the trail continue till you get to the root issue and then the root issue has to mandatorily have an action item/lesson learned. This should also result in answers to: What was the technical issue that caused the incident? What was the reason the incident wasn't mitigated sooner?</p></ac:rich-text-body></ac:structured-macro><ol start=\"1\"><li><p>Why:&nbsp; App was crashing for impacted members </p></li><li><p>Why:&nbsp;Deep link used in the iterable experiment was causing the crash. </p></li><li><p>Why: [Assumption] If users were on a specific screen or they have started a playlist and if they receive a deep link, it would crash.  Might apply to app version  - &lt;1.180 (This needs to validated) - Refer - <ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"40a51aef-b1bf-4566-9e23-4346d6823963\"><ac:parameter ac:name=\"key\">ETE-4758</ac:parameter><ac:parameter ac:name=\"serverId\">b75f938a-9135-3e8a-b70b-9bd825e38cc0</ac:parameter><ac:parameter ac:name=\"server\">System Jira</ac:parameter></ac:structured-macro> <ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"e0659b2c-bc6c-42e5-a399-fe45bfe70d9f\"><ac:parameter ac:name=\"key\">ETE-4758</ac:parameter><ac:parameter ac:name=\"serverId\">b75f938a-9135-3e8a-b70b-9bd825e38cc0</ac:parameter><ac:parameter ac:name=\"server\">System Jira</ac:parameter></ac:structured-macro>  </p></li><li><p>Why:&nbsp;GROWMO doesn&rsquo;t test the provided deep link in all possible envs/app versions.  Deep link testing process is not streamlined and need further revision. </p></li><li><p>Why:&nbsp;GROWMO should have one team to contact to test deep link before experiment launch </p></li><li><p>HOME- pod will be starting point moving forward. </p></li><li><p>We found that after deep link is released, it breaks because of other merges. We miss the regression testing. </p></li></ol><h2>Architectural Context</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"524008be-ede5-4a41-8157-b493759baa9e\"><ac:rich-text-body><p>Give background and architectural context of the service/data flow in question, this helps reader understand the whole picture.</p></ac:rich-text-body></ac:structured-macro><p>&nbsp;</p><h2><ac:inline-comment-marker ac:ref=\"eb9696a4-18f1-4053-a5f7-7418560d3669\">Root Cause Analysis</ac:inline-comment-marker> </h2><h3>Major Problems Identified</h3><p>List the major issues that the analysis identified.</p><ul><li><p><strong>Immediate Cause</strong>:</p><ul><li><p>An Iterable campaign was turned on before all the bugs for an experiment were fixed.</p></li><li><p>Additionally, the campaign accidentally applied to older versions of the app that definitely couldn&rsquo;t handle the new deep link that the campaign used.</p></li></ul></li><li><p><strong>Contributing Factors</strong>:</p><ul><li><p>Human error led to the campaign being misconfigured and turned on before devs/QA had fixed and signed off on the experimental code.</p></li></ul></li><li><p><strong>Underlying Causes:</strong></p><ul><li><p><strong>Technical Infrastructure:</strong></p><ul><li><p>Iterable doesn&rsquo;t seem to have a great way to target minimum app versions. You have to manually create a large blacklist of recent app versions.</p></li><li><p>Deep links have a systemic problem where they open their target screens on top of existing screens (keeping them mounted in the background). In some cases, this causes unexpected interactions and can even crash the app which is what happened in this case.</p><ul><li><p>We had put out a patch for this one deep link in the latest version of the app, but that didn&rsquo;t help the older versions.</p></li></ul></li></ul></li><li><p><strong>Processes and Procedures</strong>:</p><ul><li><p>Deep links don&rsquo;t have any controls for disabling them if a problem is found with them in older app versions. Right now the best we can do is to stop sending out a bad deep link but that doesn&rsquo;t help any links that have already been sent out via Iterable, email, coaches, etc.</p></li></ul></li><li><p><strong>Human Factors</strong>:</p><ul><li><p>We need to ensure that devs and QA sign off on experiments and deep links before they&rsquo;re released.</p></li></ul></li></ul></li></ul><h2>Lessons Learned</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"f7f1474c-7e9a-4f26-9af7-c9d9fda5a450\"><ac:rich-text-body><p>The lessons learned from the 5 Whys and potentially more get added as a list. Add a mandatory section on how Time to Detection and Time to Resolution can be cut in half.</p><p>Focus on how lessons learnt become a part of our dev/deploy/monitoring flow, what steps we have taken to integrate those lessons.</p></ac:rich-text-body></ac:structured-macro><h4>Actionable Insights:</h4><p>Summarize the key insights and lessons learned from the 5 Whys analysis. Focus on actionable recommendations and corrective actions that can be implemented to prevent similar incidents in the future. Include recommendations for process improvements, system enhancements, training needs, and any other measures deemed necessary based on the analysis.</p><ul><li><p /></li><li><p>&nbsp;</p></li></ul><h4>Time to Detection</h4><p>&nbsp;</p><h4>Time to Resolution</h4><p>&nbsp;</p><h2>Action Items</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"5b991cfc-ba07-443f-a158-5c1f50665cb1\"><ac:rich-text-body><p>Action items from the 5 Whys section get added here. Every action item needs to have an owner, a JIRA ticket and an ETA.</p></ac:rich-text-body></ac:structured-macro><p>Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:</p><ul><li><p><strong>Technical Solutions:</strong> </p><ul><li><p>We expanded alerts</p></li><li><p>We need better testing and regression </p></li></ul></li><li><p><strong>Process Improvements: </strong></p><ul><li><p>GROMO should have one point of contact from product team </p></li></ul></li><li><p><strong>Training and Education: </strong></p><ul><li><p>Document slack channels to check and announce errors. </p></li></ul></li><li><p><span style=\"color: rgb(151,160,175);\">Providing training programs or workshops to enhance technical skills, promote best practices, and improve communication within teams.</span></p></li><li><p><strong>Monitoring and Alerting:</strong> <em><span style=\"color: rgb(151,160,175);\">Enhancing monitoring tools and alerting mechanisms to detect issues proactively and minimize downtime.</span></em></p></li><li><p><strong>Testing Improvements:</strong>&nbsp; <em><span style=\"color: rgb(151,160,175);\">Address gaps in testing that allowed this incident to occur and will prevent this from happening again.</span></em></p></li></ul><p>&nbsp;</p><hr /><h3>Post Mortem Notes:&nbsp;</h3>",
        "content_text": "Start date: 7/7/2025Close date: 7/9/2025Severity: Adverse Event level (1-4):  required for Enso, Global App, and PerifitInitial RCA Draft: https://docs.google.com/document/d/1fz7tC2yZ-HpbLtogNt0E_REQhF6fzMLpwxfsYV05Gho/edit?tab=t.0#heading=h.l6qqzv3ruynr NOTE: that this doc is written within 2 weeks of the incident happening. After this is written a review is setup with senior tech leaders in the org.Author: The person writing the doc and going through the RCA in OE meetings. If there is more than one author, the first author is considered to be the primary point of contact. NOTE: Delete the instruction text in each section as you fill it out.Example of a well written RCA HingeHealth main_db Database Incident Client Facing Summary (if necessary): Exec Summary~700 members were impacted by deep link iterable experiment. These members were facing app crash issue. This was found by old alert (sentry?). Because of volume was low, team paid attention on the second day. ImpactFill out the details of the impact of the incident starting with the end user/customer and then working from there to the business  impact. Be as specific as you can (X customers impacted, Y transactions failed, $Z lost…Don't put a lot of details about how or why the incident happened, but just focus on the impact. Keep the why for later. For example, \"users could not log in for 30 mins because of auth service unavailability\" instead of \"one aws zone (us-east-02) had a drop in availability and our load balancing setup had a bug so any requests for log in going to us-east-02 failed for 30 mins\".)Customer Impact: (note who we consider the customer here; eg participant, client)Security breach: / PHI Data Leak if any (note- PHI Data cannot be added to this doc and RCA docs about security breaches should be access controlled - work with IT Sec on thisBusiness Impact:Internal Impact:Timeline (Pacific time, with full timestamps)YYYY-MM-DD HH:MM: Timeline entries (everything from inception of the incident through incident closure. Err on the side providing more detail than less)Specifically record, when Incident Detected/Reported and when Incident Mitigated5 WHYs - Why did this incident happen?Start from how the incident was discovered and then work down from there. Note that there may be a few branches of 5 Whys for issues that have multiple root causes. Make sure you don’t answer the question in the first why. Let the trail continue till you get to the root issue and then the root issue has to mandatorily have an action item/lesson learned. This should also result in answers to: What was the technical issue that caused the incident? What was the reason the incident wasn't mitigated sooner?Why:  App was crashing for impacted members Why: Deep link used in the iterable experiment was causing the crash. Why: [Assumption] If users were on a specific screen or they have started a playlist and if they receive a deep link, it would crash. Might apply to app version - <1.180 (This needs to validated) - Refer - ETE-4758b75f938a-9135-3e8a-b70b-9bd825e38cc0System Jira ETE-4758b75f938a-9135-3e8a-b70b-9bd825e38cc0System Jira Why: GROWMO doesn’t test the provided deep link in all possible envs/app versions. Deep link testing process is not streamlined and need further revision. Why: GROWMO should have one team to contact to test deep link before experiment launch HOME- pod will be starting point moving forward. We found that after deep link is released, it breaks because of other merges. We miss the regression testing. Architectural ContextGive background and architectural context of the service/data flow in question, this helps reader understand the whole picture. Root Cause Analysis Major Problems IdentifiedList the major issues that the analysis identified.Immediate Cause:An Iterable campaign was turned on before all the bugs for an experiment were fixed.Additionally, the campaign accidentally applied to older versions of the app that definitely couldn’t handle the new deep link that the campaign used.Contributing Factors:Human error led to the campaign being misconfigured and turned on before devs/QA had fixed and signed off on the experimental code.Underlying Causes:Technical Infrastructure:Iterable doesn’t seem to have a great way to target minimum app versions. You have to manually create a large blacklist of recent app versions.Deep links have a systemic problem where they open their target screens on top of existing screens (keeping them mounted in the background). In some cases, this causes unexpected interactions and can even crash the app which is what happened in this case.We had put out a patch for this one deep link in the latest version of the app, but that didn’t help the older versions.Processes and Procedures:Deep links don’t have any controls for disabling them if a problem is found with them in older app versions. Right now the best we can do is to stop sending out a bad deep link but that doesn’t help any links that have already been sent out via Iterable, email, coaches, etc.Human Factors:We need to ensure that devs and QA sign off on experiments and deep links before they’re released.Lessons LearnedThe lessons learned from the 5 Whys and potentially more get added as a list. Add a mandatory section on how Time to Detection and Time to Resolution can be cut in half.Focus on how lessons learnt become a part of our dev/deploy/monitoring flow, what steps we have taken to integrate those lessons.Actionable Insights:Summarize the key insights and lessons learned from the 5 Whys analysis. Focus on actionable recommendations and corrective actions that can be implemented to prevent similar incidents in the future. Include recommendations for process improvements, system enhancements, training needs, and any other measures deemed necessary based on the analysis. Time to Detection Time to Resolution Action ItemsAction items from the 5 Whys section get added here. Every action item needs to have an owner, a JIRA ticket and an ETA.Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:Technical Solutions: We expanded alertsWe need better testing and regression Process Improvements: GROMO should have one point of contact from product team Training and Education: Document slack channels to check and announce errors. Providing training programs or workshops to enhance technical skills, promote best practices, and improve communication within teams.Monitoring and Alerting: Enhancing monitoring tools and alerting mechanisms to detect issues proactively and minimize downtime.Testing Improvements:  Address gaps in testing that allowed this incident to occur and will prevent this from happening again. Post Mortem Notes:",
        "title": "RCA: IR-368 Members are impacted by app crash",
        "page_metadata": {
          "space": "RND",
          "page_id": "1601405375",
          "estimated_word_count": 1045
        }
      },
      "analysis_results": {
        "content_analysis": {
          "incident_summary": "Exec Summary~700 members were.",
          "users_impacted": "User impact details not clearly specified in RCA document",
          "root_causes": [
            "IdentifiedList the major issues that the analysis identified",
            "An Iterable campaign was turned on before all the bugs for an experiment were fixed",
            "Additionally, the campaign accidentally applied to older versions of the app that definitely couldn’t handle the new deep link that the campaign used",
            "Iterable doesn’t seem to have a great way to target minimum app versions",
            "You have to manually create a large blacklist of recent app versions"
          ],
          "analysis_quality": "low"
        },
        "quality_assessment": {
          "score": 91,
          "grade": "A",
          "feedback": "Exceptional RCA (91/100). Strongest area: Root Cause Analysis. Consider sharing as template. Minor enhancement opportunity: Timeline Detection.",
          "strengths": [
            "Timeline Detection: Dedicated timeline section",
            "Timeline Detection: Detection methods described",
            "Impact Assessment: User impact quantified with numbers"
          ],
          "critical_gaps": [
            "Timeline Detection: Lacks specific timestamps"
          ]
        },
        "business_impact": {
          "impact_score": 4,
          "customer_count_affected": "700 users",
          "revenue_impact_est": "Low: $1K-5K potential impact",
          "service_downtime_minutes": 30,
          "severity_justification": "Moderate business impact; Service interruption (30 minutes)"
        },
        "technical_analysis": {
          "root_cause_category": "Process/Human Error",
          "detection_time_minutes": 0,
          "resolution_time_minutes": 0,
          "technical_debt_level": "Medium - Some improvements required",
          "automation_score": 5
        }
      }
    },
    {
      "jira_data": {
        "ticket_key": "IR-367",
        "summary": "Pelvic trainer card missing for members",
        "priority": "P4",
        "created_date": "2025-07-10T10:38:10.279-0700",
        "status": "Closed",
        "description": "Members under {{Pelvic Hypo}} and {{Pelvic Urge}} indications are not seeing pelvic trainer card on legacy home",
        "custom_fields": {
          "incident_urgency": "P4",
          "pods_engaged": "Home-Library",
          "reporter": null,
          "assignee": null
        },
        "comments": [],
        "metadata": {
          "jira_url": "https://hingehealth.atlassian.net/browse/IR-367",
          "labels": [],
          "component": null,
          "resolution": null
        }
      },
      "confluence_data": {
        "rca_url": "https://hingehealth.atlassian.net/wiki/spaces/RND/pages/1594556816/RCA+IR-367+Pelvic+trainer+card+missing+for+members",
        "rca_available": true,
        "content_html": "<ac:layout><ac:layout-section ac:type=\"fixed-width\" ac:breakout-mode=\"default\"><ac:layout-cell><p><strong>Start date:</strong>&nbsp;<time datetime=\"2025-07-10\" /> </p><p><strong>Close date:</strong>&nbsp;<time datetime=\"2025-07-10\" /> </p><p><strong>Severity</strong>:&nbsp;Sub P4</p><p><strong>Author</strong>: <ac:link><ri:user ri:account-id=\"618c1661fba4d0006ad73cca\" ri:local-id=\"c58450a1-3553-4bf0-98cc-20f6619e4e72\" /></ac:link> <ac:link><ri:user ri:account-id=\"6287dc32f0302e0068bfe26d\" ri:local-id=\"8be525b9-1e4e-41d8-b482-a4f5da72c034\" /></ac:link> </p><h2>Exec Summary</h2><p>This bug involved the disappearance of the pelvic trainer card for members under the Pelvic Hypo and Pelvic Urge indications. This issue was detected on July 10, 2025, and was caused by a regression introduced in v1.180.</p><ac:structured-macro ac:name=\"note\" ac:schema-version=\"1\" ac:macro-id=\"4485e13c-6226-4f35-bf89-30327daa46e9\"><ac:rich-text-body><p>This never reached the threshold for an incident. This was a bug.</p></ac:rich-text-body></ac:structured-macro><h2>Impact</h2><ul><li><p><strong>Customer Impact: </strong>50 members reached out to member support. Product forecasted that potentially ~260 members may have been impacted based on Mixpanel reports:</p><ul><li><p>Members had to meet the following conditions to encounter the error:</p><ul><li><p><a href=\"https://mixpanel.com/s/4rDtWA\"> on simplified home AND had a pelvic trainer card viewed event in the last 3 months</a></p></li></ul></li></ul></li></ul><h2>Timeline (Pacific time, with full timestamps)</h2><p>2026-06-24 10:44: <a href=\"https://github.com/hinge-health/phoenix/pull/26907\">Commit</a> introduced. Testing gap between order card, setup, snd start card which failed to identify missing card in flow.</p><hr /></ac:layout-cell></ac:layout-section><ac:layout-section ac:type=\"three_equal\" ac:breakout-mode=\"default\"><ac:layout-cell><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"630\" ac:original-width=\"549\" ac:custom-width=\"true\" ac:alt=\"image (9).png\" ac:width=\"221\"><ri:attachment ri:filename=\"image (9).png\" ri:version-at-save=\"2\" /></ac:image><p><strong>This card was appearing.</strong></p></ac:layout-cell><ac:layout-cell><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"370\" ac:original-width=\"365\" ac:custom-width=\"true\" ac:alt=\"image (10).png\" ac:width=\"221\"><ri:attachment ri:filename=\"image (10).png\" ri:version-at-save=\"2\" /></ac:image><p><strong>This card was appearing.</strong></p></ac:layout-cell><ac:layout-cell><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"74\" ac:original-width=\"164\" ac:custom-width=\"true\" ac:alt=\"image (11).png\" ac:width=\"164\"><ri:attachment ri:filename=\"image (11).png\" ri:version-at-save=\"2\" /></ac:image><p><strong>This card was missing.</strong></p></ac:layout-cell></ac:layout-section><ac:layout-section ac:type=\"fixed-width\" ac:breakout-mode=\"default\"><ac:layout-cell><hr /><p>2026-07-10 10:42: <strong>Incident reported. </strong></p><p>2026-07-10 12:21: Home pod identified the breaking <a href=\"https://github.com/hinge-health/phoenix/pull/27321\">commit</a> and pushed a fix to remediate. <strong>Incident remediated.</strong></p><p>2026-07-11 05:47: Report that validation of a fix in alpha has uncovered two new regression issues CV-4992, HOME-3626. The new issues are caused by the 'homepod_simplified_home' feature flag that must be disabled for specific users based on user tags <code>pelvic_trainer_eligible, enabled_pelvic_trainer</code>. </p><p>2026-07-11 12:15 - <a href=\"https://github.com/hinge-health/phoenix/pull/27352\">PR</a> is approved to add tags to whitelist to exclude affected users.</p><p>2026-07-11 16:33 - Official QE sign-off for the 1.180.1 hotfix, confirming the resolution successful. </p><p>2026-07-12 09:08 - Release engineer announces that the 1.180.1 hotfix has been successfully pushed to 100% in the app stores. <strong>Incident resolved.</strong></p><h2>5 WHYs - Why did this incident happen?</h2><ol start=\"1\"><li><p>Why:&nbsp; Was there an incident?</p><ol start=\"1\"><li><p>Because Pelvic Trainer eligible members could not access their start card on home after completing setup.</p></li></ol></li><li><p>Why was this regression not caught earlier by our automated or manual testing?</p><ol start=\"1\"><li><p>Because the test plan pre RC did not include regression testing of the full, end-to-end Pelvic Trainer user journey specifically on the Legacy Home path.</p></li></ol></li><li><p> Why didn't the test plan mandate re-validating the full journey on Legacy Home? </p><ol start=\"1\"><li><p>Because our regression process was too narrowly scoped. The test flow for a user completing setup and returning to the home screen was not considered a mandatory regression check.</p></li></ol></li><li><p> Why was our regression process and test coverage insufficient?  </p><ol start=\"1\"><li><p>Test runbooks did not explicitly include coverage for known edge-case cohorts, such as unified users who gain access to the Pelvic Trainer, making them easy to miss.</p></li></ol></li><li><p>Why do our processes lack mandatory, holistic regression checks for all user cohorts and feature-flagged experiences?</p><ol start=\"1\"><li><p>We lack a formal policy that defines the minimum regression scope for a feature area when a hotfix is introduced. The decision of what to re-test is left to individual judgment rather than being guided by a system that maps feature dependencies and critical user journeys.</p></li></ol></li></ol><h2>Root Cause Analysis</h2><h3>Major Problems Identified</h3><ul><li><p><strong>Insufficient Regression Scope: </strong>The core issue was that the regression testing process for the pelvic trainer flow was too narrowly defined. It failed to re-validate the complete, end-to-end user journey for the affected feature.</p></li><li><p><strong>Omission of Edge-Case User Cohorts: </strong>Test plans and runbooks do not adequately account for all known user types, specifically missing the non wph unified users who can gain access to the Pelvic Trainer feature.</p></li><li><p><strong>Lack of Formal Policy for Test Scoping</strong>: The root cause is a systemic process failure. There is no formal policy that mandates a minimum scope for regression testing, leaving the critical decision of what to test to individual judgment, which is prone to error.</p></li></ul><h2>Lessons Learned</h2><h4>Actionable Insights:</h4><p>The core lesson from this incident is that our quality assurance process has a systemic vulnerability: we rely on individual judgment rather than formal policy to define the scope of regression testing. This incident was not caused by a single mistake but was the predictable outcome of a process that lacks the necessary rigor to handle the complexity of our application, which includes multiple user cohorts and feature-flagged experiences (Legacy vs. Simplified Home).</p><ol start=\"1\"><li><p>Narrowly Scoped Testing is High-Risk: Focusing hotfix testing only on the intended fix, without re-validating the full end-to-end user journey, creates significant blind spots where regressions can occur.</p></li><li><p>Implicit Knowledge is a Process Flaw: Our test plans fail to explicitly account for all user cohorts because this information is treated as implicit knowledge held by a few team members, not as a formal requirement in our test runbooks.</p></li><li><p>Untested Code Paths Are Unsafe Code Paths: Any major user experience controlled by a feature flag represents a distinct code path. Failing to test all active paths is equivalent to not testing the feature at all for that segment of users.</p></li></ol><h4>Time to Detection</h4><p>15 days, 23 hours, and 58 minutes</p><h4>Time to Resolution</h4><p>&nbsp;1h 39m</p><h2>Action Items</h2><p><strong>Create a Centralized User Cohort Directory</strong></p><ul><li><p>Action: Develop and maintain a central repository (e.g., a Confluence page) that clearly defines all major user cohorts, their attributes, and their critical user journeys.</p></li><li><p>Details: This directory will serve as a definitive reference for developers and QA, eliminating ambiguity and reducing reliance on a few individuals' knowledge.&nbsp;</p></li></ul></ac:layout-cell></ac:layout-section></ac:layout>",
        "content_text": "Start date:  Close date:  Severity: Sub P4Author: Exec SummaryThis bug involved the disappearance of the pelvic trainer card for members under the Pelvic Hypo and Pelvic Urge indications. This issue was detected on July 10, 2025, and was caused by a regression introduced in v1.180.This never reached the threshold for an incident. This was a bug.ImpactCustomer Impact: 50 members reached out to member support. Product forecasted that potentially ~260 members may have been impacted based on Mixpanel reports:Members had to meet the following conditions to encounter the error: on simplified home AND had a pelvic trainer card viewed event in the last 3 monthsTimeline (Pacific time, with full timestamps)2026-06-24 10:44: Commit introduced. Testing gap between order card, setup, snd start card which failed to identify missing card in flow.This card was appearing.This card was appearing.This card was missing.2026-07-10 10:42: Incident reported. 2026-07-10 12:21: Home pod identified the breaking commit and pushed a fix to remediate. Incident remediated.2026-07-11 05:47: Report that validation of a fix in alpha has uncovered two new regression issues CV-4992, HOME-3626. The new issues are caused by the 'homepod_simplified_home' feature flag that must be disabled for specific users based on user tags pelvic_trainer_eligible, enabled_pelvic_trainer. 2026-07-11 12:15 - PR is approved to add tags to whitelist to exclude affected users.2026-07-11 16:33 - Official QE sign-off for the 1.180.1 hotfix, confirming the resolution successful. 2026-07-12 09:08 - Release engineer announces that the 1.180.1 hotfix has been successfully pushed to 100% in the app stores. Incident resolved.5 WHYs - Why did this incident happen?Why:  Was there an incident?Because Pelvic Trainer eligible members could not access their start card on home after completing setup.Why was this regression not caught earlier by our automated or manual testing?Because the test plan pre RC did not include regression testing of the full, end-to-end Pelvic Trainer user journey specifically on the Legacy Home path. Why didn't the test plan mandate re-validating the full journey on Legacy Home? Because our regression process was too narrowly scoped. The test flow for a user completing setup and returning to the home screen was not considered a mandatory regression check. Why was our regression process and test coverage insufficient? Test runbooks did not explicitly include coverage for known edge-case cohorts, such as unified users who gain access to the Pelvic Trainer, making them easy to miss.Why do our processes lack mandatory, holistic regression checks for all user cohorts and feature-flagged experiences?We lack a formal policy that defines the minimum regression scope for a feature area when a hotfix is introduced. The decision of what to re-test is left to individual judgment rather than being guided by a system that maps feature dependencies and critical user journeys.Root Cause AnalysisMajor Problems IdentifiedInsufficient Regression Scope: The core issue was that the regression testing process for the pelvic trainer flow was too narrowly defined. It failed to re-validate the complete, end-to-end user journey for the affected feature.Omission of Edge-Case User Cohorts: Test plans and runbooks do not adequately account for all known user types, specifically missing the non wph unified users who can gain access to the Pelvic Trainer feature.Lack of Formal Policy for Test Scoping: The root cause is a systemic process failure. There is no formal policy that mandates a minimum scope for regression testing, leaving the critical decision of what to test to individual judgment, which is prone to error.Lessons LearnedActionable Insights:The core lesson from this incident is that our quality assurance process has a systemic vulnerability: we rely on individual judgment rather than formal policy to define the scope of regression testing. This incident was not caused by a single mistake but was the predictable outcome of a process that lacks the necessary rigor to handle the complexity of our application, which includes multiple user cohorts and feature-flagged experiences (Legacy vs. Simplified Home).Narrowly Scoped Testing is High-Risk: Focusing hotfix testing only on the intended fix, without re-validating the full end-to-end user journey, creates significant blind spots where regressions can occur.Implicit Knowledge is a Process Flaw: Our test plans fail to explicitly account for all user cohorts because this information is treated as implicit knowledge held by a few team members, not as a formal requirement in our test runbooks.Untested Code Paths Are Unsafe Code Paths: Any major user experience controlled by a feature flag represents a distinct code path. Failing to test all active paths is equivalent to not testing the feature at all for that segment of users.Time to Detection15 days, 23 hours, and 58 minutesTime to Resolution 1h 39mAction ItemsCreate a Centralized User Cohort DirectoryAction: Develop and maintain a central repository (e.g., a Confluence page) that clearly defines all major user cohorts, their attributes, and their critical user journeys.Details: This directory will serve as a definitive reference for developers and QA, eliminating ambiguity and reducing reliance on a few individuals' knowledge.",
        "title": "RCA: IR-367 Pelvic trainer card missing for members",
        "page_metadata": {
          "space": "RND",
          "page_id": "1594556816",
          "estimated_word_count": 804
        }
      },
      "analysis_results": {
        "content_analysis": {
          "incident_summary": "This bug involved the disappearance of the pelvic trainer card for members under the Pelvic Hypo and Pelvic Urge indications. This issue was detected on July 10, 2025, and was caused by a regression introduced in v1.",
          "users_impacted": "50 members reached out to member; 260 members; 260 members may have been",
          "root_causes": [
            "It failed to re-validate the complete, end-to-end user journey for the affected feature",
            "Lack of Formal Policy for Test Scoping: The root cause is a systemic process failure"
          ],
          "analysis_quality": "low"
        },
        "quality_assessment": {
          "score": 74,
          "grade": "B",
          "feedback": "Solid RCA (74/100) with good foundation. Strong: Communication Clarity. Focus improvement on: Action Items Prevention. Address: Actions focus on fixing rather than prevention.",
          "strengths": [
            "Timeline Detection: Detailed timestamps provided",
            "Timeline Detection: Dedicated timeline section",
            "Timeline Detection: Detection methods described"
          ],
          "critical_gaps": [
            "Action Items Prevention: Actions focus on fixing rather than prevention"
          ]
        },
        "business_impact": {
          "impact_score": 5,
          "customer_count_affected": "50 users",
          "revenue_impact_est": "Minimal: <$500 potential impact",
          "service_downtime_minutes": 1380,
          "severity_justification": "Moderate business impact; Extended downtime (1380 minutes)"
        },
        "technical_analysis": {
          "root_cause_category": "Code Bug",
          "detection_time_minutes": 0,
          "resolution_time_minutes": 0,
          "technical_debt_level": "Medium - Some improvements required",
          "automation_score": 4
        }
      }
    },
    {
      "jira_data": {
        "ticket_key": "IR-366",
        "summary": "Daily max adjusted workloads incorrectly set in Admin Panel",
        "priority": "P3",
        "created_date": "2025-07-09T23:32:50.016-0700",
        "status": "Closed",
        "description": "A bug was identified where all Care Team Members had their daily max adjusted workloads incorrectly set to 10, reverting from higher values previously set (e.g., PTs typically have a daily max workload between 250-300). This caused member capacity to be hit early and prevented members from being accepted into the program. The issue was resolved by adjusting the daily max capacity back to appropriate levels.",
        "custom_fields": {
          "incident_urgency": "P3",
          "pods_engaged": "Care Ops",
          "reporter": null,
          "assignee": null
        },
        "comments": [],
        "metadata": {
          "jira_url": "https://hingehealth.atlassian.net/browse/IR-366",
          "labels": [],
          "component": null,
          "resolution": null
        }
      },
      "confluence_data": {
        "rca_url": "https://hingehealth.atlassian.net/wiki/spaces/RND/pages/1593508234/RCA+IR-366+Daily+max+adjusted+workloads+incorrectly+set+in+Admin+Panel",
        "rca_available": true,
        "content_html": "<p><strong>Start date:</strong>&nbsp;<strong>2025-07-03(07:30 PM PDT)</strong></p><p><strong>Close date:</strong>&nbsp;<strong>2025-07-07(07:52 AM PDT)</strong></p><p><strong>Severity</strong>:&nbsp;P3</p><p><strong><span style=\"color: rgb(0,102,68);\">NOTE: that this doc is written within 2 weeks of the incident happening. After this is written a review is setup with senior tech leaders in the org.</span></strong></p><p><strong>Author</strong>: <ac:link><ri:user ri:account-id=\"712020:40a4bcd4-22c8-4afe-87d6-6372ce7180d2\" ri:local-id=\"9f18bf07-863b-44ec-a9e9-a8d26d208e33\" /></ac:link> <ac:link><ri:user ri:account-id=\"712020:3b8bb575-505e-4a7f-9457-2883e4848b6a\" ri:local-id=\"35717af9-fd1d-4559-a06e-51fb0342b26a\" /></ac:link> <br /></p><p>&nbsp;Client Facing Summary:</p><p>The issue involved, delayed members from being accepted into programs by pushing them into manual review.</p><h2>Exec Summary</h2><p><em><span style=\"color: rgb(151,160,175);\">A bug was identified where all Care Team Members had their daily max adjusted workloads incorrectly set to 10, reverting from higher values previously set (e.g., PTs typically have a daily max workload between 250-300). </span></em><br /><br /><em><span style=\"color: rgb(151,160,175);\">This caused member capacity to be hit early and prevented members from being accepted into the program. </span></em><br /><br /><em><span style=\"color: rgb(151,160,175);\">The issue was resolved by adjusting the daily max capacity back to appropriate levels.</span></em><br /></p><p><em><span style=\"color: rgb(151,160,175);\">The manual review process was being triggered more than usual due to a technical issue and the unavailability of onboarding specialists during the holiday(04th July 2025).</span></em></p><h2>Impact</h2><ul><li><p><strong>Customer Impact: </strong></p><ul><li><p>1772 members went to manual review due to no MES available between July 3, 2025 7:05 PM PDT and July 7, 2025 7:52 AM PDT. </p></li><li><p>These members were all assigned to a MES by 07th July morning (PDT), as well as some assigned over the weekend as caps naturally reset.</p></li><li><p>These members would be waiting until manual review is done.</p></li></ul></li><li><p><strong>Security breach: </strong>None</p></li><li><p><strong>Business Impact: </strong><br /><br /><strong>Due to a delay in manual review processing</strong>, we currently have <strong>a backlog of 622 pending acceptances (approximately $500K in credits)</strong>. These <strong>credits will be applied on the date of acceptance</strong> post manual review.  <strong><span style=\"background-color: rgb(211,241,167);\"><del>This reflects a delay in crediting, not a loss</del></span></strong><span style=\"background-color: rgb(211,241,167);\"><del>.   </del></span><strong><span style=\"background-color: rgb(254,222,200);\"><ac:inline-comment-marker ac:ref=\"dd1db657-ce64-499a-af85-87e1f1922d55\">Latest information on the loss - from July 4th to 7th, 2219 Manual Review users, amounting to loss of $246k</ac:inline-comment-marker></span></strong><br /><br /><strong>Around ~1K applications that got accepted later than usual</strong>, with a delay of more than a day.<br /><br />Updated trends for Acceptance rate, where we can see that the <strong>backlogged AQs have been resolved now</strong> with <strong>the acceptance rate around the 85%+ mark</strong> (<em>which is around the normal mark if we see last one/two week data</em>).<br /><br /></p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"551\" ac:original-width=\"1014\" ac:custom-width=\"true\" ac:alt=\"image-20250718-035812.png\" ac:width=\"736\"><ri:attachment ri:filename=\"image-20250718-035812.png\" ri:version-at-save=\"1\" /></ac:image><p><br /></p></li><li><p><strong>Internal Impact: </strong>Yet to Ascertain</p></li></ul><h2>Timeline (Pacific time, with full timestamps)</h2><p><strong>Incident Slack </strong><a href=\"https://hingehealth.enterprise.slack.com/archives/C094W0TJ49X\"><strong><u>channel</u></strong></a><strong> Discussions</strong> -</p><table data-table-width=\"760\" data-layout=\"default\" ac:local-id=\"13745987-a445-424b-a886-39b01283ef9c\"><colgroup><col style=\"width: 225.0px;\" /><col style=\"width: 166.0px;\" /><col style=\"width: 366.0px;\" /></colgroup><tbody><tr><td><p><strong>Timestamp(PDT)</strong></p></td><td><p><strong>Author</strong></p></td><td><p><strong>Message</strong></p></td></tr><tr><td><p>2025-07-05 01:03 PM PDT</p></td><td><p>Jonathan Westfield</p></td><td><p>hey natasha - wanted to share a quick heads up on some data anomalies we observed from yesterday..</p><p>.&bull; we observed a really low accepted member number at 247. even with the holiday we would've expected higher.</p><ul><li><p><strong>looking a little closer, we had 748 aq completes [more in-line with expectations], which means a ~33% accept rate, very low</strong></p></li><li><p><strong>it appears that two things have happened&hellip;</strong></p></li></ul><p><br /><strong> &nbsp; ◦ we sent a much higher % of users to manual review [60% compared to a 10-20% norm]</strong></p><p><strong>&nbsp; ◦ due to the holiday, none of the manual review folks have been reviewed yet</strong></p><ul><li><p>if this is all true, then yesterday was a technical issue? based on the partial of today, it looks like it was just yesterday, but lets check tomorrow</p></li><li><p><strong>all those users sent to manual review should ultimately be reviewed and a large part accepted, but let's check monday [assuming thats when folks sign back on after the holiday to review]</strong></p></li><li><p><strong>the shortfall as of now is about 622 acceptances [~$500K bookings]. I'm assuming they won't get backdated to July 4th once reviewed and accepted, but instead be credited to the date of the manual review</strong></p></li></ul></td></tr><tr><td><p>2025-07-05 01:33 PM PDT</p></td><td><p>Natasha Udpa</p></td><td><p><a href=\"https://app.mode.com/hinge_health/reports/a30af0d30211/runs/f85d4816af8a\">https://app.mode.com/hinge_health/reports/a30af0d30211/runs/f85d4816af8a</a> - <strong>this looks like there aren't onboarding specialists to support. An anomaly in comparison to earlier dates.</strong> We should follow up or add Hannah Yelton and Austin Ord to the thread - late last year we expanded the pool to avoid running into this issue so I'm surprised it's coming up again</p></td></tr><tr><td><p>2025-07-06 06:53 AM PDT</p><p /></td><td><p>Jonathan Westfield</p></td><td><p><strong>Looks like same issue continued into yesterday contrary to what I thought the partial said yesterday [i.e. much lower than expected new accepted members at 269 due to most users being sent to manual review, &gt;50%]</strong>. I assume the same will happen today, Sunday, which will cause a non-trivial forecast miss, so will add the forecast team</p></td></tr><tr><td><p>2025-07-07 09:36 AM PDT</p></td><td><p>Hannah Yelton</p></td><td><p>Catching up from last week - I'll check with the team why we ran into this issue on Friday exactly but we usually see this issue when there is a high number of sick call outs (which often happens around holidays).And the reason caps exist in general is because the assignment engine (how member are assigned to care team members) will flood MESs with too many members if we don't have some sort of caps in place since it's still a somewhat basic assignment tool. Saisundar (the new CareOps PM) is working on improving this logic to avoid this from happening but the tooling has not been updated yet. If we don't have caps in place, a MES might get a thousand members in one day whereas their peers would get none. The caps help us control that.</p></td></tr><tr><td><p>2025-07-07 09:55 AM PDT</p></td><td><p>Hannah Yelton</p></td><td><p>Just heard back from the team - Sounds like there was a testing error that led to this issue.</p><p>Bringing in Sunil / Saisundar into this thread as well. I know we've chatted about permissions before / the risk of anyone with Super access accidentally adjusting the DE tabs, but sharing this example of the larger impact when accidents happen.</p><p><a href=\"https://hingehealth.slack.com/archives/CDST58LR0/p1751904022615939\" data-card-appearance=\"inline\">https://hingehealth.slack.com/archives/CDST58LR0/p1751904022615939</a> </p></td></tr><tr><td><p>2025-07-07 11:38 AM PDT</p></td><td><p>Hannah Yelton</p></td><td><p><strong>Looks like the caps were adjusted to the lower value on July 3, 2025 7:05 PM PDT and then corrected back on July 7, 2025 7:52 AM PDT.</strong> <strong>Some members were getting through, but since caps were lower those days, members were sent to manual review.</strong>From what we can tell from our side, we set the caps are their normal levels and those numbers were then accidentally adjusted by eng while testing.Sunil I think in terms of preventing this in the future, we need some sort of access controls in place to ensure production values are not changed without DE approval.</p></td></tr><tr><td><p>2025-07-07 11:47 AM PDT</p></td><td><p>Natasha Udpa</p></td><td><p>Thanks Hannah - Is there any dashboarding / alerting in place ?</p></td></tr><tr><td><p>2025-07-07 11:49 AM PDT</p><p /></td><td><p>Hannah Yelton</p></td><td><p>There is the #clinical-alerts channel, but it's pretty hard to follow / messy so not a great source of alerting. And then in general (during working hours), Member Ops and DE both keep an eye on manual review.</p></td></tr><tr><td><p>2025-07-07 05:31 PM PDT</p></td><td><p>Henrique De Saboia</p></td><td><p>Thank you Hannah. Sunil Please let us know how we can prevent this from happening from a development perspective.Hannah Yelton Do you have an impact analysis of the number of member who were impacted and had their onboarding delayed?</p></td></tr><tr><td><p>2025-07-07 07:17 PM PDT</p></td><td><p>Hannah Yelton</p></td><td><p><strong>Yes, agreed.</strong></p><p><strong>Member Ops shared a dashboard - Looks like 1772 members went to manual review due to no MES available between 7/3 and 7/7. These members were all assigned to a MES by this morning, as well as some assigned over the weekend as caps naturally reset.</strong></p><p>@Sunil</p><p>&nbsp;I'm not sure what the engineer was testing / how they were able to edit the values, but I think it's worth digging into that.</p></td></tr></tbody></table><h2><strong>Summarising the above chats containing data with screenshots</strong></h2><p><strong>Observed a really low accepted member number at 247. even with the holiday we would've expected higher.</strong></p><p><strong>Looking a little closer, we had 748 aq completes [more in-line with expectations], which means a ~33% accept rate, very low.</strong></p><p><strong>All those users sent to manual review should ultimately be reviewed and a large part accepted, </strong>but let's check monday [assuming thats when folks sign back on after the holiday to review]</p><p><strong>The shortfall as of now is about 622 acceptances [~$500K bookings].</strong> I'm assuming they won't get backdated to July 4th once reviewed and accepted, <strong>but instead be credited to the date of the manual review</strong></p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"591\" ac:original-width=\"1600\" ac:custom-width=\"true\" ac:width=\"644\" ac:src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXePt3iT-N7cSlPs1iUIEiBcvPf8EE5kbRqSt7nEA8uAOriU81uMJfX_LlkhGRTDJ-X_9ZO8h65U0yiUhrPEpvF-UheNyhHUBwPacan4ivZdUfp5wg45KwQ__WYvLqwhvSw557nfuA?key=YkwSMlVdrxF9dER1hOSrLQ\"><ri:url ri:value=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXePt3iT-N7cSlPs1iUIEiBcvPf8EE5kbRqSt7nEA8uAOriU81uMJfX_LlkhGRTDJ-X_9ZO8h65U0yiUhrPEpvF-UheNyhHUBwPacan4ivZdUfp5wg45KwQ__WYvLqwhvSw557nfuA?key=YkwSMlVdrxF9dER1hOSrLQ\" /></ac:image><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"547\" ac:original-width=\"1600\" ac:custom-width=\"true\" ac:width=\"662\" ac:src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXd-MDDHfpBe2N8dNTELDjEj_1yKQfKJNA1zVPdzpcMIr11KnHOupx2c50xU35rPNC2ubQvS0Q_E6AAA2IHTfwSJRrJKNcoxRhDf3G3mfa2YOuJIKPODnITrblDEDu2JYdAQWA7a4Q?key=YkwSMlVdrxF9dER1hOSrLQ\"><ri:url ri:value=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXd-MDDHfpBe2N8dNTELDjEj_1yKQfKJNA1zVPdzpcMIr11KnHOupx2c50xU35rPNC2ubQvS0Q_E6AAA2IHTfwSJRrJKNcoxRhDf3G3mfa2YOuJIKPODnITrblDEDu2JYdAQWA7a4Q?key=YkwSMlVdrxF9dER1hOSrLQ\" /></ac:image><p><em>Specifically record, when <strong>Incident Detected/Reported </strong>and when <strong>Incident Mitigated</strong></em><br /></p><p>Incident Reported - July 5, 2025 1:05 PM PDT<br />Incident Mitigated - July 7, 2025 7:52 AM PDT<br /></p><h2>5 WHYs - Why did this incident happen?</h2><ol start=\"1\"><li><p><strong>Why:&nbsp; Was there an incident?</strong><br /><br />An engineer, while testing a feature, unintentionally performed a bulk update for &ldquo;Max Adjusted Workloads&rdquo;, updating all CTMs workload in the production Admin Panel, mistakenly believing they were working within a development environment.<br /></p></li><li><p><strong>Why:&nbsp;Why was it possible to make such a change in production without preventative checks?</strong><br /><br />The Admin Panel allows user with (Super and Staff role) admin access to perform bulk updates to critical workload settings without additional confirmation steps or warnings, making it possible for an accidental change to impact all Care Team Member records at once.<br /></p></li><li><p><strong>Why:&nbsp;Why did the system not detect or flag this unusual change?</strong><br /></p><p>There were no automated alerts, monitoring, or validation checks implemented to detect bulk updates to Max Adjusted Workloads or to verify the reasonableness of the new values (e.g., a daily max of 10, which is significantly below the norm).<br /></p></li><li><p><strong>Why:&nbsp;Why was the change not immediately noticed and reverted?</strong><br /><br />The UI did not reflect that a mass update had occurred or who performed it, making it difficult for both the operator and system observers to trace or flag the event.<br />No audit trail or comprehensive logging existed for these actions: the Admin Panel did not capture the identity of the administrator making bulk updates nor record a timestamp for such changes.<br /></p></li><li><p><strong>Why:&nbsp;Why was there no immediate operational or downstream alert as the issue manifested?</strong><br /><br />There were no targeted alerts or anomaly detection in place to notify teams when the manual review volume increased dramatically. The existing #clinical-alerts Slack channel was not tuned for actionable insights, and regular monitoring was limited due to the holiday period when key staff were unavailable. This allowed the impact to persist over few days.</p></li></ol><h2>Architectural Context</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"debddee3-79f8-47e9-b4a2-eb48ad1b6e87\"><ac:rich-text-body><p>Give background and architectural context of the service/data flow in question, this helps reader understand the whole picture.</p></ac:rich-text-body></ac:structured-macro><p>&nbsp;<br />The table CareTeamMemberMaxWorkloads contains the data for daily workload and max daily workloads for care team members (PTs, Coaches and Onboarding Specialists). <br /><br />The accidental bulk update from Admin panel prod env made the values to a much lower level than the normal one.</p><h4>Root Cause Analysis<br /><br />Additional Note<br /><br />On 04th July 2025, an accidental <a href=\"https://app.hingehealth.com/admin/max_adjusted_workloads/66874\">record </a>got created in Admin Panel Prod env for Max Daily Workload affecting  one CTM member by <ac:link><ri:user ri:account-id=\"712020:40a4bcd4-22c8-4afe-87d6-6372ce7180d2\" ri:local-id=\"05c62de3-a1fe-46bd-98a6-bc64c61d03b2\" /></ac:link> . <br /><br />This was duly reported to <ac:link><ri:user ri:account-id=\"712020:3b8bb575-505e-4a7f-9457-2883e4848b6a\" ri:local-id=\"a73f87e2-e660-4a9b-bd62-084cd43ca45a\" /></ac:link> <ac:link><ri:user ri:account-id=\"712020:f149fcfe-1d0b-4073-bb5f-846ea50d55b8\" ri:local-id=\"c053e4d2-8376-43ca-ba7a-7a6e50c79a0e\" /></ac:link> and <ac:link><ri:user ri:account-id=\"641dd3d89d2bc6c90a8bb27d\" ri:local-id=\"571e9ac0-beee-428b-ba58-2f490673c82b\" /></ac:link> .<br /><br /><ac:inline-comment-marker ac:ref=\"359a2eb0-9b49-452f-bac3-b7aab6c23b2b\">The issue with all previous Max Workload setting started happening around the same time, which can be attributed to accidental Bulk Update during the same testing which went unnoticed.</ac:inline-comment-marker><br /><br />Note: The logs or database or UI does not capture the actor or timing of bulk update action performed.</h4><h3>Major Problems Identified</h3><p>List the major issues that the analysis identified.</p><ul><li><p><strong>Immediate Cause</strong>: Human error - An engineer, while testing a feature, in Admin Panel Prod, mistakenly triggered actions believing they were working within a development environment.<br /></p></li><li><p><strong>Contributing Factors</strong>: <br /><br />UI did not reflect that a mass update had occurred or who performed it.<br /><br />No automated alerts, monitoring, or validation checks implemented to verify the reasonableness of the new values.<br /><br />No targeted alerts or anomaly detection in place to notify teams when the manual review volume increased dramatically<br /><em><span style=\"color: rgb(151,160,175);\">.</span></em></p></li><li><p><strong>Underlying Causes:</strong></p><ul><li><p><strong>Technical Infrastructure:</strong> <em><span style=\"color: rgb(151,160,175);\">None</span></em></p></li><li><p><strong>Processes and Procedures</strong>: <br />Lack of alerts and monitoring for anomaly in members count being sent for manual review </p></li></ul></li></ul><h2>Lessons Learned</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"d5cc30f6-8be8-4185-9fc8-11cfcc9e2f14\"><ac:rich-text-body><p>The lessons learned from the 5 Whys and potentially more get added as a list. Add a mandatory section on how Time to Detection and Time to Resolution can be cut in half.</p><p>Focus on how lessons learnt become a part of our dev/deploy/monitoring flow, what steps we have taken to integrate those lessons.</p></ac:rich-text-body></ac:structured-macro><h4>Actionable Insights:</h4><p>Summarize the key insights and lessons learned from the 5 Whys analysis. Focus on actionable recommendations and corrective actions that can be implemented to prevent similar incidents in the future. Include recommendations for process improvements, system enhancements, training needs, and any other measures deemed necessary based on the analysis.</p><h4>Time to Detection</h4><p>&nbsp;42 hours 47 mins</p><h4>Time to Resolution</h4><p>&nbsp;43 hours 52 mins<br /><br />Note: US holiday on 04th July followed by weekends extended the duration for detection and resolution time.</p><h2>Action Items</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"241b4bdc-ceb1-4dff-9cc6-242b371d1a0e\"><ac:rich-text-body><p>Action items from the 5 Whys section get added here. Every action item needs to have an owner, a JIRA ticket and an ETA.</p></ac:rich-text-body></ac:structured-macro><p>Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:</p><ul><li><p><strong><ac:inline-comment-marker ac:ref=\"dfbfa70c-9201-4441-91c7-1c0d1ec34062\">Technical Solutions:</ac:inline-comment-marker></strong><ac:inline-comment-marker ac:ref=\"dfbfa70c-9201-4441-91c7-1c0d1ec34062\"> </ac:inline-comment-marker><br /><br /><ac:inline-comment-marker ac:ref=\"dfbfa70c-9201-4441-91c7-1c0d1ec34062\">Implement event triggers (for example if there is a spike in manual review, we need to get an alert)</ac:inline-comment-marker><br /><br /><ac:inline-comment-marker ac:ref=\"dfbfa70c-9201-4441-91c7-1c0d1ec34062\">Deleting &lsquo;Bulk Update Max Daily Workloads&rsquo; button from the UI so that workloads cannot be updated from the UI.</ac:inline-comment-marker><br /></p></li><li><p><strong><ac:inline-comment-marker ac:ref=\"dfbfa70c-9201-4441-91c7-1c0d1ec34062\">Process Improvements: </ac:inline-comment-marker></strong><br /><br /><ac:inline-comment-marker ac:ref=\"dfbfa70c-9201-4441-91c7-1c0d1ec34062\">Adding a verification step  to verify the reasonableness of the new values while performing Bulk Update Max Daily Workloads.</ac:inline-comment-marker><br /><ac:inline-comment-marker ac:ref=\"dfbfa70c-9201-4441-91c7-1c0d1ec34062\"> </ac:inline-comment-marker></p></li><li><p><strong><ac:inline-comment-marker ac:ref=\"dfbfa70c-9201-4441-91c7-1c0d1ec34062\">Monitoring and Alerting:</ac:inline-comment-marker></strong><ac:inline-comment-marker ac:ref=\"dfbfa70c-9201-4441-91c7-1c0d1ec34062\"> </ac:inline-comment-marker><br /><br /><ac:inline-comment-marker ac:ref=\"dfbfa70c-9201-4441-91c7-1c0d1ec34062\">Setup an alert in channel for bulk updates on Max Daily Workloads.</ac:inline-comment-marker><br /></p></li></ul><p>&nbsp;</p><hr /><h3>Post Mortem Notes:&nbsp;</h3>",
        "content_text": "Start date: 2025-07-03(07:30 PM PDT)Close date: 2025-07-07(07:52 AM PDT)Severity: P3NOTE: that this doc is written within 2 weeks of the incident happening. After this is written a review is setup with senior tech leaders in the org.Author: Client Facing Summary:The issue involved, delayed members from being accepted into programs by pushing them into manual review.Exec SummaryA bug was identified where all Care Team Members had their daily max adjusted workloads incorrectly set to 10, reverting from higher values previously set (e.g., PTs typically have a daily max workload between 250-300). This caused member capacity to be hit early and prevented members from being accepted into the program. The issue was resolved by adjusting the daily max capacity back to appropriate levels.The manual review process was being triggered more than usual due to a technical issue and the unavailability of onboarding specialists during the holiday(04th July 2025).ImpactCustomer Impact: 1772 members went to manual review due to no MES available between July 3, 2025 7:05 PM PDT and July 7, 2025 7:52 AM PDT. These members were all assigned to a MES by 07th July morning (PDT), as well as some assigned over the weekend as caps naturally reset.These members would be waiting until manual review is done.Security breach: NoneBusiness Impact: Due to a delay in manual review processing, we currently have a backlog of 622 pending acceptances (approximately $500K in credits). These credits will be applied on the date of acceptance post manual review. This reflects a delay in crediting, not a loss. Latest information on the loss - from July 4th to 7th, 2219 Manual Review users, amounting to loss of $246kAround ~1K applications that got accepted later than usual, with a delay of more than a day.Updated trends for Acceptance rate, where we can see that the backlogged AQs have been resolved now with the acceptance rate around the 85%+ mark (which is around the normal mark if we see last one/two week data).Internal Impact: Yet to AscertainTimeline (Pacific time, with full timestamps)Incident Slack channel Discussions -Timestamp(PDT)AuthorMessage2025-07-05 01:03 PM PDTJonathan Westfieldhey natasha - wanted to share a quick heads up on some data anomalies we observed from yesterday...• we observed a really low accepted member number at 247. even with the holiday we would've expected higher.looking a little closer, we had 748 aq completes [more in-line with expectations], which means a ~33% accept rate, very lowit appears that two things have happened…   ◦ we sent a much higher % of users to manual review [60% compared to a 10-20% norm]  ◦ due to the holiday, none of the manual review folks have been reviewed yetif this is all true, then yesterday was a technical issue? based on the partial of today, it looks like it was just yesterday, but lets check tomorrowall those users sent to manual review should ultimately be reviewed and a large part accepted, but let's check monday [assuming thats when folks sign back on after the holiday to review]the shortfall as of now is about 622 acceptances [~$500K bookings]. I'm assuming they won't get backdated to July 4th once reviewed and accepted, but instead be credited to the date of the manual review2025-07-05 01:33 PM PDTNatasha Udpahttps://app.mode.com/hinge_health/reports/a30af0d30211/runs/f85d4816af8a - this looks like there aren't onboarding specialists to support. An anomaly in comparison to earlier dates. We should follow up or add Hannah Yelton and Austin Ord to the thread - late last year we expanded the pool to avoid running into this issue so I'm surprised it's coming up again2025-07-06 06:53 AM PDTJonathan WestfieldLooks like same issue continued into yesterday contrary to what I thought the partial said yesterday [i.e. much lower than expected new accepted members at 269 due to most users being sent to manual review, >50%]. I assume the same will happen today, Sunday, which will cause a non-trivial forecast miss, so will add the forecast team2025-07-07 09:36 AM PDTHannah YeltonCatching up from last week - I'll check with the team why we ran into this issue on Friday exactly but we usually see this issue when there is a high number of sick call outs (which often happens around holidays).And the reason caps exist in general is because the assignment engine (how member are assigned to care team members) will flood MESs with too many members if we don't have some sort of caps in place since it's still a somewhat basic assignment tool. Saisundar (the new CareOps PM) is working on improving this logic to avoid this from happening but the tooling has not been updated yet. If we don't have caps in place, a MES might get a thousand members in one day whereas their peers would get none. The caps help us control that.2025-07-07 09:55 AM PDTHannah YeltonJust heard back from the team - Sounds like there was a testing error that led to this issue.Bringing in Sunil / Saisundar into this thread as well. I know we've chatted about permissions before / the risk of anyone with Super access accidentally adjusting the DE tabs, but sharing this example of the larger impact when accidents happen.https://hingehealth.slack.com/archives/CDST58LR0/p1751904022615939 2025-07-07 11:38 AM PDTHannah YeltonLooks like the caps were adjusted to the lower value on July 3, 2025 7:05 PM PDT and then corrected back on July 7, 2025 7:52 AM PDT. Some members were getting through, but since caps were lower those days, members were sent to manual review.From what we can tell from our side, we set the caps are their normal levels and those numbers were then accidentally adjusted by eng while testing.Sunil I think in terms of preventing this in the future, we need some sort of access controls in place to ensure production values are not changed without DE approval.2025-07-07 11:47 AM PDTNatasha UdpaThanks Hannah - Is there any dashboarding / alerting in place ?2025-07-07 11:49 AM PDTHannah YeltonThere is the #clinical-alerts channel, but it's pretty hard to follow / messy so not a great source of alerting. And then in general (during working hours), Member Ops and DE both keep an eye on manual review.2025-07-07 05:31 PM PDTHenrique De SaboiaThank you Hannah. Sunil Please let us know how we can prevent this from happening from a development perspective.Hannah Yelton Do you have an impact analysis of the number of member who were impacted and had their onboarding delayed?2025-07-07 07:17 PM PDTHannah YeltonYes, agreed.Member Ops shared a dashboard - Looks like 1772 members went to manual review due to no MES available between 7/3 and 7/7. These members were all assigned to a MES by this morning, as well as some assigned over the weekend as caps naturally reset.@Sunil I'm not sure what the engineer was testing / how they were able to edit the values, but I think it's worth digging into that.Summarising the above chats containing data with screenshotsObserved a really low accepted member number at 247. even with the holiday we would've expected higher.Looking a little closer, we had 748 aq completes [more in-line with expectations], which means a ~33% accept rate, very low.All those users sent to manual review should ultimately be reviewed and a large part accepted, but let's check monday [assuming thats when folks sign back on after the holiday to review]The shortfall as of now is about 622 acceptances [~$500K bookings]. I'm assuming they won't get backdated to July 4th once reviewed and accepted, but instead be credited to the date of the manual reviewSpecifically record, when Incident Detected/Reported and when Incident MitigatedIncident Reported - July 5, 2025 1:05 PM PDTIncident Mitigated - July 7, 2025 7:52 AM PDT5 WHYs - Why did this incident happen?Why:  Was there an incident?An engineer, while testing a feature, unintentionally performed a bulk update for “Max Adjusted Workloads”, updating all CTMs workload in the production Admin Panel, mistakenly believing they were working within a development environment.Why: Why was it possible to make such a change in production without preventative checks?The Admin Panel allows user with (Super and Staff role) admin access to perform bulk updates to critical workload settings without additional confirmation steps or warnings, making it possible for an accidental change to impact all Care Team Member records at once.Why: Why did the system not detect or flag this unusual change?There were no automated alerts, monitoring, or validation checks implemented to detect bulk updates to Max Adjusted Workloads or to verify the reasonableness of the new values (e.g., a daily max of 10, which is significantly below the norm).Why: Why was the change not immediately noticed and reverted?The UI did not reflect that a mass update had occurred or who performed it, making it difficult for both the operator and system observers to trace or flag the event.No audit trail or comprehensive logging existed for these actions: the Admin Panel did not capture the identity of the administrator making bulk updates nor record a timestamp for such changes.Why: Why was there no immediate operational or downstream alert as the issue manifested?There were no targeted alerts or anomaly detection in place to notify teams when the manual review volume increased dramatically. The existing #clinical-alerts Slack channel was not tuned for actionable insights, and regular monitoring was limited due to the holiday period when key staff were unavailable. This allowed the impact to persist over few days.Architectural ContextGive background and architectural context of the service/data flow in question, this helps reader understand the whole picture. The table CareTeamMemberMaxWorkloads contains the data for daily workload and max daily workloads for care team members (PTs, Coaches and Onboarding Specialists). The accidental bulk update from Admin panel prod env made the values to a much lower level than the normal one.Root Cause AnalysisAdditional NoteOn 04th July 2025, an accidental record got created in Admin Panel Prod env for Max Daily Workload affecting one CTM member by . This was duly reported to and .The issue with all previous Max Workload setting started happening around the same time, which can be attributed to accidental Bulk Update during the same testing which went unnoticed.Note: The logs or database or UI does not capture the actor or timing of bulk update action performed.Major Problems IdentifiedList the major issues that the analysis identified.Immediate Cause: Human error - An engineer, while testing a feature, in Admin Panel Prod, mistakenly triggered actions believing they were working within a development environment.Contributing Factors: UI did not reflect that a mass update had occurred or who performed it.No automated alerts, monitoring, or validation checks implemented to verify the reasonableness of the new values.No targeted alerts or anomaly detection in place to notify teams when the manual review volume increased dramatically.Underlying Causes:Technical Infrastructure: NoneProcesses and Procedures: Lack of alerts and monitoring for anomaly in members count being sent for manual review Lessons LearnedThe lessons learned from the 5 Whys and potentially more get added as a list. Add a mandatory section on how Time to Detection and Time to Resolution can be cut in half.Focus on how lessons learnt become a part of our dev/deploy/monitoring flow, what steps we have taken to integrate those lessons.Actionable Insights:Summarize the key insights and lessons learned from the 5 Whys analysis. Focus on actionable recommendations and corrective actions that can be implemented to prevent similar incidents in the future. Include recommendations for process improvements, system enhancements, training needs, and any other measures deemed necessary based on the analysis.Time to Detection 42 hours 47 minsTime to Resolution 43 hours 52 minsNote: US holiday on 04th July followed by weekends extended the duration for detection and resolution time.Action ItemsAction items from the 5 Whys section get added here. Every action item needs to have an owner, a JIRA ticket and an ETA.Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:Technical Solutions: Implement event triggers (for example if there is a spike in manual review, we need to get an alert)Deleting ‘Bulk Update Max Daily Workloads’ button from the UI so that workloads cannot be updated from the UI.Process Improvements: Adding a verification step to verify the reasonableness of the new values while performing Bulk Update Max Daily Workloads. Monitoring and Alerting: Setup an alert in channel for bulk updates on Max Daily Workloads. Post Mortem Notes:",
        "title": "RCA: IR-366 Daily max adjusted workloads incorrectly set in Admin Panel",
        "page_metadata": {
          "space": "RND",
          "page_id": "1593508234",
          "estimated_word_count": 2031
        }
      },
      "analysis_results": {
        "content_analysis": {
          "incident_summary": "A bug was identified where all Care Team Members had their daily max adjusted workloads incorrectly set to 10, reverting from higher values previously set (e. , PTs typically have a daily max workload between 250-300).",
          "users_impacted": "1772 members; Services: Onboarding services impacted",
          "root_causes": [
            "Additional NoteOn 04th July 2025, an accidental record got created in Admin Panel Prod env for Max Daily Workload affecting one CTM member by",
            "This was duly reported to and",
            "The logs or database or UI does not capture the actor or timing of bulk update action performed",
            "NoneProcesses and Procedures: Lack of alerts and monitoring for anomaly in members count being sent for manual review"
          ],
          "analysis_quality": "low"
        },
        "quality_assessment": {
          "score": 97,
          "grade": "A",
          "feedback": "Exceptional RCA (97/100). Strongest area: Root Cause Analysis. Consider sharing as template. Minor enhancement opportunity: Communication Clarity.",
          "strengths": [
            "Timeline Detection: Detailed timestamps provided",
            "Timeline Detection: Dedicated timeline section",
            "Timeline Detection: Detection methods described"
          ],
          "critical_gaps": [
            "None identified"
          ]
        },
        "business_impact": {
          "impact_score": 5,
          "customer_count_affected": "1772 users",
          "revenue_impact_est": "Minimal: <$1K potential impact",
          "service_downtime_minutes": 2567,
          "severity_justification": "Moderate business impact; Extended downtime (2567 minutes)"
        },
        "technical_analysis": {
          "root_cause_category": "Monitoring/Alerting Gap",
          "detection_time_minutes": 2520,
          "resolution_time_minutes": 0,
          "technical_debt_level": "Medium - Some improvements required",
          "automation_score": 5
        }
      }
    },
    {
      "jira_data": {
        "ticket_key": "IR-365",
        "summary": "Unable to see ET session activity in Carehub ",
        "priority": "P3",
        "created_date": "2025-07-08T09:17:54.322-0700",
        "status": "In Progress",
        "description": "CTMs are reporting that significant delays in member activity appearing in CareHub, a lag of 10 hours or more for completed activities—such as ETs—to sync and become visible in the system. This is leading to confusion, especially with re-engager tasks, which are often triggered when it appears that members have been inactive, even though they have resumed activity. While article reads are sometimes updated more quickly, ET data are often delayed, and in some cases, member-reported completions from the previous day are still missing from CareHub.\n\nDPT thread in Slack: [https://hingehealth.slack.com/archives/C06M3R0Q0SF/p1751928669803059|https://hingehealth.slack.com/archives/C06M3R0Q0SF/p1751928669803059|smart-link] \n\n\n\n----\n\n* Slack Channel: [https://hingehealth.enterprise.slack.com/archives/C095HAKPRDW|https://hingehealth.enterprise.slack.com/archives/C095HAKPRDW|smart-link] \n* Related ticket: [https://hingehealth.atlassian.net/browse/KAIL-3261|https://hingehealth.atlassian.net/browse/KAIL-3261]",
        "custom_fields": {
          "incident_urgency": "P3",
          "pods_engaged": "Data Engineering",
          "reporter": null,
          "assignee": null
        },
        "comments": [],
        "metadata": {
          "jira_url": "https://hingehealth.atlassian.net/browse/IR-365",
          "labels": [],
          "component": null,
          "resolution": null
        }
      },
      "confluence_data": {
        "rca_url": "https://hingehealth.atlassian.net/wiki/spaces/RND/pages/1589969110/RCA+IR-365+Unable+to+see+ET+session+activity+in+Carehub",
        "rca_available": true,
        "content_html": "<p><strong>Start date:</strong>&nbsp;July 8th 9:07 am PT</p><p><strong>Close date:</strong>&nbsp;July 9th 3:35 am PT</p><p><strong>Severity</strong>:&nbsp;P3</p><p><strong>Adverse Event level (1-4)</strong>: &nbsp;<em>required for Enso, Global App, and Perifit</em></p><p /><p><strong><span style=\"color: rgb(0,102,68);\">NOTE: that this doc is written within 2 weeks of the incident happening. After this is written a review is setup with senior tech leaders in the org.</span></strong></p><p><strong>Author</strong>: <em><span style=\"color: rgb(151,160,175);\">The person writing the doc and going through the RCA in OE meetings. If there is more than one author, the first author is considered to be the primary point of contact.</span></em></p><p>&nbsp;</p><p><strong><span style=\"color: rgb(255,86,48);\">NOTE: Delete the instruction text in each section as you fill it out.</span></strong></p><h2><strong>Example of a well written RCA</strong> <a href=\"https://docs.google.com/document/d/1pP9aDigsCYf3WvPCSTEoGoipQbb-Ec1uDDkca1622MU/edit#heading=h.yih5usn5ed71\"><u>HingeHealth main_db Database Incident</u></a>&nbsp;</h2><h2>Client Facing Summary (if necessary):</h2><p>&nbsp;</p><h2>Exec Summary</h2><p>On July 3rd, a change was merged into production to parameterize the UMR secondary Job process for backfilling purposes. However, the date parameterization within this change was not correctly resolving the expected date functionalities.<br />The misconfigured date parameterization caused the job's date range to consistently pick records only for the actual run date, instead of the intended `run_date +1`. This led to an incomplete processing of records in the UMR ET session and assigned exercises, as fewer records than expected were processed.</p><h2>Impact</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"59430d62-ba1b-424b-88f7-7885e0de7ee5\"><ac:rich-text-body><p>Fill out the details of the impact of the incident starting with the end user/customer and then working from there to the business&nbsp; impact. Be as specific as you can (X customers impacted, Y transactions failed, $Z lost&hellip;Don't put a lot of details about how or why the incident happened, but just focus on the impact. Keep the why for later. For example, &quot;users could not log in for 30 mins because of auth service unavailability&quot; instead of &quot;one aws zone (us-east-02) had a drop in availability and our load balancing setup had a bug so any requests for log in going to us-east-02 failed for 30 mins&quot;.)</p></ac:rich-text-body></ac:structured-macro><ul><li><p><strong>Customer Impact: </strong><em><span style=\"color: rgb(151,160,175);\">(note who we consider the customer here; eg participant, client)</span></em></p></li><li><p><strong>Security breach: </strong><span style=\"color: rgb(151,160,175);\">/ PHI Data Leak if any (note- PHI Data cannot be added to this doc and RCA docs about security breaches should be access controlled - work with IT Sec on this</span></p></li><li><p><strong>Business Impact: </strong>Indirect member impact as coaches were not able to suggest next actions to members</p></li><li><p><strong>Internal Impact: </strong>Exercises were peformed. certain cases reengager workflow was performed. member and CTM. Exercises was not visible in carehub (Jul 2nd, 7th -9th) </p></li></ul><h2>Timeline (Pacific time, with full timestamps)</h2><p><em><strong>2025-07-08 09:07 AM PT</strong>: Coaches reporting that they are </em>unable to view completed ET sessions in Care hub.</p><p /><p><em>Specifically record, when <strong>Incident Detected/Reported </strong>and when <strong>Incident Mitigated</strong></em></p><p>Coach time reported &rarr; Resolution time</p><h2>5 WHYs - Why did this incident happen?</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"865dc9bc-50a5-4d5e-bbfb-9718dccd4459\"><ac:rich-text-body><p>Start from how the incident was discovered and then work down from there. Note that there may be a few branches of 5 Whys for issues that have multiple root causes. Make sure you don&rsquo;t answer the question in the first why. Let the trail continue till you get to the root issue and then the root issue has to mandatorily have an action item/lesson learned. This should also result in answers to: What was the technical issue that caused the incident? What was the reason the incident wasn't mitigated sooner?</p></ac:rich-text-body></ac:structured-macro><ol start=\"1\"><li><p>Why:&nbsp; Was there an incident?</p></li><li><p>Why:&nbsp;UMR data sync job failed to generate data for the current day</p></li><li><p>Why:&nbsp;UMR data sync job had been modified to include backfill capability between two dates.</p></li><li><p>Why:&nbsp;Backfill capability is provided to address data gaps in historical runs (Backfill was done manually earlier)</p></li><li><p>Why:&nbsp;To introduce backfill greater than sql condition was changed to between. Since it is between for timestamps, only till previous day end was considered. So there was atmost a delay of 23h 59m possible.</p></li></ol><h2>Architectural Context</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"057c39ac-f888-40d1-b80a-ebaf945771ca\"><ac:rich-text-body><p>Give background and architectural context of the service/data flow in question, this helps reader understand the whole picture.</p></ac:rich-text-body></ac:structured-macro><p>&nbsp;</p><h2>Root Cause Analysis</h2><h3>Major Problems Identified</h3><p>List the major issues that the analysis identified.</p><ul><li><p><strong>Immediate Cause</strong>: Code change in UMR pipeline jobs which provides the consumption data for CHRS.</p></li><li><p><strong>Contributing Factors</strong>:   </p><ul><li><p>Extensive data testing was not carried out in lower environments. This specific edge case was not tested in dev/stage.</p></li><li><p>UMR data monitoring was not fully setup to cover the cases</p></li></ul></li><li><p><strong>Underlying Causes:</strong></p><ul><li><p><strong>Technical Infrastructure:</strong> <em><span style=\"color: rgb(151,160,175);\">Assess the state of the company's technical infrastructure, including network stability, server performance, and software dependencies.</span></em></p></li><li><p><strong>Processes and Procedures</strong>: <em><span style=\"color: rgb(151,160,175);\">Evaluate the effectiveness of existing processes and procedures related to deployment, maintenance, and troubleshooting. Identify any gaps or weaknesses.</span></em></p></li><li><p><strong>Human Factors</strong>: <em><span style=\"color: rgb(151,160,175);\">Consider the role of human factors, such as training deficiencies, communication issues, or decision-making processes, in the incident.</span></em></p></li></ul></li></ul><h2>Lessons Learned</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"77564d26-3e35-47f4-a2d4-872eb9581f69\"><ac:rich-text-body><p>The lessons learned from the 5 Whys and potentially more get added as a list. Add a mandatory section on how Time to Detection and Time to Resolution can be cut in half.</p><p>Focus on how lessons learnt become a part of our dev/deploy/monitoring flow, what steps we have taken to integrate those lessons.</p></ac:rich-text-body></ac:structured-macro><h4>Actionable Insights:</h4><h3>Key Insights from the Analysis</h3><p /><p>The incident happened because a change to the UMR data sync job, meant to add a new backfill feature, went wrong. The core problem was a simple but critical mistake: changing the SQL query to use the <code>BETWEEN</code> operator instead of <code>GREATER THAN</code>. This change wasn't properly tested or reviewed, which created a gap in the data and caused the sync to fail.</p><hr /><h3>How to Fix This and Prevent It from Happening Again</h3><p>To avoid similar issues in the future, we need to make some practical changes to our process and systems.</p><h4>1. Fix Our Process</h4><ul><li><p><strong>Make Code Reviews Mandatory:</strong> Before any code goes live, at least one other person needs to review it. They should look for logical errors, not just typos.</p></li><li><p><strong>Test Everything:</strong> We need a solid testing plan. This means running the new code in a test environment to see how it handles both daily operations and backfill requests. We should also specifically test tricky &quot;edge cases&quot; like what happens at midnight.</p></li><li><p><strong>Track All Changes:</strong> We must keep a record of every code change, why it was made, and how it was tested.</p></li></ul><h4>2. Improve Our Systems</h4><ul><li><p><strong>Separate the Jobs:</strong> The daily data sync is too important to be mixed with backfill. We should have one job that runs every day with the reliable <code>GREATER THAN</code> logic, and a separate, on-demand script for backfilling old data. This way, if the backfill script has a bug, it won't break the daily sync.</p></li><li><p><strong>Set Up Alerts:</strong> The system should tell us immediately if a data job fails or if it doesn't produce any data. This will help us catch problems much faster.</p></li></ul><p /><h4>Time to Detection</h4><p>&nbsp;</p><h4>Time to Resolution</h4><p>&nbsp;</p><h2>Action Items</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"1c14c5e2-ec40-419d-bae2-68f1773c51e3\"><ac:rich-text-body><p>Action items from the 5 Whys section get added here. Every action item needs to have an owner, a JIRA ticket and an ETA.</p></ac:rich-text-body></ac:structured-macro><p>Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:</p><ul><li><p><strong>Process Improvements: </strong></p><ul><li><p>Better testing procedures</p></li><li><p>Thorough PR review for customer facing datasets</p></li></ul></li><li><p><strong>Monitoring and Alerting:</strong> </p><ul><li><p>UMR data mismatch </p></li></ul></li><li><p><strong>Testing Improvements:</strong>&nbsp; </p><ul><li><p><em>E2E Regression testing will have to be completed by QE team for changes in UMR</em></p></li></ul></li></ul><hr /><h3>Post Mortem Notes:&nbsp;</h3>",
        "content_text": "Start date: July 8th 9:07 am PTClose date: July 9th 3:35 am PTSeverity: P3Adverse Event level (1-4):  required for Enso, Global App, and PerifitNOTE: that this doc is written within 2 weeks of the incident happening. After this is written a review is setup with senior tech leaders in the org.Author: The person writing the doc and going through the RCA in OE meetings. If there is more than one author, the first author is considered to be the primary point of contact. NOTE: Delete the instruction text in each section as you fill it out.Example of a well written RCA HingeHealth main_db Database Incident Client Facing Summary (if necessary): Exec SummaryOn July 3rd, a change was merged into production to parameterize the UMR secondary Job process for backfilling purposes. However, the date parameterization within this change was not correctly resolving the expected date functionalities.The misconfigured date parameterization caused the job's date range to consistently pick records only for the actual run date, instead of the intended `run_date +1`. This led to an incomplete processing of records in the UMR ET session and assigned exercises, as fewer records than expected were processed.ImpactFill out the details of the impact of the incident starting with the end user/customer and then working from there to the business  impact. Be as specific as you can (X customers impacted, Y transactions failed, $Z lost…Don't put a lot of details about how or why the incident happened, but just focus on the impact. Keep the why for later. For example, \"users could not log in for 30 mins because of auth service unavailability\" instead of \"one aws zone (us-east-02) had a drop in availability and our load balancing setup had a bug so any requests for log in going to us-east-02 failed for 30 mins\".)Customer Impact: (note who we consider the customer here; eg participant, client)Security breach: / PHI Data Leak if any (note- PHI Data cannot be added to this doc and RCA docs about security breaches should be access controlled - work with IT Sec on thisBusiness Impact: Indirect member impact as coaches were not able to suggest next actions to membersInternal Impact: Exercises were peformed. certain cases reengager workflow was performed. member and CTM. Exercises was not visible in carehub (Jul 2nd, 7th -9th) Timeline (Pacific time, with full timestamps)2025-07-08 09:07 AM PT: Coaches reporting that they are unable to view completed ET sessions in Care hub.Specifically record, when Incident Detected/Reported and when Incident MitigatedCoach time reported → Resolution time5 WHYs - Why did this incident happen?Start from how the incident was discovered and then work down from there. Note that there may be a few branches of 5 Whys for issues that have multiple root causes. Make sure you don’t answer the question in the first why. Let the trail continue till you get to the root issue and then the root issue has to mandatorily have an action item/lesson learned. This should also result in answers to: What was the technical issue that caused the incident? What was the reason the incident wasn't mitigated sooner?Why:  Was there an incident?Why: UMR data sync job failed to generate data for the current dayWhy: UMR data sync job had been modified to include backfill capability between two dates.Why: Backfill capability is provided to address data gaps in historical runs (Backfill was done manually earlier)Why: To introduce backfill greater than sql condition was changed to between. Since it is between for timestamps, only till previous day end was considered. So there was atmost a delay of 23h 59m possible.Architectural ContextGive background and architectural context of the service/data flow in question, this helps reader understand the whole picture. Root Cause AnalysisMajor Problems IdentifiedList the major issues that the analysis identified.Immediate Cause: Code change in UMR pipeline jobs which provides the consumption data for CHRS.Contributing Factors: Extensive data testing was not carried out in lower environments. This specific edge case was not tested in dev/stage.UMR data monitoring was not fully setup to cover the casesUnderlying Causes:Technical Infrastructure: Assess the state of the company's technical infrastructure, including network stability, server performance, and software dependencies.Processes and Procedures: Evaluate the effectiveness of existing processes and procedures related to deployment, maintenance, and troubleshooting. Identify any gaps or weaknesses.Human Factors: Consider the role of human factors, such as training deficiencies, communication issues, or decision-making processes, in the incident.Lessons LearnedThe lessons learned from the 5 Whys and potentially more get added as a list. Add a mandatory section on how Time to Detection and Time to Resolution can be cut in half.Focus on how lessons learnt become a part of our dev/deploy/monitoring flow, what steps we have taken to integrate those lessons.Actionable Insights:Key Insights from the AnalysisThe incident happened because a change to the UMR data sync job, meant to add a new backfill feature, went wrong. The core problem was a simple but critical mistake: changing the SQL query to use the BETWEEN operator instead of GREATER THAN. This change wasn't properly tested or reviewed, which created a gap in the data and caused the sync to fail.How to Fix This and Prevent It from Happening AgainTo avoid similar issues in the future, we need to make some practical changes to our process and systems.1. Fix Our ProcessMake Code Reviews Mandatory: Before any code goes live, at least one other person needs to review it. They should look for logical errors, not just typos.Test Everything: We need a solid testing plan. This means running the new code in a test environment to see how it handles both daily operations and backfill requests. We should also specifically test tricky \"edge cases\" like what happens at midnight.Track All Changes: We must keep a record of every code change, why it was made, and how it was tested.2. Improve Our SystemsSeparate the Jobs: The daily data sync is too important to be mixed with backfill. We should have one job that runs every day with the reliable GREATER THAN logic, and a separate, on-demand script for backfilling old data. This way, if the backfill script has a bug, it won't break the daily sync.Set Up Alerts: The system should tell us immediately if a data job fails or if it doesn't produce any data. This will help us catch problems much faster.Time to Detection Time to Resolution Action ItemsAction items from the 5 Whys section get added here. Every action item needs to have an owner, a JIRA ticket and an ETA.Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:Process Improvements: Better testing proceduresThorough PR review for customer facing datasetsMonitoring and Alerting: UMR data mismatch Testing Improvements:  E2E Regression testing will have to be completed by QE team for changes in UMRPost Mortem Notes:",
        "title": "RCA: IR-365 Unable to see ET session activity in Carehub",
        "page_metadata": {
          "space": "RND",
          "page_id": "1589969110",
          "estimated_word_count": 1129
        }
      },
      "analysis_results": {
        "content_analysis": {
          "incident_summary": "On July 3rd, a change was merged into production to parameterize the UMR secondary Job process for backfilling purposes. However, the date parameterization within this change was not correctly resolving the expected date functionalities.",
          "users_impacted": "User impact details not clearly specified in RCA document",
          "root_causes": [
            "IdentifiedList the major issues that the analysis identified",
            "Code change in UMR pipeline jobs which provides the consumption data for CHRS",
            "Extensive data testing was not carried out in lower environments",
            "Assess the state of the company's",
            "Database connectivity issues"
          ],
          "analysis_quality": "low"
        },
        "quality_assessment": {
          "score": 87,
          "grade": "A",
          "feedback": "Exceptional RCA (87/100). Strongest area: Root Cause Analysis. Consider sharing as template. Minor enhancement opportunity: Impact Assessment.",
          "strengths": [
            "Timeline Detection: Detailed timestamps provided",
            "Timeline Detection: Dedicated timeline section",
            "Timeline Detection: Detection methods described"
          ],
          "critical_gaps": [
            "None identified"
          ]
        },
        "business_impact": {
          "impact_score": 4,
          "customer_count_affected": "User impact not specified",
          "revenue_impact_est": "Low: $1K-5K potential impact",
          "service_downtime_minutes": 30,
          "severity_justification": "Moderate business impact; Service interruption (30 minutes)"
        },
        "technical_analysis": {
          "root_cause_category": "Process/Human Error",
          "detection_time_minutes": 0,
          "resolution_time_minutes": 0,
          "technical_debt_level": "Medium - Some improvements required",
          "automation_score": 5
        }
      }
    }
  ]
}