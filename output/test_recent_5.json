{
  "export_metadata": {
    "timestamp": "2025-08-05T01:13:16.213141",
    "total_incidents": 5,
    "incidents_with_rca": 5,
    "rca_analyzed_count": 5,
    "date_range": "2025-07-17 to 2025-07-31",
    "export_type": "recent_5",
    "rca_match_rate": "100.0%",
    "analysis_success_rate": "100.0%",
    "schema_version": "1.0",
    "purpose": "LLM analysis and custom processing"
  },
  "incidents": [
    {
      "jira_data": {
        "ticket_key": "IR-375",
        "summary": "Unable to start playlist for FTU on alpha and prod app",
        "priority": "P3",
        "created_date": "2025-07-31T11:12:49.901-0700",
        "status": "Root Cause Analysis",
        "description": "IM on personal app on personal device noticed that she was unable to start her first playlist. @Kevin Huang and IM checked in her alpha app and saw the same issue. She also downloaded the app store prod app and its occurring there as well.   \n\nWe did not reach threshold so no incident was triggered. IM manually triggered event.   \n\n10:54 uuid 2690490",
        "custom_fields": {
          "incident_urgency": "P3",
          "pods_engaged": "Rewards and Insights (RAIN), Program Experience, ET Experience (ETE), Responsible: Activity Platform",
          "reporter": null,
          "assignee": null
        },
        "comments": [],
        "metadata": {
          "jira_url": "https://hingehealth.atlassian.net/browse/IR-375",
          "labels": [],
          "component": null,
          "resolution": null
        }
      },
      "confluence_data": {
        "rca_url": "https://hingehealth.atlassian.net/wiki/spaces/RND/pages/1634599285/RCA+IR-375+Unable+to+start+playlist+for+FTU+on+alpha+and+prod+app",
        "rca_available": true,
        "content_html": "<p><strong>Start date:</strong>&nbsp;<time datetime=\"2025-07-31\" /> at 10:54am PST</p><p><strong>Close date:</strong>&nbsp;11:30 am PST</p><p><strong>Severity</strong>:&nbsp;P3</p><p><strong>Adverse Event level (1-4)</strong>: &nbsp;<em>required for Enso, Global App, and Perifit</em></p><p /><p><strong><span style=\"color: rgb(0,102,68);\">NOTE: that this doc is written within 2 weeks of the incident happening. After this is written a review is setup with senior tech leaders in the org.</span></strong></p><p><strong>Author</strong>: <em><span style=\"color: rgb(151,160,175);\">The person writing the doc and going through the RCA in OE meetings. If there is more than one author, the first author is considered to be the primary point of contact.</span></em></p><p>&nbsp;</p><p><strong><span style=\"color: rgb(255,86,48);\">NOTE: Delete the instruction text in each section as you fill it out.</span></strong></p><h2><strong>Example of a well written RCA</strong> <a href=\"https://docs.google.com/document/d/1pP9aDigsCYf3WvPCSTEoGoipQbb-Ec1uDDkca1622MU/edit#heading=h.yih5usn5ed71\"><u>HingeHealth main_db Database Incident</u></a>&nbsp;</h2><h2>Client Facing Summary (if necessary):</h2><p>During the incident, FTU were unable to start their first playlist. The impact of this incident was 16 users. This is not an estimated number. Service has been restored and users are now able to engage with their first playlist.</p><p>&nbsp;</p><h2>Exec Summary</h2><p>A PR was deployed that uncovered a poorly documented check that presented a parameter in the <code>etSessionRecordCreate</code> as optional. This cause a session config to not be generated which fired a 404 resulting in FTU not being able to by pass or advance past the FTU intro video. This has been mitigated.</p><h2>Impact</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"acb6655f-880f-4237-8a86-b89fe25fe816\"><ac:rich-text-body><p>Fill out the details of the impact of the incident starting with the end user/customer and then working from there to the business&nbsp; impact. Be as specific as you can (X customers impacted, Y transactions failed, $Z lost&hellip;Don't put a lot of details about how or why the incident happened, but just focus on the impact. Keep the why for later. For example, &quot;users could not log in for 30 mins because of auth service unavailability&quot; instead of &quot;one aws zone (us-east-02) had a drop in availability and our load balancing setup had a bug so any requests for log in going to us-east-02 failed for 30 mins&quot;.)</p></ac:rich-text-body></ac:structured-macro><ul><li><p><strong>Customer Impact: </strong>Users were unable to start FTU, during the time the faulty deploy was live, approx. 10:30am PST to 11:30 am PST on July 31</p></li><li><p><strong>Security breach: N/A</strong><span style=\"color: rgb(151,160,175);\">/ PHI Data Leak if any (note- PHI Data cannot be added to this doc and RCA docs about security breaches should be access controlled - work with IT Sec on this</span></p></li><li><p><strong>Business Impact: </strong></p><ul><li><p>17 users were unable to start a new FTU playlist</p></li></ul></li><li><p><strong>Internal Impact: N/A</strong></p></li></ul><h2>Timeline (Pacific time, with full timestamps)</h2><p><em><strong>YYYY-MM-DD HH:MM</strong>: Timeline entries (everything from inception of the incident through incident closure. Err on the side providing more detail than less)</em></p><p><em>Specifically record, when <strong>Incident Detected/Reported </strong>and when <strong>Incident Mitigated</strong></em></p><p /><ul><li><p><em><strong>2025-07-31 10:54 PST </strong></em><ac:link><ri:user ri:account-id=\"712020:bf526aee-7514-45cf-a4d3-436c39bda770\" ri:local-id=\"fa639fb2-8576-4691-80a0-93f4d63c36e5\" /></ac:link> reported issues as a FTU being unable to access the first time user playlist to ETE. <ac:link><ri:user ri:account-id=\"6234e59d1dcf800070eaa177\" ri:local-id=\"0dc53eef-9603-4b11-b08f-7a5c15c21fed\" /></ac:link> and <ac:link><ri:user ri:account-id=\"712020:bf526aee-7514-45cf-a4d3-436c39bda770\" ri:local-id=\"2cc70ebd-3006-4bfb-a409-660759cdf3f8\" /></ac:link> investigated via sentry to see if this was a one off or multi user incident. </p></li><li><p><em><strong>2025-07-31 11:02 PST</strong></em> Reported by <ac:link><ri:user ri:account-id=\"712020:bf526aee-7514-45cf-a4d3-436c39bda770\" ri:local-id=\"ffb2c569-3e50-4c42-956d-6b25df1a3099\" /></ac:link> via #incidents <a href=\"https://hingehealth.slack.com/archives/C03QM11HXPE/p1753984940616319\">slack channel </a></p></li><li><p><em><strong>2025-07-31 11:06 PST</strong></em> <a href=\"https://hingehealth.enterprise.slack.com/admin/user_groups\">@ops-ir-comms</a> is alerted </p></li><li><p><em><strong>2025-07-31 11:08 PST</strong></em> <a href=\"https://hingehealth.sentry.io/issues/6732753068/events/4ac3a8c80bb34ba18d3ce4a75064a62c/events/?project=5173911&amp;statsPeriod=24h\" data-card-appearance=\"inline\">https://hingehealth.sentry.io/issues/6732753068/events/4ac3a8c80bb34ba18d3ce4a75064a62c/events/?project=5173911&amp;statsPeriod=24h</a></p></li><li><p><em><strong>2025-07-31 11:16 PST</strong></em> Revert was created <a href=\"https://github.com/hinge-health/phoenix-bff/pull/4206\" data-card-appearance=\"inline\">https://github.com/hinge-health/phoenix-bff/pull/4206</a> </p></li><li><p><em><strong>2025-07-31 11:16 PST</strong></em> PHX BFF frozen</p></li><li><p><em><strong>2025-07-31 11:18 PST</strong></em> Mitigated with prod deploy to last-known-working-version around</p><ul><li><p>CI run <a href=\"https://github.com/hinge-health/phoenix-bff/actions/runs/16656705682\">https://github.com/hinge-health/phoenix-bff/actions/runs/16656705682</a> </p></li></ul></li><li><p><em><strong>2025-07-31 11:45 PST</strong></em> <a href=\"https://github.com/hinge-health/phoenix-bff/pull/4212\" data-card-appearance=\"inline\">https://github.com/hinge-health/phoenix-bff/pull/4212</a> merged</p></li><li><p><em><strong>2025-07-31 12:37 PST</strong></em> PHX BFF unfrozen</p></li></ul><h2>5 WHYs - Why did this incident happen?</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"9ab52040-ad82-4125-b979-c921e975849e\"><ac:rich-text-body><p>Start from how the incident was discovered and then work down from there. Note that there may be a few branches of 5 Whys for issues that have multiple root causes. Make sure you don&rsquo;t answer the question in the first why. Let the trail continue till you get to the root issue and then the root issue has to mandatorily have an action item/lesson learned. This should also result in answers to: What was the technical issue that caused the incident? What was the reason the incident wasn't mitigated sooner?</p></ac:rich-text-body></ac:structured-macro><ol start=\"1\"><li><p>Why:&nbsp; Was there an incident? yes</p></li><li><p>Why:&nbsp;<a href=\"https://github.com/hinge-health/phoenix-bff/pull/4206\" data-card-appearance=\"inline\">https://github.com/hinge-health/phoenix-bff/pull/4206</a> Adds pain trends to the HlnVariant enum and bumps the acitivty service package verstion to get the same update to the shared type PR was merged</p></li><li><p>Why:&nbsp;We tracked it back to this PR <a href=\"https://github.com/hinge-health/phoenix-bff/pull/4182\" data-card-appearance=\"inline\">https://github.com/hinge-health/phoenix-bff/pull/4182</a> </p></li><li><p>Why:&nbsp;We were not passing the session config which was in fact not optional resulting in the config source not being passed to the front end</p></li></ol><h2>Architectural Context</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"53c73de9-b6cf-4dbe-aeef-9c8950a41635\"><ac:rich-text-body><p>Give background and architectural context of the service/data flow in question, this helps reader understand the whole picture.</p></ac:rich-text-body></ac:structured-macro><p>&nbsp;</p><h2>Root Cause Analysis</h2><h3>Major Problems Identified</h3><p>List the major issues that the analysis identified.</p><ul><li><p><strong>Immediate Cause</strong>: <strong>IM used the production app</strong></p></li><li><p><strong>Contributing Factors</strong>: <strong>Human error, could have tested more, need better documentation in line in the app to alert ICs to the two paths of data flow (session config is not actually optional)</strong></p></li><li><p><strong>Underlying Causes:</strong></p><ul><li><p><strong>Technical Infrastructure:</strong> <em><span style=\"color: rgb(151,160,175);\">Assess the state of the company's technical infrastructure, including network stability, server performance, and software dependencies.</span></em></p></li><li><p><strong>Processes and Procedures</strong>: <em><span style=\"color: rgb(151,160,175);\">Evaluate the effectiveness of existing processes and procedures related to deployment, maintenance, and troubleshooting. Identify any gaps or weaknesses.</span></em></p></li><li><p><strong>Human Factors</strong>: see above</p></li></ul></li></ul><h2>Lessons Learned</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"e4850bd1-cc69-453c-b2d3-a58c3630d745\"><ac:rich-text-body><p>The lessons learned from the 5 Whys and potentially more get added as a list. Add a mandatory section on how Time to Detection and Time to Resolution can be cut in half.</p><p>Focus on how lessons learnt become a part of our dev/deploy/monitoring flow, what steps we have taken to integrate those lessons.</p></ac:rich-text-body></ac:structured-macro><h4>Actionable Insights:</h4><p>Summarize the key insights and lessons learned from the 5 Whys analysis. Focus on actionable recommendations and corrective actions that can be implemented to prevent similar incidents in the future. Include recommendations for process improvements, system enhancements, training needs, and any other measures deemed necessary based on the analysis.</p><ul><li><p>Better inline commenting to alert ICs to the split nature of our users during unification</p></li><li><p>Better end to end testing before merge on app workflows that might potentially be impacted</p></li></ul><h4>Time to Detection</h4><p>10:20 PST&nbsp;</p><h4>Time to Resolution</h4><p>&nbsp;11:30 PST until full resolution</p><h2>Action Items</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"f5fe8302-e70c-4592-93a6-5afa64b732ff\"><ac:rich-text-body><p>Action items from the 5 Whys section get added here. Every action item needs to have an owner, a JIRA ticket and an ETA.</p></ac:rich-text-body></ac:structured-macro><p>Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:</p><ul><li><p><strong>Technical Solutions:</strong> <em><span style=\"color: rgb(151,160,175);\">Implementing upgrades, patches, or enhancements to improve system reliability, scalability, and performance.</span></em></p></li><li><p><strong>Process Improvements: Better monitoring of FTU workflows </strong></p></li><li><p><strong>Training and Education: </strong><span style=\"color: rgb(151,160,175);\">Providing training programs or workshops to enhance technical skills, promote best practices, and improve communication within teams.</span></p></li><li><p><strong>Monitoring and Alerting: Better monitoring of FTU workflows, consider lowering threshold for earlier alerting </strong> <em><span style=\"color: rgb(151,160,175);\">Enhancing monitoring tools and alerting mechanisms to detect issues proactively and minimize downtime.</span></em></p></li><li><p><strong>Testing Improvements:</strong>&nbsp; <em><span style=\"color: rgb(151,160,175);\">Address gaps in testing that allowed this incident to occur and will prevent this from happening again.</span></em></p></li></ul><p>&nbsp;</p><hr /><h3>Post Mortem Notes:&nbsp;</h3><p><a href=\"https://github.com/hinge-health/phoenix-bff/pull/4214\" data-card-appearance=\"inline\">https://github.com/hinge-health/phoenix-bff/pull/4214</a> </p>",
        "content_text": "Start date:  at 10:54am PSTClose date: 11:30 am PSTSeverity: P3Adverse Event level (1-4):  required for Enso, Global App, and PerifitNOTE: that this doc is written within 2 weeks of the incident happening. After this is written a review is setup with senior tech leaders in the org.Author: The person writing the doc and going through the RCA in OE meetings. If there is more than one author, the first author is considered to be the primary point of contact. NOTE: Delete the instruction text in each section as you fill it out.Example of a well written RCA HingeHealth main_db Database Incident Client Facing Summary (if necessary):During the incident, FTU were unable to start their first playlist. The impact of this incident was 16 users. This is not an estimated number. Service has been restored and users are now able to engage with their first playlist. Exec SummaryA PR was deployed that uncovered a poorly documented check that presented a parameter in the etSessionRecordCreate as optional. This cause a session config to not be generated which fired a 404 resulting in FTU not being able to by pass or advance past the FTU intro video. This has been mitigated.ImpactFill out the details of the impact of the incident starting with the end user/customer and then working from there to the business  impact. Be as specific as you can (X customers impacted, Y transactions failed, $Z lost…Don't put a lot of details about how or why the incident happened, but just focus on the impact. Keep the why for later. For example, \"users could not log in for 30 mins because of auth service unavailability\" instead of \"one aws zone (us-east-02) had a drop in availability and our load balancing setup had a bug so any requests for log in going to us-east-02 failed for 30 mins\".)Customer Impact: Users were unable to start FTU, during the time the faulty deploy was live, approx. 10:30am PST to 11:30 am PST on July 31Security breach: N/A/ PHI Data Leak if any (note- PHI Data cannot be added to this doc and RCA docs about security breaches should be access controlled - work with IT Sec on thisBusiness Impact: 17 users were unable to start a new FTU playlistInternal Impact: N/ATimeline (Pacific time, with full timestamps)YYYY-MM-DD HH:MM: Timeline entries (everything from inception of the incident through incident closure. Err on the side providing more detail than less)Specifically record, when Incident Detected/Reported and when Incident Mitigated2025-07-31 10:54 PST reported issues as a FTU being unable to access the first time user playlist to ETE. and investigated via sentry to see if this was a one off or multi user incident. 2025-07-31 11:02 PST Reported by via #incidents slack channel 2025-07-31 11:06 PST @ops-ir-comms is alerted 2025-07-31 11:08 PST https://hingehealth.sentry.io/issues/6732753068/events/4ac3a8c80bb34ba18d3ce4a75064a62c/events/?project=5173911&statsPeriod=24h2025-07-31 11:16 PST Revert was created https://github.com/hinge-health/phoenix-bff/pull/4206 2025-07-31 11:16 PST PHX BFF frozen2025-07-31 11:18 PST Mitigated with prod deploy to last-known-working-version aroundCI run https://github.com/hinge-health/phoenix-bff/actions/runs/16656705682 2025-07-31 11:45 PST https://github.com/hinge-health/phoenix-bff/pull/4212 merged2025-07-31 12:37 PST PHX BFF unfrozen5 WHYs - Why did this incident happen?Start from how the incident was discovered and then work down from there. Note that there may be a few branches of 5 Whys for issues that have multiple root causes. Make sure you don’t answer the question in the first why. Let the trail continue till you get to the root issue and then the root issue has to mandatorily have an action item/lesson learned. This should also result in answers to: What was the technical issue that caused the incident? What was the reason the incident wasn't mitigated sooner?Why:  Was there an incident? yesWhy: https://github.com/hinge-health/phoenix-bff/pull/4206 Adds pain trends to the HlnVariant enum and bumps the acitivty service package verstion to get the same update to the shared type PR was mergedWhy: We tracked it back to this PR https://github.com/hinge-health/phoenix-bff/pull/4182 Why: We were not passing the session config which was in fact not optional resulting in the config source not being passed to the front endArchitectural ContextGive background and architectural context of the service/data flow in question, this helps reader understand the whole picture. Root Cause AnalysisMajor Problems IdentifiedList the major issues that the analysis identified.Immediate Cause: IM used the production appContributing Factors: Human error, could have tested more, need better documentation in line in the app to alert ICs to the two paths of data flow (session config is not actually optional)Underlying Causes:Technical Infrastructure: Assess the state of the company's technical infrastructure, including network stability, server performance, and software dependencies.Processes and Procedures: Evaluate the effectiveness of existing processes and procedures related to deployment, maintenance, and troubleshooting. Identify any gaps or weaknesses.Human Factors: see aboveLessons LearnedThe lessons learned from the 5 Whys and potentially more get added as a list. Add a mandatory section on how Time to Detection and Time to Resolution can be cut in half.Focus on how lessons learnt become a part of our dev/deploy/monitoring flow, what steps we have taken to integrate those lessons.Actionable Insights:Summarize the key insights and lessons learned from the 5 Whys analysis. Focus on actionable recommendations and corrective actions that can be implemented to prevent similar incidents in the future. Include recommendations for process improvements, system enhancements, training needs, and any other measures deemed necessary based on the analysis.Better inline commenting to alert ICs to the split nature of our users during unificationBetter end to end testing before merge on app workflows that might potentially be impactedTime to Detection10:20 PST Time to Resolution 11:30 PST until full resolutionAction ItemsAction items from the 5 Whys section get added here. Every action item needs to have an owner, a JIRA ticket and an ETA.Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:Technical Solutions: Implementing upgrades, patches, or enhancements to improve system reliability, scalability, and performance.Process Improvements: Better monitoring of FTU workflows Training and Education: Providing training programs or workshops to enhance technical skills, promote best practices, and improve communication within teams.Monitoring and Alerting: Better monitoring of FTU workflows, consider lowering threshold for earlier alerting Enhancing monitoring tools and alerting mechanisms to detect issues proactively and minimize downtime.Testing Improvements:  Address gaps in testing that allowed this incident to occur and will prevent this from happening again. Post Mortem Notes: https://github.com/hinge-health/phoenix-bff/pull/4214",
        "title": "RCA: IR-375 Unable to start playlist for FTU on alpha and prod app",
        "page_metadata": {
          "space": "RND",
          "page_id": "1634599285",
          "estimated_word_count": 1038
        }
      },
      "analysis_results": {
        "content_analysis": {
          "incident_summary": "A PR was deployed that uncovered a poorly documented check that presented a parameter in the etSessionRecordCreate as optional. This cause a session config to not be generated which fired a 404 resulting in FTU not being able to by pass or advance past the FTU intro video.",
          "users_impacted": "Duration: From 2025-07-31 10:54 PST to incident resolution",
          "root_causes": [
            "IdentifiedList the major issues that the analysis identified",
            "Processes and Procedures: Evaluate the effectiveness of existing processes and procedures related to deployment, maintenance, and troubleshooting",
            "Identify any gaps or weaknesses",
            "IM used the production app",
            "Assess the state of the company's"
          ],
          "analysis_quality": "low"
        },
        "quality_assessment": {
          "score": 95,
          "grade": "A",
          "feedback": "Exceptional RCA (95/100). Strongest area: Root Cause Analysis. Consider sharing as template. Minor enhancement opportunity: Action Items Prevention.",
          "strengths": [
            "Timeline Detection: Detailed timestamps provided",
            "Timeline Detection: Dedicated timeline section",
            "Timeline Detection: Detection methods described"
          ],
          "critical_gaps": [
            "None identified"
          ]
        },
        "business_impact": {
          "impact_score": 4,
          "customer_count_affected": "16 users",
          "revenue_impact_est": "Low: $1K-5K potential impact",
          "service_downtime_minutes": 30,
          "severity_justification": "Moderate business impact; Service interruption (30 minutes)"
        },
        "technical_analysis": {
          "root_cause_category": "Process/Human Error",
          "detection_time_minutes": 0,
          "resolution_time_minutes": 0,
          "technical_debt_level": "Medium - Some improvements required",
          "automation_score": 3
        }
      }
    },
    {
      "jira_data": {
        "ticket_key": "IR-374",
        "summary": "First Week Goal workflow is not transitioning to the correct state.",
        "priority": "P3",
        "created_date": "2025-07-24T09:36:57.310-0700",
        "status": "Follow Up Work",
        "description": "First Week Goal workflow is not transitioning to the correct state.",
        "custom_fields": {
          "incident_urgency": "P3",
          "pods_engaged": "Workflows, Responsible: Workflows (Care Hub)",
          "reporter": null,
          "assignee": null
        },
        "comments": [],
        "metadata": {
          "jira_url": "https://hingehealth.atlassian.net/browse/IR-374",
          "labels": [],
          "component": null,
          "resolution": null
        }
      },
      "confluence_data": {
        "rca_url": "https://hingehealth.atlassian.net/wiki/spaces/RND/pages/1621688511/RCA+IR-374+First+Week+Goal+workflow+is+not+transitioning+to+the+correct+state.",
        "rca_available": true,
        "content_html": "<p><strong>Start date:</strong>&nbsp;July 7 10:50 AM PST</p><p><strong>Close date:</strong>&nbsp;July 25 4:30 AM PST</p><p><strong>Severity</strong>:&nbsp;P3</p><p /><p><strong><span style=\"color: rgb(0,102,68);\">NOTE: that this doc is written within 2 weeks of the incident happening. After this is written a review is setup with senior tech leaders in the org.</span></strong></p><p><strong>Author</strong>: <ac:link><ri:user ri:account-id=\"712020:70a30034-8ebe-41b2-8439-fb874d2f12a7\" ri:local-id=\"2c5dceae-6eec-4baf-9a60-4db026599427\" /></ac:link> </p><p>&nbsp;</p><p><strong><span style=\"color: rgb(255,86,48);\">NOTE: Delete the instruction text in each section as you fill it out.</span></strong></p><h2>Exec Summary</h2><p>On July 24th, QA identified FWG workflows not appearing timely in Carehub UI during regression testing, initially blocking the weekly release. Investigation showed workflows were generated but not transitioning to visible states due to a faulty timer mechanism in the User Workflow Service. After the release proceeded and the issue reproduced in production, triage team determined that timer-dependent workflows weren't transitioning correctly post-expiry. The problem was resolved on July 25th through an emergency patch deployment and data migration to emit missed events and correct affected workflow due dates.</p><h2>Impact</h2><ul><li><p><strong>Customer Impact: </strong>CTMS were not able to see the FWG and pelvic trainer outreach workflows on Carehub UI</p></li><li><p><strong>Security breach: </strong>NA</p></li><li><p><strong>Internal Impact: </strong>17k FWG workflows and 1.2k pelvic trainer outreach workflows were delayed in Carehub UI</p></li></ul><h2>Timeline (Pacific time, with full timestamps)</h2><p><em><strong>2025-07-07 03:39 PM IST</strong>: </em><strong>Incident began</strong>, <a href=\"https://github.com/hinge-health/user-workflow-service/pull/2109\" data-card-appearance=\"inline\">https://github.com/hinge-health/user-workflow-service/pull/2109</a>  PR<ac:inline-comment-marker ac:ref=\"fe174e71-0751-47d4-bfe4-67b70a20440a\"> which transitioned from eslint to oxlint as part of performance improvement initiative was merged</ac:inline-comment-marker></p><p><em><strong>2025-07-09 10:12 PM IST: </strong></em>QA team reported that pelvic trainer outreach workflow wasn&rsquo;t triggering on stage env, even reported failures in automation test suite and after sometime confirmed the same issue in prod env, cut a ticket <ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"17846c1a-43fd-47ef-b2d3-f42277761b5f\"><ac:parameter ac:name=\"key\">MERU-4976</ac:parameter><ac:parameter ac:name=\"serverId\">b75f938a-9135-3e8a-b70b-9bd825e38cc0</ac:parameter><ac:parameter ac:name=\"server\">System Jira</ac:parameter></ac:structured-macro>  and called it out as a blocker for carehub weekly release.</p><p><em><strong>2025-07-09 11:21 PM IST: </strong></em>Developers confirmed that the workflow was being generated in both stage and prod environments. The workflow is designed to transition after 7 days, and the remaining workflows were scheduled to transition at their expected times. Based on this, the issue was considered to be behaving as expected at that point in time, and the QA team deprioritized it.</p><p><em><strong>2025-07-24 11:57 AM IST: </strong></em>QA team reported that FWG(first week goal) workflow was showing up in Carehub UI with a delay intermittently, cut a ticket <ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"50c5dcd1-863e-4174-bfa9-3c2ad7b34489\"><ac:parameter ac:name=\"key\">MERU-5177</ac:parameter><ac:parameter ac:name=\"serverId\">b75f938a-9135-3e8a-b70b-9bd825e38cc0</ac:parameter><ac:parameter ac:name=\"server\">System Jira</ac:parameter></ac:structured-macro>  and called it out as a blocker for the weekly release.</p><p><em><strong>2025-07-24 04:21 PM IST:  </strong></em>On initial investigation, it looked like a redis issue in stage environment given that the workflows were showing up on the UI after some delay intermittently. Since UWS is on CD and workflows were created after a delay, The issue was deprioritized and the team decided to proceed with the weekly release of other carehub services and validate the issue on Prod. </p><p><em><strong>2025-07-24 07:48 PM IST</strong>:  </em><strong>Incident detected</strong>, QE confirmed that the issue is reproducible on prod after the deployment. On checking the prod database, it was found that there are ~17k workflows which did not transition to correct state after the scheduled time. The prod logs were also missing for the workflow transition process which validated the bug. The triage team was assembled to look further into it from different angles.</p><p><em><strong>2025-07-24 10:07 PM IST</strong>:  </em><strong>Incident reported</strong>, and the triage team continued looking into possible causes and understand the actual impact.</p><p><em><strong>2025-07-25 12:30 PM IST</strong>:  </em>Triage team found the root cause and raised a patch <a href=\"https://github.com/hinge-health/user-workflow-service/pull/2146\">PR</a> to fix the delayed timer issue.</p><p><em><strong>2025-07-25 12:57 PM IST</strong>:  </em>QA team confirmed that the fix is working and the issue is mitigated on stage environment.</p><p><em><strong>2025-07-25 01:19 PM IST</strong>:  </em><strong>Incident mitigated,</strong> <ac:inline-comment-marker ac:ref=\"8dee81f1-13b9-4411-a727-ebc645b6d481\">The </ac:inline-comment-marker><ac:inline-comment-marker ac:ref=\"8dee81f1-13b9-4411-a727-ebc645b6d481\"><a href=\"https://github.com/hinge-health/user-workflow-service/pull/2146\" data-card-appearance=\"inline\">https://github.com/hinge-health/user-workflow-service/pull/2146</a></ac:inline-comment-marker><ac:inline-comment-marker ac:ref=\"8dee81f1-13b9-4411-a727-ebc645b6d481\">  PR to patch</ac:inline-comment-marker><strong><ac:inline-comment-marker ac:ref=\"8dee81f1-13b9-4411-a727-ebc645b6d481\"> </ac:inline-comment-marker></strong><ac:inline-comment-marker ac:ref=\"8dee81f1-13b9-4411-a727-ebc645b6d481\">the issue was merged and was deployed to prod env and QA team validated the fix to be working in prod env.</ac:inline-comment-marker></p><p><em><strong>2025-07-25 03:29 PM IST</strong></em>: <a href=\"https://github.com/hinge-health/user-workflow-service/pull/2145\" data-card-appearance=\"inline\">https://github.com/hinge-health/user-workflow-service/pull/2145</a>  PR to emit the respective events to transition the workflows to correct state was merged.</p><p><em><strong>2025-07-25 04:53 PM IST</strong></em>: <a href=\"https://github.com/hinge-health/user-workflow-service/pull/2148\" data-card-appearance=\"inline\">https://github.com/hinge-health/user-workflow-service/pull/2148</a>  data migration PR to update the due date of the transitioned workflows was merged.</p><p><em><strong>2025-07-25 05:03 PM IST</strong>:  </em><strong>Incident closed</strong></p><p /><h2>5 WHYs - Why did this incident happen?</h2><ol start=\"1\"><li><p><strong>Why did workflows fail to transition from one state to another?</strong></p></li></ol><ul><li><p>Because the timer mechanism in UWS failed to function properly, preventing workflows from transitioning to the appropriate state post expiry.</p></li></ul><ol start=\"2\"><li><p><strong>Why did the timer mechanism fail?</strong>&nbsp;</p></li></ol><ul><li><p>Because automatic linting during the eslint to oxlint transition removed a critical variable from destructuring that was essential for the timer functionality.</p></li></ul><ol start=\"3\"><li><p><strong>Why wasn't this breaking change detected earlier before an incident was created?</strong></p></li></ol><ul><li><p>Because the initial bug report on July 9th for pelvic trainer outreach was incorrectly assessed as being isolated to the staging environment and was deprioritized, preventing proper investigation of what was actually a systemic issue affecting the timer mechanism.</p></li></ul><ol start=\"4\"><li><p><strong>Why didn't the code review process catch this critical issue?</strong></p></li></ol><ul><li><p>Because the large scope of the PR made it difficult for reviewers to thoroughly assess all automated changes, and there was likely insufficient focus on verifying that automatic linting changes didn't break runtime dependencies, especially for complex mechanisms like timers.</p></li></ul><ol start=\"5\"><li><p><strong>Why didn't automated testing or monitoring catch this runtime dependency break?</strong></p></li></ol><ul><li><p>Because the existing test suite lacked comprehensive integration tests that validated timer-dependent workflow state transitions, and there were no monitoring setup to catch the absence of workflow transitions.</p></li></ul><h2>Architectural Context</h2><p>&nbsp;<ac:link ac:card-appearance=\"inline\"><ri:page ri:content-title=\"User Workflow Service - Quick Guide\" ri:version-at-save=\"15\" /><ac:link-body>User Workflow Service - Quick Guide</ac:link-body></ac:link> </p><h2>Root Cause Analysis</h2><h3>Major Problems Identified</h3><p>List the major issues that the analysis identified.</p><ul><li><p><strong>Immediate Cause</strong>: PR to migrate eslint to oxlint as part of performance improvement initiative.</p></li><li><p><strong>Contributing Factors</strong>: Human error and inadequate integration tests to validate timer-dependent workflow state transitions</p></li><li><p><strong>Underlying Causes:</strong></p><ul><li><p><strong>Technical Infrastructure:</strong> NA</p></li><li><p><strong>Processes and Procedures</strong>: NA</p></li><li><p><strong>Human Factors</strong>: <em><span style=\"color: rgb(151,160,175);\">Consider the role of human factors, such as training deficiencies, communication issues, or decision-making processes, in the incident.</span></em></p></li></ul></li></ul><h2>Lessons Learned</h2><h4>Actionable Insights:</h4><p>Summarize the key insights and lessons learned from the 5 Whys analysis. Focus on actionable recommendations and corrective actions that can be implemented to prevent similar incidents in the future. Include recommendations for process improvements, system enhancements, training needs, and any other measures deemed necessary based on the analysis.</p><ul><li><p>Setup monitoring to detect any anomalies in workflow transition from passive states</p></li><li><p>&nbsp;Develop integration tests specifically validating timer-dependent workflow state transitions and other critical runtime dependencies</p></li><li><p>Establish guidelines limiting PR scope for automated changes to enable thorough manual review</p></li></ul><h4>Time to Detection</h4><p>&nbsp;~2h 20m</p><h4>Time to Resolution</h4><p>&nbsp;~15h</p><h2>Action Items</h2><p>Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:</p><ul><li><p><strong><ac:inline-comment-marker ac:ref=\"384a2407-1574-432e-9e99-eef0c3e36841\">Technical Solutions:</ac:inline-comment-marker></strong><ac:inline-comment-marker ac:ref=\"384a2407-1574-432e-9e99-eef0c3e36841\"> </ac:inline-comment-marker></p></li></ul><ac:task-list>\n<ac:task>\n<ac:task-id>1</ac:task-id>\n<ac:task-uuid>34430da3-2ed0-46fe-97b1-ccfa13eb9d77</ac:task-uuid>\n<ac:task-status>complete</ac:task-status>\n<ac:task-body><span class=\"placeholder-inline-tasks\">Patch fix: <a href=\"https://github.com/hinge-health/user-workflow-service/pull/2146\" data-card-appearance=\"inline\">https://github.com/hinge-health/user-workflow-service/pull/2146</a> </span></ac:task-body>\n</ac:task>\n<ac:task>\n<ac:task-id>2</ac:task-id>\n<ac:task-uuid>3cb42ce2-0242-4313-842c-a64b8693a526</ac:task-uuid>\n<ac:task-status>complete</ac:task-status>\n<ac:task-body><span class=\"placeholder-inline-tasks\">Script to emit transition events: <a href=\"https://github.com/hinge-health/user-workflow-service/pull/2145\" data-card-appearance=\"inline\">https://github.com/hinge-health/user-workflow-service/pull/2145</a> </span></ac:task-body>\n</ac:task>\n<ac:task>\n<ac:task-id>3</ac:task-id>\n<ac:task-uuid>9d6bbee1-4e32-48a4-bb25-b2be257ad2f3</ac:task-uuid>\n<ac:task-status>complete</ac:task-status>\n<ac:task-body><span class=\"placeholder-inline-tasks\">Data migration to update workflow due dates: <a href=\"https://github.com/hinge-health/user-workflow-service/pull/2148\" data-card-appearance=\"inline\">https://github.com/hinge-health/user-workflow-service/pull/2148</a> </span></ac:task-body>\n</ac:task>\n</ac:task-list><ul><li><p><strong>Process Improvements:</strong></p><ul><li><p>Break large PRs into smaller ones for ease of review</p></li></ul></li><li><p><strong>Training and Education: </strong>NA</p></li><li><p><strong>Monitoring and Alerting:</strong></p><ul><li><p>Setup Mode report to identify workflows in bad states</p><ul><li><p>Owner: <ac:link><ri:user ri:account-id=\"712020:e1dbe56c-0d43-4c28-bd7d-dd50759cc1a8\" ri:local-id=\"82a8b7e2-2339-4048-b0cb-b837704346fb\" /></ac:link> </p></li><li><p><ac:inline-comment-marker ac:ref=\"86404d4a-cbeb-45a3-8006-4ee53f2dc80d\">JIRA</ac:inline-comment-marker>: <ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"2b9aed7a-69cd-4a12-9b69-648372cc76c1\"><ac:parameter ac:name=\"key\">OPSDATA-910</ac:parameter><ac:parameter ac:name=\"serverId\">b75f938a-9135-3e8a-b70b-9bd825e38cc0</ac:parameter><ac:parameter ac:name=\"server\">System Jira</ac:parameter></ac:structured-macro> </p><ac:task-list>\n<ac:task>\n<ac:task-id>4</ac:task-id>\n<ac:task-uuid>8e895381-7bdd-4031-bde0-eba4edd5d696</ac:task-uuid>\n<ac:task-status>complete</ac:task-status>\n<ac:task-body><span class=\"placeholder-inline-tasks\">Completed</span></ac:task-body>\n</ac:task>\n</ac:task-list></li><li><p><a href=\"https://app.mode.com/hinge_health/reports/7822aa515578\">https://app.mode.com/hinge_health/reports/7822aa515578</a></p></li></ul></li><li><p>Implement anomaly based monitors on passive state transitions</p><ul><li><p>Owner: <ac:link><ri:user ri:account-id=\"641dd3d89d2bc6c90a8bb27d\" ri:local-id=\"16b0135d-4722-42c3-8d00-6e302de0edb8\" /></ac:link> </p></li><li><p>JIRA: <ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"94f3fe22-d7fd-43d0-949e-644282e5fae6\"><ac:parameter ac:name=\"key\">NAN-5061</ac:parameter><ac:parameter ac:name=\"serverId\">b75f938a-9135-3e8a-b70b-9bd825e38cc0</ac:parameter><ac:parameter ac:name=\"server\">System Jira</ac:parameter></ac:structured-macro> </p><ac:task-list>\n<ac:task>\n<ac:task-id>5</ac:task-id>\n<ac:task-uuid>fe192056-7bff-492f-a7c6-2b22c14df7ed</ac:task-uuid>\n<ac:task-status>complete</ac:task-status>\n<ac:task-body><span class=\"placeholder-inline-tasks\">Completed</span></ac:task-body>\n</ac:task>\n</ac:task-list></li></ul></li></ul></li><li><p><strong>Testing Improvements:</strong>&nbsp;</p><ul><li><p>Improve integration tests to validate timer-dependent workflow state transitions</p><ul><li><p>Owner: <ac:link><ri:user ri:account-id=\"712020:72cada1b-1910-4e7a-9ff8-75484d2c00e3\" ri:local-id=\"4868af82-b1d0-4866-8329-1ab80b01df00\" /></ac:link> </p></li><li><p>JIRA: <ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"19a6c9f2-cbe7-44c2-b52c-6eb668a1b961\"><ac:parameter ac:name=\"key\">MERU-5243</ac:parameter><ac:parameter ac:name=\"serverId\">b75f938a-9135-3e8a-b70b-9bd825e38cc0</ac:parameter><ac:parameter ac:name=\"server\">System Jira</ac:parameter></ac:structured-macro> </p></li><li><p>ETA: August 29th, 2025</p><ac:task-list>\n<ac:task>\n<ac:task-id>6</ac:task-id>\n<ac:task-uuid>05dbdcf8-cb13-4b1e-adec-bf0fcf06db88</ac:task-uuid>\n<ac:task-status>incomplete</ac:task-status>\n<ac:task-body><span class=\"placeholder-inline-tasks\">Pending</span></ac:task-body>\n</ac:task>\n</ac:task-list></li></ul></li></ul></li></ul><p>&nbsp;</p><hr /><h3>Post Mortem Notes:&nbsp;</h3><p>NA</p>",
        "content_text": "Start date: July 7 10:50 AM PSTClose date: July 25 4:30 AM PSTSeverity: P3NOTE: that this doc is written within 2 weeks of the incident happening. After this is written a review is setup with senior tech leaders in the org.Author: NOTE: Delete the instruction text in each section as you fill it out.Exec SummaryOn July 24th, QA identified FWG workflows not appearing timely in Carehub UI during regression testing, initially blocking the weekly release. Investigation showed workflows were generated but not transitioning to visible states due to a faulty timer mechanism in the User Workflow Service. After the release proceeded and the issue reproduced in production, triage team determined that timer-dependent workflows weren't transitioning correctly post-expiry. The problem was resolved on July 25th through an emergency patch deployment and data migration to emit missed events and correct affected workflow due dates.ImpactCustomer Impact: CTMS were not able to see the FWG and pelvic trainer outreach workflows on Carehub UISecurity breach: NAInternal Impact: 17k FWG workflows and 1.2k pelvic trainer outreach workflows were delayed in Carehub UITimeline (Pacific time, with full timestamps)2025-07-07 03:39 PM IST: Incident began, https://github.com/hinge-health/user-workflow-service/pull/2109 PR which transitioned from eslint to oxlint as part of performance improvement initiative was merged2025-07-09 10:12 PM IST: QA team reported that pelvic trainer outreach workflow wasn’t triggering on stage env, even reported failures in automation test suite and after sometime confirmed the same issue in prod env, cut a ticket MERU-4976b75f938a-9135-3e8a-b70b-9bd825e38cc0System Jira and called it out as a blocker for carehub weekly release.2025-07-09 11:21 PM IST: Developers confirmed that the workflow was being generated in both stage and prod environments. The workflow is designed to transition after 7 days, and the remaining workflows were scheduled to transition at their expected times. Based on this, the issue was considered to be behaving as expected at that point in time, and the QA team deprioritized it.2025-07-24 11:57 AM IST: QA team reported that FWG(first week goal) workflow was showing up in Carehub UI with a delay intermittently, cut a ticket MERU-5177b75f938a-9135-3e8a-b70b-9bd825e38cc0System Jira and called it out as a blocker for the weekly release.2025-07-24 04:21 PM IST: On initial investigation, it looked like a redis issue in stage environment given that the workflows were showing up on the UI after some delay intermittently. Since UWS is on CD and workflows were created after a delay, The issue was deprioritized and the team decided to proceed with the weekly release of other carehub services and validate the issue on Prod. 2025-07-24 07:48 PM IST: Incident detected, QE confirmed that the issue is reproducible on prod after the deployment. On checking the prod database, it was found that there are ~17k workflows which did not transition to correct state after the scheduled time. The prod logs were also missing for the workflow transition process which validated the bug. The triage team was assembled to look further into it from different angles.2025-07-24 10:07 PM IST: Incident reported, and the triage team continued looking into possible causes and understand the actual impact.2025-07-25 12:30 PM IST: Triage team found the root cause and raised a patch PR to fix the delayed timer issue.2025-07-25 12:57 PM IST: QA team confirmed that the fix is working and the issue is mitigated on stage environment.2025-07-25 01:19 PM IST: Incident mitigated, The https://github.com/hinge-health/user-workflow-service/pull/2146 PR to patch the issue was merged and was deployed to prod env and QA team validated the fix to be working in prod env.2025-07-25 03:29 PM IST: https://github.com/hinge-health/user-workflow-service/pull/2145 PR to emit the respective events to transition the workflows to correct state was merged.2025-07-25 04:53 PM IST: https://github.com/hinge-health/user-workflow-service/pull/2148 data migration PR to update the due date of the transitioned workflows was merged.2025-07-25 05:03 PM IST: Incident closed5 WHYs - Why did this incident happen?Why did workflows fail to transition from one state to another?Because the timer mechanism in UWS failed to function properly, preventing workflows from transitioning to the appropriate state post expiry.Why did the timer mechanism fail? Because automatic linting during the eslint to oxlint transition removed a critical variable from destructuring that was essential for the timer functionality.Why wasn't this breaking change detected earlier before an incident was created?Because the initial bug report on July 9th for pelvic trainer outreach was incorrectly assessed as being isolated to the staging environment and was deprioritized, preventing proper investigation of what was actually a systemic issue affecting the timer mechanism.Why didn't the code review process catch this critical issue?Because the large scope of the PR made it difficult for reviewers to thoroughly assess all automated changes, and there was likely insufficient focus on verifying that automatic linting changes didn't break runtime dependencies, especially for complex mechanisms like timers.Why didn't automated testing or monitoring catch this runtime dependency break?Because the existing test suite lacked comprehensive integration tests that validated timer-dependent workflow state transitions, and there were no monitoring setup to catch the absence of workflow transitions.Architectural Context User Workflow Service - Quick Guide Root Cause AnalysisMajor Problems IdentifiedList the major issues that the analysis identified.Immediate Cause: PR to migrate eslint to oxlint as part of performance improvement initiative.Contributing Factors: Human error and inadequate integration tests to validate timer-dependent workflow state transitionsUnderlying Causes:Technical Infrastructure: NAProcesses and Procedures: NAHuman Factors: Consider the role of human factors, such as training deficiencies, communication issues, or decision-making processes, in the incident.Lessons LearnedActionable Insights:Summarize the key insights and lessons learned from the 5 Whys analysis. Focus on actionable recommendations and corrective actions that can be implemented to prevent similar incidents in the future. Include recommendations for process improvements, system enhancements, training needs, and any other measures deemed necessary based on the analysis.Setup monitoring to detect any anomalies in workflow transition from passive states Develop integration tests specifically validating timer-dependent workflow state transitions and other critical runtime dependenciesEstablish guidelines limiting PR scope for automated changes to enable thorough manual reviewTime to Detection ~2h 20mTime to Resolution ~15hAction ItemsBased on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:Technical Solutions: 1 34430da3-2ed0-46fe-97b1-ccfa13eb9d77 complete Patch fix: https://github.com/hinge-health/user-workflow-service/pull/2146 2 3cb42ce2-0242-4313-842c-a64b8693a526 complete Script to emit transition events: https://github.com/hinge-health/user-workflow-service/pull/2145 3 9d6bbee1-4e32-48a4-bb25-b2be257ad2f3 complete Data migration to update workflow due dates: https://github.com/hinge-health/user-workflow-service/pull/2148 Process Improvements:Break large PRs into smaller ones for ease of reviewTraining and Education: NAMonitoring and Alerting:Setup Mode report to identify workflows in bad statesOwner: JIRA: OPSDATA-910b75f938a-9135-3e8a-b70b-9bd825e38cc0System Jira 4 8e895381-7bdd-4031-bde0-eba4edd5d696 complete Completed https://app.mode.com/hinge_health/reports/7822aa515578Implement anomaly based monitors on passive state transitionsOwner: JIRA: NAN-5061b75f938a-9135-3e8a-b70b-9bd825e38cc0System Jira 5 fe192056-7bff-492f-a7c6-2b22c14df7ed complete Completed Testing Improvements: Improve integration tests to validate timer-dependent workflow state transitionsOwner: JIRA: MERU-5243b75f938a-9135-3e8a-b70b-9bd825e38cc0System Jira ETA: August 29th, 2025 6 05dbdcf8-cb13-4b1e-adec-bf0fcf06db88 incomplete Pending Post Mortem Notes: NA",
        "title": "RCA: IR-374 First Week Goal workflow is not transitioning to the correct state.",
        "page_metadata": {
          "space": "RND",
          "page_id": "1621688511",
          "estimated_word_count": 1095
        }
      },
      "analysis_results": {
        "content_analysis": {
          "incident_summary": "On July 24th, QA identified FWG workflows not appearing timely in Carehub UI during regression testing, initially blocking the weekly release. Investigation showed workflows were generated but not transitioning to visible states due to a faulty timer mechanism in the User Workflow Service.",
          "users_impacted": "User impact details not clearly specified in RCA document",
          "root_causes": [
            "IdentifiedList the major issues that the analysis identified",
            "PR to migrate eslint to oxlint as part of performance improvement initiative",
            "Database connectivity issues",
            "Deployment failure"
          ],
          "analysis_quality": "low"
        },
        "quality_assessment": {
          "score": 81,
          "grade": "B",
          "feedback": "Solid RCA (81/100) with good foundation. Strong: Root Cause Analysis. Focus improvement on: Impact Assessment. Address: Business impact not addressed.",
          "strengths": [
            "Timeline Detection: Detailed timestamps provided",
            "Timeline Detection: Dedicated timeline section",
            "Timeline Detection: Detection methods described"
          ],
          "critical_gaps": [
            "Impact Assessment: Business impact not addressed"
          ]
        },
        "business_impact": {
          "impact_score": 3,
          "customer_count_affected": "User impact not specified",
          "revenue_impact_est": "Minimal: <$1K potential impact",
          "service_downtime_minutes": 0,
          "severity_justification": "Limited business impact"
        },
        "technical_analysis": {
          "root_cause_category": "Deployment Issue",
          "detection_time_minutes": 0,
          "resolution_time_minutes": 0,
          "technical_debt_level": "Low - Minor cleanup needed",
          "automation_score": 5
        }
      }
    },
    {
      "jira_data": {
        "ticket_key": "IR-373",
        "summary": "AQ/mobileapp/carehub Was inaccessible to members and coaches.",
        "priority": "P2",
        "created_date": "2025-07-22T11:18:58.718-0700",
        "status": "Root Cause Analysis",
        "description": null,
        "custom_fields": {
          "incident_urgency": "P2",
          "pods_engaged": "Access (EBB), Program Experience, Onboarding, Security",
          "reporter": null,
          "assignee": null
        },
        "comments": [],
        "metadata": {
          "jira_url": "https://hingehealth.atlassian.net/browse/IR-373",
          "labels": [],
          "component": null,
          "resolution": null
        }
      },
      "confluence_data": {
        "rca_url": "https://hingehealth.atlassian.net/wiki/spaces/RND/pages/1616052558/RCA+IR-373+Carehub+and+admin+panel+not+loading",
        "rca_available": true,
        "content_html": "<p><strong>Start date:</strong>&nbsp;July 22, 2025</p><p><strong>Close date:</strong>&nbsp;July 22, 2025</p><p><strong>Severity</strong>:&nbsp;P2</p><p /><p><strong>NOTE: that this doc is written within 2 weeks of the incident happening. After this is written a review is setup with senior tech leaders in the org.</strong></p><p><strong>Author</strong>: Yufei Fu</p><p>&nbsp;<strong>NOTE: Delete the instruction text in each section as you fill it out.</strong></p><h2>Client Facing Summary (if necessary):</h2><p>During the incident, members were unable to load the application. Per the authentication logs in Okta , as traffic was redirected there, the impact of this incident was <ac:inline-comment-marker ac:ref=\"44ecb016-410e-4f1b-b88c-d8cd5d4a8161\">173 users</ac:inline-comment-marker>. <ac:inline-comment-marker ac:ref=\"1e909c1c-d5d8-48d2-ada9-7326ef489987\">This is not an estimated number,  instead these are recorded as attempted authentications in Okta. </ac:inline-comment-marker></p><p>Pending : We are getting other numbers from people that might&rsquo;ve been affected from losing connectivity to backend services waiting on Data Science team. </p><h2>Exec Summary</h2><p>A change to Cloudflare origin rules briefly redirected all traffic to Retool instead of the intended target. Phoenix-bff, operations-hub-bff, airflow, were excluded. </p><h2>Impact</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"7aa30a3e-a672-4e33-b499-70af7919c220\"><ac:rich-text-body><p>Fill out the details of the impact of the incident starting with the end user/customer and then working from there to the business&nbsp; impact. Be as specific as you can (X customers impacted, Y transactions failed, $Z lost&hellip;Don't put a lot of details about how or why the incident happened, but just focus on the impact. Keep the why for later. For example, &quot;users could not log in for 30 mins because of auth service unavailability&quot; instead of &quot;one aws zone (us-east-02) had a drop in availability and our load balancing setup had a bug so any requests for log in going to us-east-02 failed for 30 mins&quot;.)</p></ac:rich-text-body></ac:structured-macro><ul><li><p><strong>Customer Impact: </strong>Services were not reachable for 10 minutes between 11:09AM to 11:19AM PST. </p></li><li><p><strong>Business Impact: </strong>Backend services were unreachable, which may have caused unexpected behavior on requests.</p></li><li><p><strong>Internal Impact: </strong>Internal apps behind the WAF were redirected to retool.</p></li></ul><h2>Timeline (Pacific time, with full timestamps)</h2><p><strong>2025-07-26 11:08AM  PST</strong>: Erroneous Cloudflare rule was deployed to onboard an application from the migration effort from F5. </p><p><strong>2025-07-26 11:09AM</strong>: Traffic started redirecting to the incorrect destination due to the deployed rule. </p><p><strong>2025-07-26 11:13AM</strong>: Reports from #it_support channel that Admin panel was not working. Followed by other reports of different apps being down. </p><p><strong>2025-07-26 11:18AM: </strong>Cloudflare rule reverted, and traffic was resuming correctly. </p><p><strong>2025-07-26 11:19AM: </strong>Monitor services for expected behavior and flow of network requests. Incident Mitigated</p><p><strong>2025-07-26 11:23: </strong>Confirmed and closed incident. </p><p>Specifically record, when <strong>Incident Detected/Reported </strong>and when <strong>Incident Mitigated</strong></p><h2>5 WHYs - Why did this incident happen?</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"12955489-8019-47d9-8c97-7af016b2c6c4\"><ac:rich-text-body><p>Start from how the incident was discovered and then work down from there. Note that there may be a few branches of 5 Whys for issues that have multiple root causes. Make sure you don&rsquo;t answer the question in the first why. Let the trail continue till you get to the root issue and then the root issue has to mandatorily have an action item/lesson learned. This should also result in answers to: What was the technical issue that caused the incident? What was the reason the incident wasn't mitigated sooner?</p></ac:rich-text-body></ac:structured-macro><ol start=\"1\"><li><p>Why:&nbsp; Internal tool traffic was redirected to Retool. External traffic (except the the services noted above) were also affected, Including the Hinge Health app.</p></li><li><p>Why:&nbsp;An incorrect Cloudflare rule was directly pushed to prod that redirected all traffic.</p></li><li><p>Why:&nbsp;In an attempt to rapidly onboard an application to Cloudflare, mistake was made by selecting &ldquo;Apply to all traffic&rdquo; instead of &ldquo;Apply to specific traffic&rdquo;.</p></li><li><p>Why:&nbsp;The usual Terraform approach had issues with the types and did not want to build, and would result in a longer delay to onboard the applications. The application needed to onboard quickly per business needs. </p></li><li><p>Why:&nbsp;We had a hybrid approach to management of Cloudflare due to the migration, and emergency change that took place for that migration. </p></li></ol><h2>Architectural Context</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"34f5b646-366a-495f-adc5-fe26ded13b55\"><ac:rich-text-body><p>Give background and architectural context of the service/data flow in question, this helps reader understand the whole picture.</p></ac:rich-text-body></ac:structured-macro><p>&nbsp;</p><h2>Root Cause Analysis</h2><h3>Major Problems Identified</h3><p>List the major issues that the analysis identified.</p><ul><li><p><strong>Immediate Cause</strong>: Manual changes. </p></li><li><p><strong>Contributing Factors</strong>: Unexpected expedited migration to Cloudflare from F5 that resulted in a hybrid of IaC and manual changes. </p></li><li><p><strong>Underlying Causes:</strong></p><ul><li><p><strong>Technical Infrastructure:</strong> NA</p></li><li><p><strong>Processes and Procedures</strong>: Completely move management of Cloudflare resources to Terraform, and terraform only. Create templates in IaC that disallow unwanted configurations, so this is not accidentally configured incorrectly. </p></li><li><p><strong>Human Factors</strong>: Poor decisioning as a result of attempting to move quickly. </p></li></ul></li></ul><h2>Lessons Learned</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"e4b3e239-f638-463b-8200-1d2aaffbb944\"><ac:rich-text-body><p>The lessons learned from the 5 Whys and potentially more get added as a list. Add a mandatory section on how Time to Detection and Time to Resolution can be cut in half.</p><p>Focus on how lessons learnt become a part of our dev/deploy/monitoring flow, what steps we have taken to integrate those lessons.</p></ac:rich-text-body></ac:structured-macro><h4>Actionable Insights:</h4><p>Summarize the key insights and lessons learned from the 5 Whys analysis. Focus on actionable recommendations and corrective actions that can be implemented to prevent similar incidents in the future. Include recommendations for process improvements, system enhancements, training needs, and any other measures deemed necessary based on the analysis.</p><ul><li><p>Align dev, stage, prod Terraform for Cloudflare to be the exact same.</p></li><li><p>Remove all permissions that can allow for manual changes, admin access only by request and approval.</p></li><li><p>Deploy of WAF configurations will be done via SRE team. </p></li></ul><h4>Time to Detection</h4><p>4 minutes. &nbsp;</p><h4>Time to Resolution</h4><p>&nbsp;10 minutes. </p><h2>Action Items</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"9576062f-8009-45fa-aab0-eb7253d62281\"><ac:rich-text-body><p>Action items from the 5 Whys section get added here. Every action item needs to have an owner, a JIRA ticket and an ETA.</p></ac:rich-text-body></ac:structured-macro><p>Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:</p><ul><li><p><strong>Technical Solutions:</strong> <ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"876b6418-ae5e-4efd-9432-2d8810233a41\"><ac:parameter ac:name=\"key\">SECIAM-3359</ac:parameter><ac:parameter ac:name=\"serverId\">b75f938a-9135-3e8a-b70b-9bd825e38cc0</ac:parameter><ac:parameter ac:name=\"server\">System Jira</ac:parameter></ac:structured-macro> - ETA <time datetime=\"2025-07-31\" /></p></li><li><p><strong>Process Improvements: </strong>Set specific maintenance windows for production level changes for Cloudflare. Per the following document <ac:link ac:card-appearance=\"inline\"><ri:page ri:space-key=\"SRECPUS\" ri:content-title=\"Scheduled Maintenance Procedure\" ri:version-at-save=\"9\" /><ac:link-body>Scheduled Maintenance Procedure</ac:link-body></ac:link> - Immediate</p></li></ul><p><ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"15f909a9-41d9-494d-9db0-37c89b4ac6a1\"><ac:parameter ac:name=\"key\">SECIAM-3598</ac:parameter><ac:parameter ac:name=\"serverId\">b75f938a-9135-3e8a-b70b-9bd825e38cc0</ac:parameter><ac:parameter ac:name=\"server\">System Jira</ac:parameter></ac:structured-macro> - <time datetime=\"2025-08-15\" /></p><ul><li><p><strong>Training and Education: </strong>NA</p></li><li><p><strong>Monitoring and Alerting:</strong>  <ac:structured-macro ac:name=\"jira\" ac:schema-version=\"1\" ac:macro-id=\"1e689b7f-3e22-4760-9ba7-d1594716da0c\"><ac:parameter ac:name=\"key\">SECIAM-3542</ac:parameter><ac:parameter ac:name=\"serverId\">b75f938a-9135-3e8a-b70b-9bd825e38cc0</ac:parameter><ac:parameter ac:name=\"server\">System Jira</ac:parameter></ac:structured-macro> - <time datetime=\"2025-07-31\" /></p></li><li><p><strong>Testing Improvements:</strong>&nbsp; </p></li></ul><p>&nbsp;</p><hr /><h3>Post Mortem Notes:&nbsp;</h3>",
        "content_text": "Start date: July 22, 2025Close date: July 22, 2025Severity: P2NOTE: that this doc is written within 2 weeks of the incident happening. After this is written a review is setup with senior tech leaders in the org.Author: Yufei Fu NOTE: Delete the instruction text in each section as you fill it out.Client Facing Summary (if necessary):During the incident, members were unable to load the application. Per the authentication logs in Okta , as traffic was redirected there, the impact of this incident was 173 users. This is not an estimated number, instead these are recorded as attempted authentications in Okta. Pending : We are getting other numbers from people that might’ve been affected from losing connectivity to backend services waiting on Data Science team. Exec SummaryA change to Cloudflare origin rules briefly redirected all traffic to Retool instead of the intended target. Phoenix-bff, operations-hub-bff, airflow, were excluded. ImpactFill out the details of the impact of the incident starting with the end user/customer and then working from there to the business  impact. Be as specific as you can (X customers impacted, Y transactions failed, $Z lost…Don't put a lot of details about how or why the incident happened, but just focus on the impact. Keep the why for later. For example, \"users could not log in for 30 mins because of auth service unavailability\" instead of \"one aws zone (us-east-02) had a drop in availability and our load balancing setup had a bug so any requests for log in going to us-east-02 failed for 30 mins\".)Customer Impact: Services were not reachable for 10 minutes between 11:09AM to 11:19AM PST. Business Impact: Backend services were unreachable, which may have caused unexpected behavior on requests.Internal Impact: Internal apps behind the WAF were redirected to retool.Timeline (Pacific time, with full timestamps)2025-07-26 11:08AM PST: Erroneous Cloudflare rule was deployed to onboard an application from the migration effort from F5. 2025-07-26 11:09AM: Traffic started redirecting to the incorrect destination due to the deployed rule. 2025-07-26 11:13AM: Reports from #it_support channel that Admin panel was not working. Followed by other reports of different apps being down. 2025-07-26 11:18AM: Cloudflare rule reverted, and traffic was resuming correctly. 2025-07-26 11:19AM: Monitor services for expected behavior and flow of network requests. Incident Mitigated2025-07-26 11:23: Confirmed and closed incident. Specifically record, when Incident Detected/Reported and when Incident Mitigated5 WHYs - Why did this incident happen?Start from how the incident was discovered and then work down from there. Note that there may be a few branches of 5 Whys for issues that have multiple root causes. Make sure you don’t answer the question in the first why. Let the trail continue till you get to the root issue and then the root issue has to mandatorily have an action item/lesson learned. This should also result in answers to: What was the technical issue that caused the incident? What was the reason the incident wasn't mitigated sooner?Why:  Internal tool traffic was redirected to Retool. External traffic (except the the services noted above) were also affected, Including the Hinge Health app.Why: An incorrect Cloudflare rule was directly pushed to prod that redirected all traffic.Why: In an attempt to rapidly onboard an application to Cloudflare, mistake was made by selecting “Apply to all traffic” instead of “Apply to specific traffic”.Why: The usual Terraform approach had issues with the types and did not want to build, and would result in a longer delay to onboard the applications. The application needed to onboard quickly per business needs. Why: We had a hybrid approach to management of Cloudflare due to the migration, and emergency change that took place for that migration. Architectural ContextGive background and architectural context of the service/data flow in question, this helps reader understand the whole picture. Root Cause AnalysisMajor Problems IdentifiedList the major issues that the analysis identified.Immediate Cause: Manual changes. Contributing Factors: Unexpected expedited migration to Cloudflare from F5 that resulted in a hybrid of IaC and manual changes. Underlying Causes:Technical Infrastructure: NAProcesses and Procedures: Completely move management of Cloudflare resources to Terraform, and terraform only. Create templates in IaC that disallow unwanted configurations, so this is not accidentally configured incorrectly. Human Factors: Poor decisioning as a result of attempting to move quickly. Lessons LearnedThe lessons learned from the 5 Whys and potentially more get added as a list. Add a mandatory section on how Time to Detection and Time to Resolution can be cut in half.Focus on how lessons learnt become a part of our dev/deploy/monitoring flow, what steps we have taken to integrate those lessons.Actionable Insights:Summarize the key insights and lessons learned from the 5 Whys analysis. Focus on actionable recommendations and corrective actions that can be implemented to prevent similar incidents in the future. Include recommendations for process improvements, system enhancements, training needs, and any other measures deemed necessary based on the analysis.Align dev, stage, prod Terraform for Cloudflare to be the exact same.Remove all permissions that can allow for manual changes, admin access only by request and approval.Deploy of WAF configurations will be done via SRE team. Time to Detection4 minutes.  Time to Resolution 10 minutes. Action ItemsAction items from the 5 Whys section get added here. Every action item needs to have an owner, a JIRA ticket and an ETA.Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:Technical Solutions: SECIAM-3359b75f938a-9135-3e8a-b70b-9bd825e38cc0System Jira - ETA Process Improvements: Set specific maintenance windows for production level changes for Cloudflare. Per the following document Scheduled Maintenance Procedure - ImmediateSECIAM-3598b75f938a-9135-3e8a-b70b-9bd825e38cc0System Jira - Training and Education: NAMonitoring and Alerting: SECIAM-3542b75f938a-9135-3e8a-b70b-9bd825e38cc0System Jira - Testing Improvements:   Post Mortem Notes:",
        "title": "RCA: IR-373 Carehub and admin panel not loading",
        "page_metadata": {
          "space": "RND",
          "page_id": "1616052558",
          "estimated_word_count": 932
        }
      },
      "analysis_results": {
        "content_analysis": {
          "incident_summary": "A change to Cloudflare origin rules briefly redirected all traffic to Retool instead of the intended target. Phoenix-bff, operations-hub-bff, airflow, were excluded.",
          "users_impacted": "User impact details not clearly specified in RCA document",
          "root_causes": [
            "IdentifiedList the major issues that the analysis identified",
            "Manual changes",
            "Unexpected expedited migration to Cloudflare from F5 that resulted in a hybrid of IaC and manual changes",
            "NAProcesses and Procedures: Completely move management of Cloudflare resources to Terraform, and terraform only",
            "Create templates in IaC that disallow unwanted configurations, so this is not accidentally configured incorrectly"
          ],
          "analysis_quality": "low"
        },
        "quality_assessment": {
          "score": 92,
          "grade": "A",
          "feedback": "Exceptional RCA (92/100). Strongest area: Root Cause Analysis. Consider sharing as template. Minor enhancement opportunity: Timeline Detection.",
          "strengths": [
            "Timeline Detection: Detailed timestamps provided",
            "Timeline Detection: Dedicated timeline section",
            "Timeline Detection: Detection methods described"
          ],
          "critical_gaps": [
            "None identified"
          ]
        },
        "business_impact": {
          "impact_score": 5,
          "customer_count_affected": "173 users",
          "revenue_impact_est": "Medium: $5K-25K potential impact",
          "service_downtime_minutes": 30,
          "severity_justification": "Moderate business impact; P2 incident with high urgency; Service interruption (30 minutes)"
        },
        "technical_analysis": {
          "root_cause_category": "Process/Human Error",
          "detection_time_minutes": 0,
          "resolution_time_minutes": 0,
          "technical_debt_level": "Medium - Some improvements required",
          "automation_score": 3
        }
      }
    },
    {
      "jira_data": {
        "ticket_key": "IR-372",
        "summary": "Configurator 2.0 is not loading for few PTs",
        "priority": "P4",
        "created_date": "2025-07-21T09:41:54.504-0700",
        "status": "Root Cause Analysis",
        "description": "PTs were unable to load member ET Plans in the Configurator 2.0\n\n!55fba9aa-89fb-42cb-915a-160123722cd2 (1).mov|width=1534,height=702,alt=\"55fba9aa-89fb-42cb-915a-160123722cd2 (1).mov\"!",
        "custom_fields": {
          "incident_urgency": "P4",
          "pods_engaged": "ET Experience (ETE), Member Record",
          "reporter": null,
          "assignee": null
        },
        "comments": [],
        "metadata": {
          "jira_url": "https://hingehealth.atlassian.net/browse/IR-372",
          "labels": [],
          "component": null,
          "resolution": null
        }
      },
      "confluence_data": {
        "rca_url": "https://hingehealth.atlassian.net/wiki/spaces/RND/pages/1612907732/RCA+IR-372+Configurator+2.0+is+not+loading+for+few+PTs",
        "rca_available": true,
        "content_html": "<p><strong>Start date:</strong>&nbsp;July 17th 12:06 PM</p><p><strong>Close date:</strong>&nbsp;July 17th 1:38 PM</p><p><strong>Severity</strong>:&nbsp;P4</p><p><strong>Adverse Event level (1-4)</strong>: </p><p><strong>Author: </strong>Mike Wroblewski&nbsp;</p><h2>Exec Summary</h2><p><em><span style=\"color: rgb(151,160,175);\">[A paragraph about the incident, how it was detected, how it was resolved along with a sentence on the impact]</span></em></p><p>On July 17th Physical Therapists began reporting that they were unable to load Exercise Therapy plans in CareHub and an incident was created.  It was quickly determined that a recently deployed PR in the Exercise Service was the cause of the incident and the commit before the offending PR was deployed to production to resolve the issue.  An official revert of the offending PR was later deployed to all environments.  There was no direct member impact, but during the incident PTs were unable to view or customize Exercise Therapy plans for members in CareHub.</p><h2>Impact</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"636290d3-d91d-4075-bdff-c5ee7316ea06\"><ac:rich-text-body><p>Fill out the details of the impact of the incident starting with the end user/customer and then working from there to the business&nbsp; impact. Be as specific as you can (X customers impacted, Y transactions failed, $Z lost&hellip;Don't put a lot of details about how or why the incident happened, but just focus on the impact. Keep the why for later. For example, &quot;users could not log in for 30 mins because of auth service unavailability&quot; instead of &quot;one aws zone (us-east-02) had a drop in availability and our load balancing setup had a bug so any requests for log in going to us-east-02 failed for 30 mins&quot;.)</p></ac:rich-text-body></ac:structured-macro><ul><li><p><strong>Customer Impact: </strong>Physical Therapists</p></li><li><p><strong>Security breach: </strong>No</p></li><li><p><strong>Business Impact: </strong>PTs were unable to view or customize Exercise Therapy plans for members in CareHub</p></li><li><p><strong>Internal Impact: </strong>Same as above</p></li></ul><h2>Timeline (Pacific time, with full timestamps)</h2><p><em><strong>YYYY-MM-DD HH:MM</strong>: Timeline entries (everything from inception of the incident through incident closure. Err on the side providing more detail than less)</em></p><p><em>Specifically record, when <strong>Incident Detected/Reported </strong>and when <strong>Incident Mitigated</strong></em></p><p>2025-07-17 12:06 PM PT - PTs begin reporting issues with Exercise Therapy plans in CareHub<br />2025-07-17 1:13 PM PT - Incident Reported<br />2025-07-17 1:20 PM PT - Offending PR in Exercise Service identified and rollback begins<br />2025-07-17 1:38 PM PT - Rollback complete and incident is resolved</p><h2>5 WHYs - Why did this incident happen?</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"598fc86c-b2d9-4ddb-a559-a9181f09193c\"><ac:rich-text-body><p>Start from how the incident was discovered and then work down from there. Note that there may be a few branches of 5 Whys for issues that have multiple root causes. Make sure you don&rsquo;t answer the question in the first why. Let the trail continue till you get to the root issue and then the root issue has to mandatorily have an action item/lesson learned. This should also result in answers to: What was the technical issue that caused the incident? What was the reason the incident wasn't mitigated sooner?</p></ac:rich-text-body></ac:structured-macro><ol start=\"1\"><li><p><strong>Why:</strong>&nbsp; Was there an incident?</p><ol start=\"1\"><li><p><strong>Answer:</strong>  Because a PR was deployed that began sending <code>exercise.dosage: null;</code> values from the <code>GET/POST v1/exercises</code> endpoint.</p></li><li><p><strong>Why:</strong>  Did we make this change if it could brake CareHub?</p><ol start=\"1\"><li><p><strong>Answer: </strong> This change did not actually break our API contract for those endpoints.  A <code>null</code> value for <code>dosage</code> has been an acceptable value for quite some time but the UI in CareHub does not appear to be able to handle that value.</p></li><li><p><strong>Why:</strong>  Didn&rsquo;t Exercise Service know that the UI in CareHub could not handle a <code>null</code> value</p><ol start=\"1\"><li><p><strong>Answer:</strong>  This is an area of improvement that we should address - when making changes to existing APIs, we need to verify with our consumer(s) that those changes can be handled by them.  In this case, we relied upon the API contract, but that clearly wasn&rsquo;t enough.</p></li></ol></li><li><p><strong>Why:</strong>  Doesn&rsquo;t CareHub handle a <code>null</code> value for <code>dosage</code> if it&rsquo;s an acceptable response value from our API?</p><ol start=\"1\"><li><p><strong>Answer:</strong>  We will need to ask them - there is probably a good reason why, but it&rsquo;s unknown at this time.</p></li></ol></li></ol></li><li><p><strong>Why:</strong>  Wasn&rsquo;t this tested in DEV before being deployed to PROD?</p><ol start=\"1\"><li><p><strong>Answer:</strong>  The <code>GET/POST v1/exercises</code> endpoints were tested in DEV, but only by directly hitting them via Postman.  This gave a false sense of security because the API contract held up, as expected.  However, we should have gone one step further by logging into CareHub and navigating to the view that actually calls the endpoints - this would have exposed the issue before we deployed to PROD.</p></li></ol></li></ol></li></ol><h2>Architectural Context</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"cbf25f8e-99c3-48a0-b493-0a5a69c90bff\"><ac:rich-text-body><p>Give background and architectural context of the service/data flow in question, this helps reader understand the whole picture.</p></ac:rich-text-body></ac:structured-macro><p>&nbsp;</p><h2>Root Cause Analysis</h2><h3>Major Problems Identified</h3><p>List the major issues that the analysis identified.</p><ul><li><p><strong>Immediate Cause</strong>: <em><span style=\"color: rgb(151,160,175);\">Describe the specific event or trigger that initiated the incident. This could include software updates, hardware failures, configuration changes, etc.</span></em><br />A PR in the Exercise Service was deployed to PROD that began sending <code>exercise.dosage: null</code> values for any exercise that didn&rsquo;t have v2 Dosage data for the <code>GET/POST v1/exercises</code> endpoints.  CareHub&rsquo;s UI for Exercise Therapy plans failed to load due to the <code>null</code> values, causing this incident.</p></li><li><p><strong>Contributing Factors</strong>: <em><span style=\"color: rgb(151,160,175);\">Identify any additional factors that contributed to the incident, such as human error, lack of redundancy, insufficient monitoring, or inadequate testing procedures.</span></em></p></li><li><p><strong>Underlying Causes:</strong></p><ul><li><p><strong>Technical Infrastructure:</strong> <em><span style=\"color: rgb(151,160,175);\">Assess the state of the company's technical infrastructure, including network stability, server performance, and software dependencies.</span></em></p></li><li><p><strong>Processes and Procedures</strong>: <em><span style=\"color: rgb(151,160,175);\">Evaluate the effectiveness of existing processes and procedures related to deployment, maintenance, and troubleshooting. Identify any gaps or weaknesses.</span></em></p></li><li><p><strong>Human Factors</strong>: <em><span style=\"color: rgb(151,160,175);\">Consider the role of human factors, such as training deficiencies, communication issues, or decision-making processes, in the incident.</span></em><br />The changes to the <code>GET/POST v1/exercises</code> endpoints were tested in DEV - via Postman - before deploying to PROD, but a full e2e test would have caught this issue.  If we would have logged into CareHub and navigated to the view that hit those endpoints, this incident could have been avoided.</p></li></ul></li></ul><h2>Lessons Learned</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"020aa167-7222-41f7-b43c-a887f349b6eb\"><ac:rich-text-body><p>The lessons learned from the 5 Whys and potentially more get added as a list. Add a mandatory section on how Time to Detection and Time to Resolution can be cut in half.</p><p>Focus on how lessons learnt become a part of our dev/deploy/monitoring flow, what steps we have taken to integrate those lessons.</p></ac:rich-text-body></ac:structured-macro><h4>Actionable Insights:</h4><p>Summarize the key insights and lessons learned from the 5 Whys analysis. Focus on actionable recommendations and corrective actions that can be implemented to prevent similar incidents in the future. Include recommendations for process improvements, system enhancements, training needs, and any other measures deemed necessary based on the analysis.</p><ul><li><p /></li><li><p>&nbsp;</p></li></ul><h4>Time to Detection</h4><p>&nbsp;</p><h4>Time to Resolution</h4><p>&nbsp;</p><h2>Action Items</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"027c3bec-6fd3-43db-b617-5c7884d347aa\"><ac:rich-text-body><p>Action items from the 5 Whys section get added here. Every action item needs to have an owner, a JIRA ticket and an ETA.</p></ac:rich-text-body></ac:structured-macro><p>Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:</p><ul><li><p><strong>Technical Solutions:</strong> <em><span style=\"color: rgb(151,160,175);\">Implementing upgrades, patches, or enhancements to improve system reliability, scalability, and performance.</span></em></p></li><li><p><strong>Process Improvements: </strong><em><span style=\"color: rgb(151,160,175);\">Reviewing and updating processes for change management, incident response, and disaster recovery.</span></em></p></li><li><p><strong>Training and Education: </strong><span style=\"color: rgb(151,160,175);\">Providing training programs or workshops to enhance technical skills, promote best practices, and improve communication within teams.</span></p></li><li><p><strong>Monitoring and Alerting:</strong> <em><span style=\"color: rgb(151,160,175);\">Enhancing monitoring tools and alerting mechanisms to detect issues proactively and minimize downtime.</span></em></p></li><li><p><strong>Testing Improvements:</strong>&nbsp; When making changes to existing APIs, we should require a full e2e test in DEV to ensure our changes don&rsquo;t impact our consumer apps/services - even if these changes don&rsquo;t break the API contract.</p></li></ul><p>&nbsp;</p><hr /><h3>Post Mortem Notes:&nbsp;</h3>",
        "content_text": "Start date: July 17th 12:06 PMClose date: July 17th 1:38 PMSeverity: P4Adverse Event level (1-4): Author: Mike Wroblewski Exec Summary[A paragraph about the incident, how it was detected, how it was resolved along with a sentence on the impact]On July 17th Physical Therapists began reporting that they were unable to load Exercise Therapy plans in CareHub and an incident was created. It was quickly determined that a recently deployed PR in the Exercise Service was the cause of the incident and the commit before the offending PR was deployed to production to resolve the issue. An official revert of the offending PR was later deployed to all environments. There was no direct member impact, but during the incident PTs were unable to view or customize Exercise Therapy plans for members in CareHub.ImpactFill out the details of the impact of the incident starting with the end user/customer and then working from there to the business  impact. Be as specific as you can (X customers impacted, Y transactions failed, $Z lost…Don't put a lot of details about how or why the incident happened, but just focus on the impact. Keep the why for later. For example, \"users could not log in for 30 mins because of auth service unavailability\" instead of \"one aws zone (us-east-02) had a drop in availability and our load balancing setup had a bug so any requests for log in going to us-east-02 failed for 30 mins\".)Customer Impact: Physical TherapistsSecurity breach: NoBusiness Impact: PTs were unable to view or customize Exercise Therapy plans for members in CareHubInternal Impact: Same as aboveTimeline (Pacific time, with full timestamps)YYYY-MM-DD HH:MM: Timeline entries (everything from inception of the incident through incident closure. Err on the side providing more detail than less)Specifically record, when Incident Detected/Reported and when Incident Mitigated2025-07-17 12:06 PM PT - PTs begin reporting issues with Exercise Therapy plans in CareHub2025-07-17 1:13 PM PT - Incident Reported2025-07-17 1:20 PM PT - Offending PR in Exercise Service identified and rollback begins2025-07-17 1:38 PM PT - Rollback complete and incident is resolved5 WHYs - Why did this incident happen?Start from how the incident was discovered and then work down from there. Note that there may be a few branches of 5 Whys for issues that have multiple root causes. Make sure you don’t answer the question in the first why. Let the trail continue till you get to the root issue and then the root issue has to mandatorily have an action item/lesson learned. This should also result in answers to: What was the technical issue that caused the incident? What was the reason the incident wasn't mitigated sooner?Why:  Was there an incident?Answer: Because a PR was deployed that began sending exercise.dosage: null; values from the GET/POST v1/exercises endpoint.Why: Did we make this change if it could brake CareHub?Answer: This change did not actually break our API contract for those endpoints. A null value for dosage has been an acceptable value for quite some time but the UI in CareHub does not appear to be able to handle that value.Why: Didn’t Exercise Service know that the UI in CareHub could not handle a null valueAnswer: This is an area of improvement that we should address - when making changes to existing APIs, we need to verify with our consumer(s) that those changes can be handled by them. In this case, we relied upon the API contract, but that clearly wasn’t enough.Why: Doesn’t CareHub handle a null value for dosage if it’s an acceptable response value from our API?Answer: We will need to ask them - there is probably a good reason why, but it’s unknown at this time.Why: Wasn’t this tested in DEV before being deployed to PROD?Answer: The GET/POST v1/exercises endpoints were tested in DEV, but only by directly hitting them via Postman. This gave a false sense of security because the API contract held up, as expected. However, we should have gone one step further by logging into CareHub and navigating to the view that actually calls the endpoints - this would have exposed the issue before we deployed to PROD.Architectural ContextGive background and architectural context of the service/data flow in question, this helps reader understand the whole picture. Root Cause AnalysisMajor Problems IdentifiedList the major issues that the analysis identified.Immediate Cause: Describe the specific event or trigger that initiated the incident. This could include software updates, hardware failures, configuration changes, etc.A PR in the Exercise Service was deployed to PROD that began sending exercise.dosage: null values for any exercise that didn’t have v2 Dosage data for the GET/POST v1/exercises endpoints. CareHub’s UI for Exercise Therapy plans failed to load due to the null values, causing this incident.Contributing Factors: Identify any additional factors that contributed to the incident, such as human error, lack of redundancy, insufficient monitoring, or inadequate testing procedures.Underlying Causes:Technical Infrastructure: Assess the state of the company's technical infrastructure, including network stability, server performance, and software dependencies.Processes and Procedures: Evaluate the effectiveness of existing processes and procedures related to deployment, maintenance, and troubleshooting. Identify any gaps or weaknesses.Human Factors: Consider the role of human factors, such as training deficiencies, communication issues, or decision-making processes, in the incident.The changes to the GET/POST v1/exercises endpoints were tested in DEV - via Postman - before deploying to PROD, but a full e2e test would have caught this issue. If we would have logged into CareHub and navigated to the view that hit those endpoints, this incident could have been avoided.Lessons LearnedThe lessons learned from the 5 Whys and potentially more get added as a list. Add a mandatory section on how Time to Detection and Time to Resolution can be cut in half.Focus on how lessons learnt become a part of our dev/deploy/monitoring flow, what steps we have taken to integrate those lessons.Actionable Insights:Summarize the key insights and lessons learned from the 5 Whys analysis. Focus on actionable recommendations and corrective actions that can be implemented to prevent similar incidents in the future. Include recommendations for process improvements, system enhancements, training needs, and any other measures deemed necessary based on the analysis. Time to Detection Time to Resolution Action ItemsAction items from the 5 Whys section get added here. Every action item needs to have an owner, a JIRA ticket and an ETA.Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:Technical Solutions: Implementing upgrades, patches, or enhancements to improve system reliability, scalability, and performance.Process Improvements: Reviewing and updating processes for change management, incident response, and disaster recovery.Training and Education: Providing training programs or workshops to enhance technical skills, promote best practices, and improve communication within teams.Monitoring and Alerting: Enhancing monitoring tools and alerting mechanisms to detect issues proactively and minimize downtime.Testing Improvements:  When making changes to existing APIs, we should require a full e2e test in DEV to ensure our changes don’t impact our consumer apps/services - even if these changes don’t break the API contract. Post Mortem Notes:",
        "title": "RCA: IR-372 Configurator 2.0 is not loading for few PTs",
        "page_metadata": {
          "space": "RND",
          "page_id": "1612907732",
          "estimated_word_count": 1158
        }
      },
      "analysis_results": {
        "content_analysis": {
          "incident_summary": "[A paragraph about the incident, how it was detected, how it was resolved along with a sentence on the.",
          "users_impacted": "Services: Production endpoints affected",
          "root_causes": [
            "IdentifiedList the major issues that the analysis identified",
            "Describe the specific event or trigger that initiated the incident",
            "This could include software updates, hardware failures, configuration changes, etc",
            "A PR in the Exercise Service was deployed to PROD that began sending exercise",
            "Assess the state of the company's"
          ],
          "analysis_quality": "low"
        },
        "quality_assessment": {
          "score": 92,
          "grade": "A",
          "feedback": "Exceptional RCA (92/100). Strongest area: Root Cause Analysis. Consider sharing as template. Minor enhancement opportunity: Impact Assessment.",
          "strengths": [
            "Timeline Detection: Detailed timestamps provided",
            "Timeline Detection: Dedicated timeline section",
            "Timeline Detection: Detection methods described"
          ],
          "critical_gaps": [
            "None identified"
          ]
        },
        "business_impact": {
          "impact_score": 3,
          "customer_count_affected": "User impact not specified",
          "revenue_impact_est": "Minimal: <$500 potential impact",
          "service_downtime_minutes": 30,
          "severity_justification": "Limited business impact; Service interruption (30 minutes)"
        },
        "technical_analysis": {
          "root_cause_category": "Process/Human Error",
          "detection_time_minutes": 0,
          "resolution_time_minutes": 0,
          "technical_debt_level": "Medium - Some improvements required",
          "automation_score": 3
        }
      }
    },
    {
      "jira_data": {
        "ticket_key": "IR-370",
        "summary": "PHX Playlist Generation failing with \"Socket Hang Up\"",
        "priority": "P4",
        "created_date": "2025-07-17T11:58:06.750-0700",
        "status": "Root Cause Analysis",
        "description": "Latest release to asset-service caused high latency for one of the end point leading to failure to generate playlist between ~11:42 to 12:10 impacting 875 members.\n\nAt 12:10 – SRE(@aaron) bumpoed up the memory from 1 GB to 2GB, that mitigated the issue. \n\nAt 12:49 – [~accountid:712020:f149fcfe-1d0b-4073-bb5f-846ea50d55b8]  reverted last deployment to alleviate any risks. ",
        "custom_fields": {
          "incident_urgency": "P4",
          "pods_engaged": "Content Platform, SRE & Cloud Platform",
          "reporter": null,
          "assignee": null
        },
        "comments": [],
        "metadata": {
          "jira_url": "https://hingehealth.atlassian.net/browse/IR-370",
          "labels": [],
          "component": null,
          "resolution": null
        }
      },
      "confluence_data": {
        "rca_url": "https://hingehealth.atlassian.net/wiki/spaces/RND/pages/1608417691/RCA+IR-370+PHX+Playlist+Generation+failing+with+Socket+Hang+Up",
        "rca_available": true,
        "content_html": "<p><strong>Start date:</strong>&nbsp;2025-07-17  11:42 AM PT</p><p><strong>Close date:</strong>&nbsp;2025-07-17  12:49 PM PT</p><p><strong>Severity</strong>:&nbsp;P4</p><p><strong>Adverse Event level (1-4)</strong>: &nbsp;</p><p><strong>Author</strong>: <ac:link><ri:user ri:account-id=\"712020:db4ac70f-fa01-49f8-ad90-d522cbb23ed4\" ri:local-id=\"90c36fd4-2754-4874-9ee0-e94e7c50a737\" /></ac:link> <ac:link><ri:user ri:account-id=\"712020:a1188b75-2d2b-42d6-94c9-b7d4fe8d12d8\" ri:local-id=\"52004c4e-2fab-4650-b48e-df79182f3c1e\" /></ac:link> </p><h2>Exec Summary</h2><p>On July 17,  asset-service encountered high latency for one of the end point /assets/search leading to failure to generate playlist between ~11:42 to 12:10 impacting 875 members.</p><p>At 12:10 &ndash; SRE(@aaron) bumped up the memory from 1 GB to 2GB, that mitigated the issue.</p><p>At 12:49 &ndash; <ac:link><ri:user ri:account-id=\"712020:f149fcfe-1d0b-4073-bb5f-846ea50d55b8\" ri:local-id=\"5c9e7753-254a-446d-8a19-5d35d068d835\" /></ac:link>  reverted last deployment to alleviate any risks.</p><h2>Impact</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"ce6a189c-d6a3-42b4-b144-0f97ef186674\"><ac:rich-text-body><p>Fill out the details of the impact of the incident starting with the end user/customer and then working from there to the business&nbsp; impact. Be as specific as you can (X customers impacted, Y transactions failed, $Z lost&hellip;Don't put a lot of details about how or why the incident happened, but just focus on the impact. Keep the why for later. For example, &quot;users could not log in for 30 mins because of auth service unavailability&quot; instead of &quot;one aws zone (us-east-02) had a drop in availability and our load balancing setup had a bug so any requests for log in going to us-east-02 failed for 30 mins&quot;.)</p></ac:rich-text-body></ac:structured-macro><ul><li><p><strong>Customer Impact: </strong>875 members<strong> </strong></p></li><li><p><strong>Security breach: </strong>No</p></li><li><p><strong>Business Impact: </strong>Playlist was not generated for impacted members</p></li><li><p><strong>Internal Impact: </strong>Playlist was not generated for impacted members</p></li></ul><h2>Timeline (Pacific time, with full timestamps)</h2><p>2025-07-17  11:42 AM PT - asset-service encountered high latency for one of the end point /assets/search</p><p>2025-07-17  12:10 PM PT &ndash; SRE( <ac:link><ri:user ri:account-id=\"6140cfb6481377006bb0c883\" ri:local-id=\"7dcab585-06b7-43c0-8bc5-61086bab2c84\" /></ac:link>  ) bumped up the memory from 1 GB to 2GB, that mitigated the issue.</p><p>2025-07-17  12:49 PM PT &ndash;  Rollback complete and incident is resolved.</p><h2>5 WHYs - Why did this incident happen?</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"a82cacea-5dfa-4961-b470-7201c450ae1d\"><ac:rich-text-body><p>Start from how the incident was discovered and then work down from there. Note that there may be a few branches of 5 Whys for issues that have multiple root causes. Make sure you don&rsquo;t answer the question in the first why. Let the trail continue till you get to the root issue and then the root issue has to mandatorily have an action item/lesson learned. This should also result in answers to: What was the technical issue that caused the incident? What was the reason the incident wasn't mitigated sooner?</p></ac:rich-text-body></ac:structured-macro><ol start=\"1\"><li><p>Why:&nbsp; Was there an incident?</p><ol start=\"1\"><li><p>Answer: There was a sudden latency surge to the v1/assets/search endpoint. </p></li></ol></li><li><p>Why:&nbsp;Was there a code change to the endpoint ?</p><ol start=\"1\"><li><p>No code change was made. But v1/assets/search was consumed by phx, onboarding and configurator app/bff. There is a new consumer Carehub (care-team-tools-bff) in the list</p></li></ol></li><li><p>Why:&nbsp;Is new consumer a problem ?</p><ol start=\"1\"><li><p>Though there were no huge volume of requests from care-team-tools-bff, the latencies started after care-team-tools-bff started consuming asset-service endpoints and a pattern of 401 requests started coming up to the v1/assets/search endpoint.</p></li></ol></li><li><p>Why:&nbsp;were there Unauthorized requests ?</p><ol start=\"1\"><li><p>care-team-tools-bff uses admin token to hit the endpoint. They are valid tokens and hitting the asset-service in local/dev works fine</p></li><li><p>Seems for some reason, there is a lag in authenticating the admin tokens</p></li></ol></li><li><p>Why:&nbsp;was there a lag in auth</p><ol start=\"1\"><li><p>search endpoint was using auth guard that use strategies [<code>&quot;hh-jwt&quot; , &quot;auth-jwt&quot;, &quot;hinge-health-jwt&quot;</code>] in the mentioned order. Hence for a token to be validated for admin token it has to be checked against strategies before reaching hinge-health-jwt which can verify it. This adds a subtle delay to each requests. Now this delay in order of million requests slowly becomes observable latency. This keeps on increasing to the point where it throws 401 errors. </p></li></ol></li></ol><h2>Architectural Context</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"dabd2424-f7ef-4643-9d8d-082b56302eb5\"><ac:rich-text-body><p>Give background and architectural context of the service/data flow in question, this helps reader understand the whole picture.</p></ac:rich-text-body></ac:structured-macro><p>&nbsp;</p><h2>Root Cause Analysis</h2><p>Details in <a href=\"https://hingehealth.atlassian.net/wiki/spaces/RND/pages/1608417691/RCA+IR-370+PHX+Playlist+Generation+failing+with+Socket+Hang+Up#Root-Cause-Analysis\" data-card-appearance=\"inline\">https://hingehealth.atlassian.net/wiki/spaces/RND/pages/1608417691/RCA+IR-370+PHX+Playlist+Generation+failing+with+Socket+Hang+Up#Root-Cause-Analysis</a> </p><h3>Major Problems Identified</h3><p>List the major issues that the analysis identified.</p><ul><li><p><strong>Immediate Cause</strong>: Subtle latency building up due to new calls from the care-team-tools-bff until a breaking point</p></li><li><p><strong>Contributing Factors</strong>:  care-team-tools-bff making more calls to the assets/search and the guided-breathing apis with the admin jwt token. The token validation adds a certain degree of latency to the request as asset-service first tries to validate token with Member specific auth strategies like Okta and LegacyAuth, before trying the admin auth strategy (hinge-health-jwt)</p></li><li><p><strong>Underlying Causes:</strong></p><ul><li><p><strong>Technical Infrastructure: </strong>This was a hard issue to figure out, as there were no visible errors on care-team-tools-bff or asset-service. search endpoint being one of the high availability endpoints could handle more load. Problem here was the auth being a choking point. </p></li><li><p><strong>Processes and Procedures</strong>: Load testing with emphasis on auth should be considered.</p></li><li><p><strong>Human Factors</strong>: <em><span style=\"color: rgb(151,160,175);\">Consider the role of human factors, such as training deficiencies, communication issues, or decision-making processes, in the incident.</span></em></p></li></ul></li></ul><h2>Lessons Learned</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"593c0553-4bc0-418e-b9dd-6289b0d96b35\"><ac:rich-text-body><p>The lessons learned from the 5 Whys and potentially more get added as a list. Add a mandatory section on how Time to Detection and Time to Resolution can be cut in half.</p><p>Focus on how lessons learnt become a part of our dev/deploy/monitoring flow, what steps we have taken to integrate those lessons.</p></ac:rich-text-body></ac:structured-macro><h4>Actionable Insights:</h4><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"2f99ab11-035b-4e43-b4c4-6190acce48ef\"><ac:rich-text-body><p>Summarize the key insights and lessons learned from the 5 Whys analysis. Focus on actionable recommendations and corrective actions that can be implemented to prevent similar incidents in the future. Include recommendations for process improvements, system enhancements, training needs, and any other measures deemed necessary based on the analysis.</p></ac:rich-text-body></ac:structured-macro><p /><p>We have exposed new endpoints in asset-service specific for the care-team-tools-bff that validates token against admin strategy. These are clones of existing member facing endpoints. For the last 2 weeks of deploying the new endpoints, the issue has not resurfaced or no significant latency is noticed in datadog</p><h4>Time to Detection</h4><p>&nbsp;</p><h4>Time to Resolution</h4><p>&nbsp;</p><h2>Action Items</h2><ac:structured-macro ac:name=\"info\" ac:schema-version=\"1\" ac:macro-id=\"38979d96-11f4-43ec-8116-c681467265f4\"><ac:rich-text-body><p>Action items from the 5 Whys section get added here. Every action item needs to have an owner, a JIRA ticket and an ETA.</p></ac:rich-text-body></ac:structured-macro><p>Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:</p><ul><li><p><strong>Technical Solutions:</strong> <em><span style=\"color: rgb(151,160,175);\">Implementing upgrades, patches, or enhancements to improve system reliability, scalability, and performance.</span></em></p></li><li><p><strong>Process Improvements:</strong> On using existing APIs for a new feature, if there is an expectation on the scale, then the API owner should be consulted (may be tagged in the TDD)<strong>.</strong></p></li><li><p><strong>Training and Education: </strong><span style=\"color: rgb(151,160,175);\">Providing training programs or workshops to enhance technical skills, promote best practices, and improve communication within teams.</span></p></li><li><p><strong>Monitoring and Alerting:</strong> <em><span style=\"color: rgb(151,160,175);\">Enhancing monitoring tools and alerting mechanisms to detect issues proactively and minimize downtime.</span></em></p></li><li><p><strong>Testing Improvements: </strong>Newer features added should be stress tested.</p></li></ul><p>&nbsp;</p><hr /><h3>Post Mortem Notes:&nbsp;</h3>",
        "content_text": "Start date: 2025-07-17 11:42 AM PTClose date: 2025-07-17 12:49 PM PTSeverity: P4Adverse Event level (1-4):  Author: Exec SummaryOn July 17, asset-service encountered high latency for one of the end point /assets/search leading to failure to generate playlist between ~11:42 to 12:10 impacting 875 members.At 12:10 – SRE(@aaron) bumped up the memory from 1 GB to 2GB, that mitigated the issue.At 12:49 – reverted last deployment to alleviate any risks.ImpactFill out the details of the impact of the incident starting with the end user/customer and then working from there to the business  impact. Be as specific as you can (X customers impacted, Y transactions failed, $Z lost…Don't put a lot of details about how or why the incident happened, but just focus on the impact. Keep the why for later. For example, \"users could not log in for 30 mins because of auth service unavailability\" instead of \"one aws zone (us-east-02) had a drop in availability and our load balancing setup had a bug so any requests for log in going to us-east-02 failed for 30 mins\".)Customer Impact: 875 members Security breach: NoBusiness Impact: Playlist was not generated for impacted membersInternal Impact: Playlist was not generated for impacted membersTimeline (Pacific time, with full timestamps)2025-07-17 11:42 AM PT - asset-service encountered high latency for one of the end point /assets/search2025-07-17 12:10 PM PT – SRE( ) bumped up the memory from 1 GB to 2GB, that mitigated the issue.2025-07-17 12:49 PM PT – Rollback complete and incident is resolved.5 WHYs - Why did this incident happen?Start from how the incident was discovered and then work down from there. Note that there may be a few branches of 5 Whys for issues that have multiple root causes. Make sure you don’t answer the question in the first why. Let the trail continue till you get to the root issue and then the root issue has to mandatorily have an action item/lesson learned. This should also result in answers to: What was the technical issue that caused the incident? What was the reason the incident wasn't mitigated sooner?Why:  Was there an incident?Answer: There was a sudden latency surge to the v1/assets/search endpoint. Why: Was there a code change to the endpoint ?No code change was made. But v1/assets/search was consumed by phx, onboarding and configurator app/bff. There is a new consumer Carehub (care-team-tools-bff) in the listWhy: Is new consumer a problem ?Though there were no huge volume of requests from care-team-tools-bff, the latencies started after care-team-tools-bff started consuming asset-service endpoints and a pattern of 401 requests started coming up to the v1/assets/search endpoint.Why: were there Unauthorized requests ?care-team-tools-bff uses admin token to hit the endpoint. They are valid tokens and hitting the asset-service in local/dev works fineSeems for some reason, there is a lag in authenticating the admin tokensWhy: was there a lag in authsearch endpoint was using auth guard that use strategies [\"hh-jwt\" , \"auth-jwt\", \"hinge-health-jwt\"] in the mentioned order. Hence for a token to be validated for admin token it has to be checked against strategies before reaching hinge-health-jwt which can verify it. This adds a subtle delay to each requests. Now this delay in order of million requests slowly becomes observable latency. This keeps on increasing to the point where it throws 401 errors. Architectural ContextGive background and architectural context of the service/data flow in question, this helps reader understand the whole picture. Root Cause AnalysisDetails in https://hingehealth.atlassian.net/wiki/spaces/RND/pages/1608417691/RCA+IR-370+PHX+Playlist+Generation+failing+with+Socket+Hang+Up#Root-Cause-Analysis Major Problems IdentifiedList the major issues that the analysis identified.Immediate Cause: Subtle latency building up due to new calls from the care-team-tools-bff until a breaking pointContributing Factors: care-team-tools-bff making more calls to the assets/search and the guided-breathing apis with the admin jwt token. The token validation adds a certain degree of latency to the request as asset-service first tries to validate token with Member specific auth strategies like Okta and LegacyAuth, before trying the admin auth strategy (hinge-health-jwt)Underlying Causes:Technical Infrastructure: This was a hard issue to figure out, as there were no visible errors on care-team-tools-bff or asset-service. search endpoint being one of the high availability endpoints could handle more load. Problem here was the auth being a choking point. Processes and Procedures: Load testing with emphasis on auth should be considered.Human Factors: Consider the role of human factors, such as training deficiencies, communication issues, or decision-making processes, in the incident.Lessons LearnedThe lessons learned from the 5 Whys and potentially more get added as a list. Add a mandatory section on how Time to Detection and Time to Resolution can be cut in half.Focus on how lessons learnt become a part of our dev/deploy/monitoring flow, what steps we have taken to integrate those lessons.Actionable Insights:Summarize the key insights and lessons learned from the 5 Whys analysis. Focus on actionable recommendations and corrective actions that can be implemented to prevent similar incidents in the future. Include recommendations for process improvements, system enhancements, training needs, and any other measures deemed necessary based on the analysis.We have exposed new endpoints in asset-service specific for the care-team-tools-bff that validates token against admin strategy. These are clones of existing member facing endpoints. For the last 2 weeks of deploying the new endpoints, the issue has not resurfaced or no significant latency is noticed in datadogTime to Detection Time to Resolution Action ItemsAction items from the 5 Whys section get added here. Every action item needs to have an owner, a JIRA ticket and an ETA.Based on the root cause analysis, provide actionable recommendations to prevent similar incidents in the future. This may include:Technical Solutions: Implementing upgrades, patches, or enhancements to improve system reliability, scalability, and performance.Process Improvements: On using existing APIs for a new feature, if there is an expectation on the scale, then the API owner should be consulted (may be tagged in the TDD).Training and Education: Providing training programs or workshops to enhance technical skills, promote best practices, and improve communication within teams.Monitoring and Alerting: Enhancing monitoring tools and alerting mechanisms to detect issues proactively and minimize downtime.Testing Improvements: Newer features added should be stress tested. Post Mortem Notes:",
        "title": "RCA: IR-370 PHX Playlist Generation failing with \"Socket Hang Up\"",
        "page_metadata": {
          "space": "RND",
          "page_id": "1608417691",
          "estimated_word_count": 1003
        }
      },
      "analysis_results": {
        "content_analysis": {
          "incident_summary": "On July 17, asset-service encountered high latency for one of the end point /assets/search leading to failure to generate playlist between ~11:42 to 12:10.",
          "users_impacted": "875 members; Services: Production endpoints affected; Services: Onboarding services impacted",
          "root_causes": [
            "//hingehealth",
            "search endpoint being one of the high availability endpoints could handle more load",
            "Problem here was the auth being a choking point",
            "Subtle latency building up due to new calls from the care-team-tools-bff until a breaking point",
            "This was a hard issue to figure out, as there were no visible errors on care-team-tools-bff or asset-service"
          ],
          "analysis_quality": "low"
        },
        "quality_assessment": {
          "score": 95,
          "grade": "A",
          "feedback": "Exceptional RCA (95/100). Strongest area: Root Cause Analysis. Consider sharing as template. Minor enhancement opportunity: Action Items Prevention.",
          "strengths": [
            "Timeline Detection: Detailed timestamps provided",
            "Timeline Detection: Dedicated timeline section",
            "Timeline Detection: Detection methods described"
          ],
          "critical_gaps": [
            "None identified"
          ]
        },
        "business_impact": {
          "impact_score": 3,
          "customer_count_affected": "875 users",
          "revenue_impact_est": "Minimal: <$500 potential impact",
          "service_downtime_minutes": 30,
          "severity_justification": "Limited business impact; Service interruption (30 minutes)"
        },
        "technical_analysis": {
          "root_cause_category": "Process/Human Error",
          "detection_time_minutes": 0,
          "resolution_time_minutes": 0,
          "technical_debt_level": "High - Major refactoring needed",
          "automation_score": 4
        }
      }
    }
  ]
}